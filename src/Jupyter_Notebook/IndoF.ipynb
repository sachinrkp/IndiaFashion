{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TCrWxIu-LNV",
        "outputId": "1a85a69a-fdde-490a-c447-6a61d13db5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSV-gOTq-p73",
        "outputId": "4994b6f3-96a4-4143-fb11-796d29eae739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/My Drive/IndoFashion.zip\n",
            " extracting: IndoFashion/.gitignore  \n",
            "  inflating: IndoFashion/chart.py    \n",
            "  inflating: IndoFashion/confusion_matrix.png  \n",
            "  inflating: IndoFashion/crop_samples.py  \n",
            "  inflating: IndoFashion/img_trainer.py  \n",
            "  inflating: IndoFashion/img_trainer_aug.py  \n",
            "  inflating: IndoFashion/img_trainer_bright.py  \n",
            "  inflating: IndoFashion/img_trainer_bright_shear.py  \n",
            "  inflating: IndoFashion/img_trainer_flip.py  \n",
            "  inflating: IndoFashion/img_trainer_flip_jitter.py  \n",
            "  inflating: IndoFashion/img_trainer_jitter.py  \n",
            "  inflating: IndoFashion/img_trainer_jitter_flip.py  \n",
            "  inflating: IndoFashion/img_trainer_no_augmentation.py  \n",
            "  inflating: IndoFashion/img_trainer_shear.py  \n",
            "  inflating: IndoFashion/img_trainer_translate_rotate.py  \n",
            "   creating: IndoFashion/models/\n",
            "   creating: IndoFashion/model_archs/\n",
            "  inflating: IndoFashion/model_archs/img_models.py  \n",
            " extracting: IndoFashion/model_archs/__init__.py  \n",
            "   creating: IndoFashion/model_archs/__pycache__/\n",
            "  inflating: IndoFashion/model_archs/__pycache__/img_models.cpython-38.pyc  \n",
            "  inflating: IndoFashion/model_archs/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: IndoFashion/requirements.txt  \n",
            "   creating: IndoFashion/tf_logs/\n",
            "   creating: IndoFashion/tf_logs/indof_r101_flip/\n",
            "   creating: IndoFashion/tf_logs/indof_r101_flip/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r101_flip/train/events.out.tfevents.1689744994.AsusVivobook.22641.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r101_flip/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r101_flip/val/events.out.tfevents.1689744994.AsusVivobook.22641.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r101_jitter/\n",
            "   creating: IndoFashion/tf_logs/indof_r101_jitter/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r101_jitter/train/events.out.tfevents.1689743222.AsusVivobook.20388.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r101_jitter/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r101_jitter/val/events.out.tfevents.1689743222.AsusVivobook.20388.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r101_jitter_flip/\n",
            "   creating: IndoFashion/tf_logs/indof_r101_jitter_flip/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r101_jitter_flip/train/events.out.tfevents.1689752435.AsusVivobook.28335.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r101_jitter_flip/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r101_jitter_flip/val/events.out.tfevents.1689752435.AsusVivobook.28335.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r101_noaug/\n",
            "   creating: IndoFashion/tf_logs/indof_r101_noaug/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r101_noaug/train/events.out.tfevents.1689749010.AsusVivobook.25764.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r101_noaug/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r101_noaug/val/events.out.tfevents.1689749010.AsusVivobook.25764.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r18_flip/\n",
            "   creating: IndoFashion/tf_logs/indof_r18_flip/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r18_flip/train/events.out.tfevents.1689744431.AsusVivobook.22288.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r18_flip/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r18_flip/val/events.out.tfevents.1689744431.AsusVivobook.22288.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r18_jitter/\n",
            "   creating: IndoFashion/tf_logs/indof_r18_jitter/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r18_jitter/train/events.out.tfevents.1689742041.AsusVivobook.19772.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r18_jitter/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r18_jitter/val/events.out.tfevents.1689742041.AsusVivobook.19772.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r18_jitter_flip/\n",
            "   creating: IndoFashion/tf_logs/indof_r18_jitter_flip/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r18_jitter_flip/train/events.out.tfevents.1689750612.AsusVivobook.27950.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r18_jitter_flip/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r18_jitter_flip/val/events.out.tfevents.1689750612.AsusVivobook.27950.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r18_noaug/\n",
            "   creating: IndoFashion/tf_logs/indof_r18_noaug/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r18_noaug/train/events.out.tfevents.1689748712.AsusVivobook.25415.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r18_noaug/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r18_noaug/val/events.out.tfevents.1689748712.AsusVivobook.25415.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r50/\n",
            "   creating: IndoFashion/tf_logs/indof_r50/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r50/train/events.out.tfevents.1689746145.AsusVivobook.24343.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r50/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r50/val/events.out.tfevents.1689746145.AsusVivobook.24343.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_flip/\n",
            "   creating: IndoFashion/tf_logs/indof_r50_flip/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_flip/train/events.out.tfevents.1689741372.AsusVivobook.18406.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_flip/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_flip/val/events.out.tfevents.1689741372.AsusVivobook.18406.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_flip_jitter/\n",
            "   creating: IndoFashion/tf_logs/indof_r50_flip_jitter/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_flip_jitter/train/events.out.tfevents.1689178897.AsusVivobook.23406.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_flip_jitter/train/events.out.tfevents.1689179003.AsusVivobook.23495.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_flip_jitter/train/events.out.tfevents.1689179072.AsusVivobook.23580.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_flip_jitter/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_flip_jitter/val/events.out.tfevents.1689178897.AsusVivobook.23406.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_flip_jitter/val/events.out.tfevents.1689179003.AsusVivobook.23495.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_flip_jitter/val/events.out.tfevents.1689179072.AsusVivobook.23580.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter/\n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/train/events.out.tfevents.1689175791.AsusVivobook.22056.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/train/events.out.tfevents.1689177775.AsusVivobook.22860.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/train/events.out.tfevents.1689177887.AsusVivobook.23226.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/train/events.out.tfevents.1689216430.AsusVivobook.2860.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/train/events.out.tfevents.1689691364.AsusVivobook.16725.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/train/events.out.tfevents.1689697472.AsusVivobook.16949.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/val/events.out.tfevents.1689175791.AsusVivobook.22056.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/val/events.out.tfevents.1689177775.AsusVivobook.22860.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/val/events.out.tfevents.1689177887.AsusVivobook.23226.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/val/events.out.tfevents.1689216430.AsusVivobook.2860.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/val/events.out.tfevents.1689691364.AsusVivobook.16725.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter/val/events.out.tfevents.1689697472.AsusVivobook.16949.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter.pt/\n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter.pt/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter.pt/train/events.out.tfevents.1689177862.AsusVivobook.23144.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter.pt/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter.pt/val/events.out.tfevents.1689177862.AsusVivobook.23144.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter_flip/\n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter_flip/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1689222508.AsusVivobook.12795.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1689223023.AsusVivobook.14326.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1689671051.AsusVivobook.15702.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1689750299.AsusVivobook.27104.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1690278180.AsusVivobook.30212.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1690279188.AsusVivobook.30612.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1690279386.AsusVivobook.30786.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1690279571.AsusVivobook.30912.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1690279692.AsusVivobook.31031.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/train/events.out.tfevents.1690766842.AsusVivobook.27343.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter_flip/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1689222508.AsusVivobook.12795.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1689223023.AsusVivobook.14326.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1689671051.AsusVivobook.15702.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1689750299.AsusVivobook.27104.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1690278180.AsusVivobook.30212.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1690279188.AsusVivobook.30612.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1690279386.AsusVivobook.30786.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1690279571.AsusVivobook.30912.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1690279692.AsusVivobook.31031.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip/val/events.out.tfevents.1690766842.AsusVivobook.27343.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter_flip_classweights/\n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter_flip_classweights/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip_classweights/train/events.out.tfevents.1690768619.AsusVivobook.4966.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_jitter_flip_classweights/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_jitter_flip_classweights/val/events.out.tfevents.1690768619.AsusVivobook.4966.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_noaug/\n",
            "   creating: IndoFashion/tf_logs/indof_r50_noaug/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/train/events.out.tfevents.1689176985.AsusVivobook.22319.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/train/events.out.tfevents.1689177060.AsusVivobook.22417.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/train/events.out.tfevents.1689177179.AsusVivobook.22505.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/train/events.out.tfevents.1689177203.AsusVivobook.22578.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/train/events.out.tfevents.1689177268.AsusVivobook.22654.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/train/events.out.tfevents.1689177411.AsusVivobook.22730.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/train/events.out.tfevents.1689746182.AsusVivobook.24461.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_noaug/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/val/events.out.tfevents.1689176985.AsusVivobook.22319.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/val/events.out.tfevents.1689177060.AsusVivobook.22417.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/val/events.out.tfevents.1689177179.AsusVivobook.22505.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/val/events.out.tfevents.1689177203.AsusVivobook.22578.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/val/events.out.tfevents.1689177268.AsusVivobook.22654.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/val/events.out.tfevents.1689177411.AsusVivobook.22730.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_noaug/val/events.out.tfevents.1689746182.AsusVivobook.24461.1  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_translate_rotate/\n",
            "   creating: IndoFashion/tf_logs/indof_r50_translate_rotate/train/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/train/events.out.tfevents.1690768512.AsusVivobook.4506.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/train/events.out.tfevents.1692020835.AsusVivobook.381.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/train/events.out.tfevents.1692021507.AsusVivobook.1183.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/train/events.out.tfevents.1692033756.AsusVivobook.15694.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/train/events.out.tfevents.1692033831.AsusVivobook.15868.0  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/train/events.out.tfevents.1692035693.AsusVivobook.20266.0  \n",
            "   creating: IndoFashion/tf_logs/indof_r50_translate_rotate/val/\n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/val/events.out.tfevents.1690768512.AsusVivobook.4506.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/val/events.out.tfevents.1692020835.AsusVivobook.381.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/val/events.out.tfevents.1692021507.AsusVivobook.1183.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/val/events.out.tfevents.1692033756.AsusVivobook.15694.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/val/events.out.tfevents.1692033831.AsusVivobook.15868.1  \n",
            " extracting: IndoFashion/tf_logs/indof_r50_translate_rotate/val/events.out.tfevents.1692035693.AsusVivobook.20266.1  \n",
            "   creating: IndoFashion/tf_logs/my_model/\n",
            "   creating: IndoFashion/tf_logs/my_model/train/\n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1688112868.AsusVivobook.14283.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1688113565.AsusVivobook.15440.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1688113736.AsusVivobook.16686.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1688114441.AsusVivobook.17845.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1688114650.AsusVivobook.18088.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1688115124.AsusVivobook.18335.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1688131699.AsusVivobook.25812.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1688627163.AsusVivobook.1088.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1688634077.AsusVivobook.2088.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690650496.AsusVivobook.2745.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690652348.AsusVivobook.5694.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690652440.AsusVivobook.5910.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690652783.AsusVivobook.6490.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690653113.AsusVivobook.7043.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690653836.AsusVivobook.8178.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690653997.AsusVivobook.8495.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690654138.AsusVivobook.8814.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690654248.AsusVivobook.9049.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690654453.AsusVivobook.9491.0  \n",
            "  inflating: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690654694.AsusVivobook.9984.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690727638.AsusVivobook.21415.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1690738713.AsusVivobook.7794.0  \n",
            " extracting: IndoFashion/tf_logs/my_model/train/events.out.tfevents.1692038526.AsusVivobook.21329.0  \n",
            "   creating: IndoFashion/tf_logs/my_model/val/\n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1688112868.AsusVivobook.14283.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1688113565.AsusVivobook.15440.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1688113736.AsusVivobook.16686.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1688114441.AsusVivobook.17845.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1688114650.AsusVivobook.18088.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1688115124.AsusVivobook.18335.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1688131699.AsusVivobook.25812.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1688627163.AsusVivobook.1088.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1688634077.AsusVivobook.2088.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690650496.AsusVivobook.2745.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690652348.AsusVivobook.5694.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690652440.AsusVivobook.5910.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690652783.AsusVivobook.6490.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690653113.AsusVivobook.7043.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690653836.AsusVivobook.8178.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690653997.AsusVivobook.8495.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690654138.AsusVivobook.8814.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690654248.AsusVivobook.9049.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690654453.AsusVivobook.9491.1  \n",
            "  inflating: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690654694.AsusVivobook.9984.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690727638.AsusVivobook.21415.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1690738713.AsusVivobook.7794.1  \n",
            " extracting: IndoFashion/tf_logs/my_model/val/events.out.tfevents.1692038526.AsusVivobook.21329.1  \n",
            "   creating: IndoFashion/utils/\n",
            "  inflating: IndoFashion/utils/common_utils.py  \n",
            "  inflating: IndoFashion/utils/config.py  \n",
            "  inflating: IndoFashion/utils/dataset.py  \n",
            "  inflating: IndoFashion/utils/enums.py  \n",
            "  inflating: IndoFashion/utils/tflogger.py  \n",
            "   creating: IndoFashion/utils/__pycache__/\n",
            "  inflating: IndoFashion/utils/__pycache__/common_utils.cpython-38.pyc  \n",
            "  inflating: IndoFashion/utils/__pycache__/config.cpython-38.pyc  \n",
            "  inflating: IndoFashion/utils/__pycache__/dataset.cpython-38.pyc  \n",
            "  inflating: IndoFashion/utils/__pycache__/enums.cpython-38.pyc  \n",
            "  inflating: IndoFashion/utils/__pycache__/tflogger.cpython-38.pyc  \n"
          ]
        }
      ],
      "source": [
        "!unzip drive/My\\ Drive/IndoFashion.zip -d IndoFashion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MWAAukV_KMh",
        "outputId": "5b45bec7-7e02-416d-b58f-e2cf0e5df610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: IndoFashionDataset/images/val/3318.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3319.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/332.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3320.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3321.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3322.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3323.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3324.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3325.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3326.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3327.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3328.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3329.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/333.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3330.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3331.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3332.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3333.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3334.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3335.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3336.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3337.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3338.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3339.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/334.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3340.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3341.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3342.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3343.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3344.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3345.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3346.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3347.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3348.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3349.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/335.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3350.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3351.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3352.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3353.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3354.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3355.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3356.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3357.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3358.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3359.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/336.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3360.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3361.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3362.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3363.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3364.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3365.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3366.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3367.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3368.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3369.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/337.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3370.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3371.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3372.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3373.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3374.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3375.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3376.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3377.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3378.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3379.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/338.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3380.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3381.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3382.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3383.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3384.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3385.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3386.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3387.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3388.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3389.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/339.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3390.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3391.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3392.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3393.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3394.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3395.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3396.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3397.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3398.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3399.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/34.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/340.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3400.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3401.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3402.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3403.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3404.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3405.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3406.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3407.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3408.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3409.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/341.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3410.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3411.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3412.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3413.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3414.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3415.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3416.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3417.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3418.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3419.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/342.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3420.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3421.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3422.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3423.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3424.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3425.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3426.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3427.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3428.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3429.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/343.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3430.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3431.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3432.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3433.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3434.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3435.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3436.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3437.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3438.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3439.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/344.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3440.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3441.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3442.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3443.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3444.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3445.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3446.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3447.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3448.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3449.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/345.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3450.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3451.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3452.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3453.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3454.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3455.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3456.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3457.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3458.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3459.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/346.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3460.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3461.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3462.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3463.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3464.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3465.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3466.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3467.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3468.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3469.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/347.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3470.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3471.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3472.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3473.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3474.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3475.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3476.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3477.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3478.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3479.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/348.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3480.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3481.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3482.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3483.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3484.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3485.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3486.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3487.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3488.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3489.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/349.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3490.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3491.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3492.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3493.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3494.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3495.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3496.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3497.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3498.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3499.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/35.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/350.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3500.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3501.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3502.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3503.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3504.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3505.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3506.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3507.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3508.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3509.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/351.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3510.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3511.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3512.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3513.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3514.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3515.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3516.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3517.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3518.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3519.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/352.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3520.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3521.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3522.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3523.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3524.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3525.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3526.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3527.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3528.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3529.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/353.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3530.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3531.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3532.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3533.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3534.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3535.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3536.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3537.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3538.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3539.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/354.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3540.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3541.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3542.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3543.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3544.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3545.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3546.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3547.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3548.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3549.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/355.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3550.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3551.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3552.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3553.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3554.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3555.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3556.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3557.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3558.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3559.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/356.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3560.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3561.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3562.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3563.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3564.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3565.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3566.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3567.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3568.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3569.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/357.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3570.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3571.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3572.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3573.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3574.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3575.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3576.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3577.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3578.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3579.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/358.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3580.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3581.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3582.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3583.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3584.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3585.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3586.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3587.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3588.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3589.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/359.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3590.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3591.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3592.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3593.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3594.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3595.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3596.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3597.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3598.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3599.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/36.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/360.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3600.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3601.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3602.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3603.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3604.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3605.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3606.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3607.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3608.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3609.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/361.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3610.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3611.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3612.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3613.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3614.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3615.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3616.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3617.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3618.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3619.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/362.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3620.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3621.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3622.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3623.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3624.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3625.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3626.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3627.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3628.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3629.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/363.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3630.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3631.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3632.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3633.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3634.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3635.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3636.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3637.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3638.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3639.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/364.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3640.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3641.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3642.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3643.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3644.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3645.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3646.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3647.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3648.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3649.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/365.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3650.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3651.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3652.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3653.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3654.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3655.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3656.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3657.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3658.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3659.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/366.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3660.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3661.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3662.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3663.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3664.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3665.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3666.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3667.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3668.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3669.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/367.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3670.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3671.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3672.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3673.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3674.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3675.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3676.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3677.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3678.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3679.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/368.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3680.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3681.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3682.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3683.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3684.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3685.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3686.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3687.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3688.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3689.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/369.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3690.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3691.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3692.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3693.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3694.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3695.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3696.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3697.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3698.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3699.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/37.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/370.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3700.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3701.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3702.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3703.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3704.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3705.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3706.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3707.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3708.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3709.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/371.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3710.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3711.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3712.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3713.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3714.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3715.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3716.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3717.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3718.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3719.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/372.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3720.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3721.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3722.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3723.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3724.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3725.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3726.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3727.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3728.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3729.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/373.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3730.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3731.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3732.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3733.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3734.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3735.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3736.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3737.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3738.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3739.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/374.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3740.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3741.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3742.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3743.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3744.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3745.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3746.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3747.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3748.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3749.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/375.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3750.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3751.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3752.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3753.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3754.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3755.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3756.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3757.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3758.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3759.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/376.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3760.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3761.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3762.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3763.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3764.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3765.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3766.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3767.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3768.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3769.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/377.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3770.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3771.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3772.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3773.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3774.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3775.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3776.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3777.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3778.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3779.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/378.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3780.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3781.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3782.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3783.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3784.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3785.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3786.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3787.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3788.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3789.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/379.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3790.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3791.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3792.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3793.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3794.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3795.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3796.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3797.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3798.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3799.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/38.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/380.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3800.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3801.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3802.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3803.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3804.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3805.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3806.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3807.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3808.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3809.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/381.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3810.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3811.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3812.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3813.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3814.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3815.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3816.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3817.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3818.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3819.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/382.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3820.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3821.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3822.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3823.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3824.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3825.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3826.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3827.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3828.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3829.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/383.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3830.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3831.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3832.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3833.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3834.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3835.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3836.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3837.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3838.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3839.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/384.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3840.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3841.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3842.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3843.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3844.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3845.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3846.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3847.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3848.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3849.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/385.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3850.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3851.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3852.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3853.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3854.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3855.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3856.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3857.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3858.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3859.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/386.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3860.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3861.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3862.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3863.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3864.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3865.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3866.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3867.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3868.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3869.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/387.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3870.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3871.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3872.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3873.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3874.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3875.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3876.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3877.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3878.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3879.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/388.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3880.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3881.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3882.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3883.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3884.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3885.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3886.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3887.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3888.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3889.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/389.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3890.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3891.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3892.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3893.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3894.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3895.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3896.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3897.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3898.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3899.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/39.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/390.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3900.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3901.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3902.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3903.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3904.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3905.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3906.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3907.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3908.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3909.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/391.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3910.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3911.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3912.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3913.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3914.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3915.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3916.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3917.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3918.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3919.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/392.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3920.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3921.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3922.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3923.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3924.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3925.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3926.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3927.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3928.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3929.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/393.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3930.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3931.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3932.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3933.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3934.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3935.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3936.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3937.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3938.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3939.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/394.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3940.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3941.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3942.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3943.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3944.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3945.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3946.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3947.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3948.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3949.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/395.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3950.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3951.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3952.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3953.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3954.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3955.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3956.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3957.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3958.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3959.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/396.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3960.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3961.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3962.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3963.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3964.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3965.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3966.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3967.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3968.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3969.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/397.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3970.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3971.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3972.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3973.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3974.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3975.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3976.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3977.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3978.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3979.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/398.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3980.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3981.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3982.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3983.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3984.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3985.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3986.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3987.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3988.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3989.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/399.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3990.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3991.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/3992.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3993.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3995.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3996.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3997.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3998.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/3999.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/4.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/40.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/400.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4000.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4001.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4002.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4003.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4004.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4005.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4006.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4007.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4008.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4009.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/401.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4010.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4011.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4012.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4013.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4014.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4015.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4016.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4017.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4018.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4019.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/402.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4020.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4021.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4022.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4023.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4024.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4025.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4026.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4027.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4028.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4029.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/403.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4030.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4031.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4032.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4033.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4034.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4035.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4036.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4037.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4038.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4039.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/404.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4040.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4041.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4042.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4043.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4044.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4045.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4046.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4047.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4048.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4049.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/405.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4050.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4051.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4052.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4053.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4054.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4055.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4056.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4057.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4058.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4059.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/406.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4060.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4061.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4062.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4063.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4064.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4065.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4066.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4067.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4068.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4069.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/407.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4070.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4071.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4072.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4073.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4074.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4075.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4076.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4077.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4078.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4079.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/408.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4080.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4081.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4082.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4083.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4084.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4085.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4086.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4087.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4088.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4089.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/409.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4090.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4091.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4092.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4093.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4094.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4095.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4096.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4097.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4098.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4099.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/41.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/410.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4100.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4101.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4102.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4103.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4104.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4105.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4106.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4107.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4108.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4109.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/411.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4110.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4111.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4112.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4113.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4114.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4115.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4116.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4117.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4118.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4119.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/412.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4120.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4121.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4122.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4123.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4124.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4125.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4126.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4127.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4128.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4129.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/413.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4130.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4131.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4132.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4133.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4134.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4135.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4136.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4137.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4138.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4139.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/414.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4140.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4141.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4142.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4143.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4144.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4145.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4146.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4147.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4148.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4149.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/415.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4150.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4151.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4152.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4153.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4154.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4155.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4156.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4157.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4158.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4159.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/416.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4160.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4161.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4162.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4163.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4164.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4165.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4166.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4167.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4168.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4169.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/417.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4170.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4171.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4172.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4173.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4174.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4175.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4176.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4177.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4178.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4179.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/418.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4180.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4181.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4182.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4183.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4184.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4185.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4186.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4187.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4188.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4189.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/419.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4190.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4191.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4192.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4193.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4194.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4195.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4196.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4197.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4198.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4199.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/42.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/420.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4200.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4201.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4202.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4203.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4204.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4205.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4206.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4207.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4208.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4209.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/421.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4210.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4211.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4212.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4213.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4214.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4215.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4216.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4217.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4218.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4219.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/422.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4220.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4221.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4222.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4223.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4224.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4225.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4226.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4227.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4228.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4229.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/423.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4230.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4231.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4232.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4233.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4234.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4235.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4236.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4237.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4238.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4239.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/424.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4240.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4241.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4242.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4243.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4244.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4245.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4246.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4247.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4248.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4249.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/425.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4250.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4251.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4252.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4253.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4254.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4255.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4256.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4257.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4258.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4259.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/426.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4260.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4261.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4262.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4263.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4264.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4265.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4266.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4267.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4268.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4269.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/427.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4270.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4271.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4272.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4273.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4274.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4275.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4276.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4277.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4278.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4279.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/428.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4280.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4281.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4282.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4283.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4284.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4285.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4286.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4287.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4288.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4289.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/429.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4290.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4291.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4292.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4293.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4294.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4295.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4296.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4297.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4298.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4299.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/43.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/430.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4300.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4301.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4302.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4303.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4304.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4305.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4306.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4307.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4308.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4309.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/431.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4310.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4311.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4312.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4313.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4314.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4315.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4316.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4317.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4318.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4319.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/432.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4320.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4321.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4322.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4323.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4324.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4325.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4326.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4327.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4328.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4329.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/433.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4330.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4331.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4332.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4333.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4334.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4335.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4336.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4337.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4338.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4339.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/434.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4340.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4341.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4342.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4343.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4344.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4345.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4346.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4347.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4348.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4349.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/435.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4350.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4351.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4352.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4353.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4354.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4355.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4356.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4357.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4358.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4359.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/436.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4360.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4361.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4362.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4363.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4364.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4365.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4366.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4367.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4368.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4369.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/437.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4370.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4371.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4372.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4373.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4374.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4375.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4376.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4377.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4378.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4379.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/438.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4380.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4381.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4382.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4383.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4384.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4385.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4386.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4387.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4388.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4389.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/439.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4390.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4391.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4392.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4393.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4394.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4395.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4396.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4397.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4398.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4399.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/44.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/440.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4400.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4401.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4402.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4403.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4404.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4405.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4406.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4407.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4408.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4409.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/441.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4410.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4411.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4412.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4413.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4414.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4415.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4416.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4417.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4418.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4419.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/442.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4420.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4421.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4422.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4423.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4424.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4425.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4426.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4427.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4428.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4429.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/443.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4430.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4431.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4432.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4433.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4434.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4435.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4436.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4437.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4438.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4439.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/444.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4440.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4441.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4442.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4443.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4444.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4445.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4446.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4447.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4448.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4449.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/445.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4450.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4451.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4452.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4453.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4454.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4455.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4456.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4457.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4458.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4459.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/446.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4460.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4461.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4462.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4463.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4464.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4465.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4466.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4467.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4468.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4469.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/447.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4470.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4471.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4472.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4473.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4474.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4475.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4476.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4477.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4478.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4479.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/448.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4480.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4481.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4482.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4483.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4484.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4485.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4486.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4487.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4488.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4489.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/449.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4490.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4491.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4492.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4493.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4494.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4495.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4496.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4497.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4498.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4499.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/45.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/450.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4500.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4501.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4502.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4503.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4504.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4505.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4506.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4507.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4508.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4509.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/451.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4510.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4511.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4512.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4513.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4514.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4515.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4516.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4517.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4518.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4519.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/452.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4520.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4521.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4522.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4523.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4524.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4525.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4526.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4527.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4528.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4529.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/453.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4530.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4531.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4532.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4533.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4534.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4535.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4536.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4537.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4538.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4539.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/454.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4540.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4541.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4542.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4543.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4544.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4545.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4546.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4547.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4548.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4549.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/455.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4550.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4551.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4552.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4553.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4554.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4555.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4556.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4557.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4558.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4559.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/456.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4560.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4561.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4562.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4563.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4564.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4565.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4566.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4567.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4568.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4569.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/457.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4570.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4571.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4572.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4573.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4574.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4575.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4576.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4577.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4578.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4579.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/458.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4580.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4581.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4582.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4583.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4584.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4585.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4586.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4587.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4588.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4589.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/459.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4590.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4591.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4592.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4593.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4594.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4595.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4596.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4597.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4598.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4599.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/46.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/460.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4600.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4601.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4602.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4603.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4604.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4605.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4606.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4607.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4608.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4609.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/461.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4610.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4611.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4612.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4613.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4614.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4615.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4616.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4617.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4618.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4619.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/462.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4620.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4621.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4622.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4623.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4624.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4625.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4626.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4627.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4628.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4629.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/463.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4630.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4631.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4632.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4633.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4634.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4635.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4636.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4637.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4638.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4639.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/464.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4640.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4641.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4642.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4643.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4644.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4645.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4646.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4647.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4648.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4649.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/465.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4650.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4651.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4652.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4653.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4654.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4655.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4656.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4657.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4658.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4659.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/466.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4660.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4661.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4662.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4663.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4664.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4665.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4666.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4667.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4668.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4669.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/467.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4670.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4671.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4672.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4673.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4674.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4675.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4676.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4677.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4678.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4679.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/468.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4680.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4681.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4682.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4683.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4684.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4685.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4686.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4687.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4688.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4689.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/469.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4690.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4691.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4692.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4693.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4694.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4695.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4696.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4697.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4698.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4699.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/47.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/470.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4700.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4701.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4702.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4703.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4704.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4705.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4706.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4707.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4708.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4709.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/471.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4710.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4711.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4712.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4713.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4714.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4715.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4716.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4717.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4718.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4719.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/472.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4720.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4721.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4722.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4723.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4724.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4725.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4726.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4727.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4728.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4729.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/473.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4730.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4731.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4732.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4733.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4734.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4735.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4736.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4737.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4738.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4739.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/474.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4740.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4741.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4742.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4743.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4744.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4745.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4746.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4747.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4748.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4749.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/475.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4750.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4751.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4752.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4753.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4754.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4755.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4756.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4757.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4758.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4759.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/476.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4760.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4761.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4762.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4763.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4764.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4765.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4766.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4767.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4768.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4769.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/477.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4770.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4771.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4772.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4773.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4774.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4775.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4776.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4777.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4778.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4779.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/478.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4780.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4781.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4782.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4783.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4784.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4785.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4786.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4787.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4788.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4789.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/479.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4790.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4791.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4792.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4793.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4794.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4795.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4796.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4797.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4798.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4799.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/48.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/480.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4800.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4801.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4802.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4803.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4804.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4805.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4806.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4807.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4808.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4809.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/481.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4810.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4811.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4812.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4813.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4814.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4815.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4816.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4817.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4818.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4819.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/482.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4820.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4821.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4822.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4823.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4824.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4825.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4826.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4827.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4828.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4829.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/483.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4830.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4831.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4832.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4833.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4834.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4835.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4836.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4837.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4838.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4839.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/484.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4840.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4841.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4842.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4843.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4844.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4845.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4846.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4847.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4848.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4849.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/485.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4850.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4851.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4852.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4853.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4854.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4855.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4856.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4857.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4858.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4859.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/486.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4860.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4861.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4862.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4863.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4864.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4865.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4866.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4867.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4868.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4869.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/487.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4870.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4871.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4872.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4873.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4874.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4875.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4876.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4877.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4878.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4879.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/488.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4880.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4881.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4882.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4883.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4884.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4885.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4886.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4887.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4888.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4889.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/489.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4890.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4891.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4892.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4893.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4894.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4895.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4896.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4897.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4898.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4899.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/49.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/490.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4900.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4901.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4902.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4903.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4904.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4905.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4906.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4907.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4908.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4909.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/491.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4910.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4911.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4912.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4913.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4914.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4915.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4916.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4917.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4918.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4919.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/492.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4920.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4921.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4922.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4923.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4924.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4925.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4926.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4927.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4928.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4929.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/493.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4930.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4931.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4932.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4933.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4934.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4935.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4936.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4937.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4938.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4939.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/494.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4940.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4941.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4942.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4943.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4944.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4945.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4946.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4947.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4948.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4949.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/495.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4950.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4951.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4952.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4953.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4954.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4955.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4956.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4957.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4958.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4959.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/496.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4960.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4961.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4962.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4963.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4964.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4965.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4966.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4967.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4968.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4969.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/497.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4970.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4971.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4972.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4973.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4974.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4975.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4976.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4977.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4978.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4979.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/498.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4980.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4981.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4982.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4983.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4984.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4985.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4986.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4987.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4988.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4989.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/499.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4990.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4991.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4992.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4993.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4994.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4995.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4996.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4997.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4998.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/4999.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/50.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/500.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5000.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5001.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5002.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5003.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5004.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5005.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5006.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5007.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5008.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5009.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/501.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5010.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5011.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5012.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5013.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5014.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5015.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5016.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5017.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5018.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5019.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/502.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5020.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5021.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5022.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5023.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5024.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5025.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5026.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5027.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5028.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5029.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/503.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5030.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5031.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5032.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5033.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5034.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5035.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5036.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5037.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5038.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5039.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/504.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5040.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5041.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5042.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5043.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5044.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5045.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5046.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5047.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5048.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5049.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/505.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5050.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5051.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5052.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5053.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5054.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5055.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5056.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5057.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5058.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5059.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/506.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5060.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5061.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5062.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5063.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5064.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5065.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5066.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5067.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5068.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5069.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/507.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5070.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5071.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5072.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5073.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5074.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5075.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5076.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5077.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5078.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5079.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/508.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5080.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5081.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5082.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5083.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5084.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5085.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5086.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5087.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5088.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5089.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/509.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5090.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5091.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5092.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5093.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5094.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5095.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5096.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5097.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5098.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5099.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/51.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/510.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5100.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5101.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5102.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5103.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5104.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5105.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5106.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5107.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5108.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5109.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/511.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5110.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5111.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5112.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5113.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5114.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5115.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5116.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5117.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5118.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5119.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/512.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5120.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5121.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5122.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5123.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5124.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5125.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5126.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5127.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5128.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5129.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/513.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5130.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5131.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5132.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5133.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5134.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5135.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5136.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5137.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5138.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5139.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/514.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5140.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5141.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5142.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5143.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5144.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5145.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5146.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5147.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5148.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5149.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/515.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5150.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5151.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5152.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5153.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5154.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5155.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5156.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5157.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5158.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5159.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/516.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5160.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5161.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5162.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5163.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5164.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5165.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5166.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5167.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5168.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5169.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/517.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5170.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5171.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5172.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5173.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5174.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5175.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5176.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5177.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5178.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5179.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/518.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5180.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5181.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5182.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5183.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5184.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5185.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5186.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5187.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5188.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5189.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/519.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5190.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5191.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5192.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5193.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5194.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5195.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5196.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5197.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5198.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5199.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/52.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/520.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5200.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5201.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5202.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5203.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5204.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5205.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5206.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5207.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5208.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5209.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/521.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5210.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5211.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5212.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5213.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5214.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5215.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5216.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5217.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5218.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5219.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/522.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5220.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5221.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5222.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5223.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5224.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5225.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5226.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5227.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5228.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5229.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/523.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5230.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5231.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5232.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5233.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5234.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5235.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5236.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5237.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5238.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5239.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/524.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5240.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5241.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5242.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5243.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5244.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5245.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5246.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5247.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5248.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5249.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/525.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5250.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5251.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5252.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5253.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5254.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5255.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5256.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5257.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5258.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5259.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/526.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5260.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5261.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5262.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5263.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5264.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5265.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5266.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5267.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5268.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5269.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/527.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5270.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5271.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5272.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5273.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5274.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5275.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5276.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5277.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5278.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5279.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/528.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5280.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5281.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5282.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5283.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5284.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5285.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5286.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5287.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5288.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5289.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/529.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5290.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5291.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5292.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5293.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5294.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5295.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5296.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5297.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5298.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5299.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/53.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/530.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5300.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5301.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5302.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5303.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5304.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5305.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5306.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5307.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5308.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5309.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/531.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5310.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5311.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5312.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5313.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5314.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5315.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5316.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5317.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5318.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5319.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/532.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5320.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5321.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5322.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5323.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5324.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5325.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5326.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5327.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5328.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5329.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/533.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5330.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5331.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5332.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5333.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5334.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5335.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5336.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5337.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5338.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5339.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/534.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5340.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5341.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5342.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5343.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5344.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5345.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5346.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5347.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5348.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5349.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/535.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5350.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5351.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5352.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5353.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5354.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5355.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5356.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5357.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5358.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5359.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/536.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5360.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5361.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5362.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5363.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5364.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5365.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5366.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5367.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5368.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5369.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/537.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5370.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5371.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5372.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5373.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5374.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5375.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5376.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5377.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5378.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5379.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/538.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5380.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5381.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5382.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5383.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5384.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5385.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5386.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5387.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5388.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5389.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/539.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5390.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5391.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5392.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5393.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5394.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5395.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5396.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5397.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5398.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5399.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/54.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/540.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5400.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5401.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5402.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5403.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5404.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5405.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5406.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5407.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5408.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5409.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/541.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5410.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5411.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5412.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5413.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5414.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5415.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5416.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5417.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5418.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5419.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/542.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5420.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5421.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5422.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5423.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5424.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5425.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5426.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5427.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5428.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5429.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/543.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5430.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5431.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5432.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5433.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5434.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5435.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5436.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5437.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5438.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5439.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/544.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5440.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5441.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5442.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5443.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5444.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5445.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5446.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5447.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5448.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5449.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/545.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5450.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5451.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5452.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5453.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5454.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5455.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5456.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5457.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5458.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5459.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/546.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5460.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5461.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5462.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5463.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5464.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5465.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5466.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5467.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5468.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5469.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/547.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5470.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5471.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5472.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5473.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5474.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5475.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5476.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5477.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5478.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5479.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/548.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5480.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5481.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5482.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5483.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5484.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5485.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5486.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5487.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5488.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5489.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/549.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5490.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5491.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5492.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5493.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5494.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5495.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5496.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5497.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5498.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5499.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/55.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/550.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5500.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5501.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5502.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5503.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5504.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5505.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5506.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5507.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5508.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5509.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/551.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5510.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5511.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5512.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5513.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5514.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5515.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5516.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5517.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5518.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5519.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/552.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5520.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5521.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5522.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5523.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5524.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5525.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5526.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5527.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5528.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5529.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/553.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5530.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5531.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5532.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5533.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5534.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5535.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5536.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5537.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5538.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5539.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/554.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5540.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5541.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5542.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5543.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5544.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5545.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5546.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5547.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5548.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5549.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/555.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5550.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5551.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5552.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5553.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5554.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5555.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5556.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5557.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5558.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5559.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/556.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5560.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5561.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5562.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5563.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5564.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5565.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5566.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5567.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5568.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5569.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/557.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5570.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5571.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5572.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5573.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5574.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5575.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5576.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5577.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5578.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5579.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/558.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5580.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5581.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5582.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5583.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5584.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5585.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5586.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5587.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5588.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5589.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/559.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5590.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5591.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5592.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5593.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5594.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5595.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5596.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5597.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5598.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5599.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/56.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/560.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5600.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5601.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5602.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5603.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5604.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5605.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5606.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5607.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5608.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5609.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/561.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5610.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5611.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5612.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5613.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5614.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5615.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5616.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5617.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5618.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5619.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/562.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5620.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5621.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5622.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5623.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5624.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5625.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5626.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5627.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5628.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5629.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/563.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5630.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5631.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5632.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5633.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5634.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5635.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5636.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5637.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5638.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5639.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/564.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5640.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5641.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5642.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5643.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5644.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5645.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5646.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5647.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5648.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5649.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/565.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5650.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5651.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5652.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5653.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5654.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5655.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5656.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5657.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5658.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5659.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/566.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5660.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5661.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5662.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5663.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5664.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5665.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5666.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5667.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5668.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5669.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/567.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5670.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5671.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5672.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5673.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5674.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5675.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5676.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5677.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5678.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5679.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/568.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5680.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5681.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5682.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5683.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5684.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5685.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5686.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5687.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5688.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5689.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/569.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5690.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5691.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5692.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5693.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5694.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5695.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5696.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5697.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5698.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5699.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/57.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/570.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5700.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5701.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5702.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5703.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5704.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5705.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5706.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5707.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5708.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5709.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/571.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5710.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5711.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5712.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5713.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5714.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5715.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5716.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5717.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5718.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5719.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/572.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5720.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5721.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5722.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5723.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5724.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5725.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5726.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5727.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5728.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5729.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/573.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5730.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5731.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5732.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5733.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5734.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5735.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5736.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5737.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5738.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5739.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/574.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5740.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5741.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5742.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5743.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5744.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5745.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5746.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5747.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5748.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5749.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/575.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5750.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5751.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5752.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5753.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5754.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5755.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5756.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5757.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5758.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5759.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/576.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5760.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5761.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5762.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5763.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5764.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5765.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5766.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5767.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5768.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5769.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/577.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5770.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5771.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5772.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5773.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5774.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5775.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5776.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5777.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5778.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5779.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/578.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5780.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5781.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5782.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5783.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5784.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5785.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5786.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5787.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5788.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5789.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/579.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5790.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5791.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5792.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5793.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5794.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5795.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5796.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5797.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5798.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5799.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/58.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/580.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5800.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5801.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5802.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5803.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5804.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5805.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5806.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5807.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5808.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5809.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/581.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5810.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5811.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5812.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5813.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5814.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5815.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5816.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5817.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5818.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5819.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/582.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5820.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5821.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5822.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5823.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5824.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5825.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5826.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5827.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5828.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5829.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/583.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5830.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5831.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5832.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5833.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5834.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5835.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5836.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5837.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5838.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5839.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/584.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5840.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5841.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5842.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5843.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5844.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5845.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5846.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5847.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5848.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5849.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/585.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5850.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5851.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5852.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5853.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5854.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5855.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5856.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5857.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5858.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5859.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/586.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5860.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5861.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5862.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5863.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5864.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5865.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5866.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5867.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5868.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5869.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/587.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5870.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5871.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5872.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5873.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5874.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5875.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5876.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5877.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5878.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5879.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/588.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5880.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5881.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5882.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5883.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5884.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5885.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5886.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5887.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5888.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5889.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/589.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5890.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5891.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5892.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5893.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5894.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5895.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5896.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5897.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5898.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5899.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/59.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/590.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5900.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5901.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5902.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5903.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5904.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5905.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5906.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5907.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5908.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5909.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/591.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5910.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5911.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5912.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5913.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5914.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5915.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5916.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5917.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5918.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5919.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/592.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5920.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5921.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5922.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5923.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5924.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5925.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5926.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5927.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5928.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5929.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/593.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5930.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5931.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5932.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5933.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5934.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5935.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5936.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5937.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5938.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5939.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/594.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5940.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5941.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5942.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5943.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5944.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5945.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5946.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5947.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5948.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5949.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/595.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5950.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5951.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5952.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5953.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5954.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5955.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5956.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5957.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5958.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5959.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/596.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5960.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5961.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5962.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5963.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5964.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5965.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5966.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5967.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5968.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5969.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/597.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5970.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5971.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5972.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5973.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5974.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5975.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5976.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5977.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5978.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5979.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/598.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5980.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5981.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5982.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5983.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5984.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5985.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5986.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5987.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5988.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5989.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/599.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5990.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5991.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5992.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5993.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5994.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5995.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5996.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/5997.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5998.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/5999.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/60.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/600.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6000.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6001.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6002.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6003.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6004.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6005.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6006.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6007.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6008.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6009.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/601.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6010.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6011.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6012.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6013.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6014.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6015.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6016.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6017.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6018.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6019.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/602.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6020.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6021.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6022.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6023.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6024.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6025.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6026.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6027.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6028.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6029.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/603.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6030.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6031.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6032.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6033.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6034.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6035.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6036.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6037.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6038.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6039.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/604.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6040.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6041.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6042.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6043.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6044.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6045.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6046.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6047.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6048.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6049.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/605.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6050.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6051.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6052.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6053.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6054.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6055.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6056.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6057.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6058.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6059.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/606.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6060.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6061.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6062.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6063.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6064.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6065.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6066.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6067.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6068.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6069.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/607.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6070.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6071.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6072.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6073.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6074.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6075.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6076.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6077.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6078.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6079.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/608.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6080.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6081.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6082.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6083.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6084.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6085.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6086.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6087.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6088.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6089.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/609.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6090.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6091.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6092.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6093.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6094.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6095.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6096.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6097.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6098.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6099.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/61.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/610.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6100.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6101.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6102.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6103.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6104.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6105.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6106.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6107.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6108.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6109.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/611.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6110.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6111.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6112.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6113.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6114.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6115.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6116.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6117.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6118.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6119.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/612.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6120.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6121.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6122.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6123.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6124.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6125.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6126.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6127.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6128.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6129.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/613.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6130.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6131.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6132.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6133.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6134.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6135.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6136.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6137.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6138.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6139.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/614.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6140.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6141.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6142.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6143.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6144.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6145.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6146.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6147.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6148.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6149.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/615.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6150.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6151.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6152.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6153.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6154.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6155.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6156.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6157.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6158.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6159.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/616.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6160.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6161.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6162.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6163.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6164.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6165.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6166.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6167.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6168.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6169.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/617.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6170.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6171.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6172.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6173.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6174.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6175.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6176.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6177.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6179.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/618.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6180.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6181.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6182.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6183.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6184.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6185.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6186.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6187.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6188.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6189.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/619.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6190.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6191.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6192.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6193.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6194.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6195.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6196.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6197.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6198.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6199.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/62.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/620.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6200.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6201.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6202.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6203.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6204.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6205.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6206.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6207.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6208.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6209.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/621.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6210.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6211.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6212.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6213.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6214.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6215.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6216.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6217.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6218.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6219.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/622.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6220.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6221.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6222.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6223.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6224.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6225.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6226.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6227.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6228.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6229.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/623.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6230.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6231.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6232.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6233.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6234.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6235.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6236.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6237.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6238.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6239.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/624.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6240.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6241.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6242.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6243.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6244.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6245.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6246.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6247.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6248.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6249.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/625.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6250.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6251.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6252.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6253.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6254.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6255.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6256.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6257.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6258.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6259.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/626.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6260.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6261.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6262.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6263.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6264.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6265.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6266.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6267.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6268.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6269.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/627.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6270.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6271.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6272.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6273.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6274.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6275.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6276.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6277.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6278.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6279.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/628.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6280.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6281.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6282.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6283.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6284.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6285.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6286.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6287.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6288.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6289.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/629.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6290.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6291.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6292.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6293.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6294.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6295.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6296.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6297.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6298.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6299.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/63.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/630.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6300.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6301.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6302.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6303.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6304.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6305.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6306.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6307.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6308.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6309.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/631.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6310.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6311.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6312.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6313.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6314.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6315.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6316.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6317.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6318.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6319.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/632.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6320.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6321.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6322.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6323.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6324.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6325.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6326.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6327.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6328.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6329.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/633.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6330.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6331.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6332.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6333.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6334.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6335.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6336.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6337.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6338.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6339.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/634.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6340.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6341.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6342.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6343.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6344.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6345.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6346.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6347.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6348.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6349.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/635.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6350.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6351.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6352.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6353.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6354.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6355.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6356.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6357.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6358.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6359.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/636.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6360.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6361.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6362.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6363.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6364.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6365.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6366.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6367.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6368.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6369.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/637.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6370.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6371.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6372.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6373.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6374.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6375.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6376.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6377.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6378.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6379.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/638.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6380.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6381.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6382.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6383.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6384.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6385.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6386.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6387.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6388.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6389.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/639.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6390.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6391.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6392.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6393.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6394.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6395.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6396.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6397.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6398.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6399.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/64.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/640.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6400.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6401.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6402.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6403.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6404.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6405.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6406.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6407.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6408.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6409.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/641.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6410.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6411.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6412.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6413.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6414.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6415.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6416.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6417.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6418.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6419.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/642.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6420.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6421.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6422.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6423.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6424.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6425.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6426.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6427.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6428.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6429.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/643.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6430.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6431.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6432.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6433.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6434.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6435.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6436.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6437.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6438.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6439.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/644.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6440.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6441.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6442.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6443.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6444.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6445.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6446.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6447.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6448.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6449.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/645.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6450.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6451.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6452.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6453.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6454.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6455.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6456.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6457.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6458.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6459.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/646.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6460.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6461.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6462.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6463.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6464.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6465.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6466.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6467.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6468.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6469.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/647.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6470.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6471.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6472.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6473.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6474.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6475.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6476.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6477.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6478.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6479.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/648.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6480.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6481.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6482.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6483.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6484.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6485.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6486.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6487.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6488.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6489.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/649.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6490.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6491.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6492.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6493.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6495.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6496.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6497.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6498.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6499.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/65.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/650.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6500.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6501.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6502.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6503.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6504.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6505.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6506.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6507.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6508.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6509.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/651.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6510.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6511.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6512.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6513.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6514.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6515.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6516.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6517.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6518.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6519.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/652.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6520.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6521.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6522.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6523.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6524.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6525.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6526.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6527.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6528.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6529.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/653.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6530.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6531.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6532.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6533.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6534.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6535.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6536.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6537.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6538.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6539.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/654.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6540.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6541.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6542.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6543.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6544.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6545.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6546.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6547.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6548.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6549.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/655.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6550.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6551.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6552.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6553.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6554.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6555.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6556.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6557.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6558.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6559.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/656.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6560.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6561.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6562.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6563.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6564.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6565.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6566.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6567.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6568.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6569.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/657.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6570.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6571.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6572.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6573.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6574.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6575.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6576.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6577.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6578.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6579.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/658.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6580.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6581.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6582.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6583.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6584.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6585.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6586.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6587.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6588.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6589.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/659.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6590.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6591.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6592.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6593.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6594.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6595.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6596.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6597.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6598.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6599.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/66.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/660.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6600.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6601.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6602.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6603.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6604.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6605.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6606.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6607.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6608.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6609.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/661.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6610.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6611.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6612.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6613.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6614.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6615.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6616.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6617.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6618.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6619.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/662.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6620.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6621.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6622.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6623.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6624.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6625.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6626.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6627.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6628.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6629.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/663.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6630.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6631.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6632.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6633.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6634.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6635.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6636.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6637.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6638.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6639.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/664.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6640.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6641.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6642.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6643.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6644.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6645.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6646.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6647.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6648.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6649.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/665.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6650.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6651.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6652.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6653.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6654.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6655.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6656.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6657.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6658.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6659.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/666.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6660.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6661.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6662.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6663.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6664.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6665.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6666.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6667.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6668.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6669.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/667.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6670.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6671.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6672.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6673.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6674.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6675.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6676.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6677.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6678.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6679.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/668.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6680.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6681.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6682.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6683.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6684.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6685.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6686.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6687.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6688.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6689.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/669.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6690.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6691.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6692.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6693.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6694.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6695.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6696.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6697.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6698.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6699.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/67.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/670.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6700.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6701.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6702.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6703.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6704.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6705.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6706.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6707.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6708.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6709.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/671.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6710.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6711.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6712.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6713.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6714.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6715.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6716.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6717.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6718.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6719.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/672.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6720.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6721.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6722.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6723.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6724.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6725.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6726.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6727.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6728.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6729.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/673.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6730.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6731.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6732.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6733.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6734.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6735.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6736.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6737.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6738.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6739.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/674.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6740.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6741.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6742.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6743.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6744.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6745.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6746.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6747.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6748.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6749.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/675.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6750.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6751.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6752.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6753.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6754.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6755.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6756.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6757.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6758.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6759.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/676.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6760.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6761.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6762.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6763.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6764.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6765.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6766.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6767.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6768.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6769.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/677.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6770.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6771.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6772.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6773.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6774.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6775.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6776.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6777.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6778.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6779.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/678.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6780.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6781.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6782.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6783.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6784.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6785.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6786.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6787.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6788.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6789.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/679.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6790.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6791.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6792.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6793.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6794.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6795.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6796.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6797.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6798.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6799.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/68.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/680.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6800.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6801.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6802.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6803.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6804.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6805.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6806.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6807.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6808.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6809.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/681.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6810.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6811.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6812.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6813.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6814.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6815.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6816.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6817.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6818.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6819.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/682.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6820.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6821.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6822.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6823.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6824.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6825.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6826.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6827.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6828.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6829.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/683.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6830.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6831.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6832.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6833.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6834.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6835.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6836.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6837.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6838.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6839.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/684.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6840.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6841.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6842.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6843.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6844.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6845.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6846.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6847.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6848.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6849.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/685.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6850.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6851.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6852.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6853.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6854.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6855.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6856.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6857.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6858.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6859.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/686.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6860.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6861.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6862.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6863.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6864.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6865.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6866.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6867.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6868.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6869.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/687.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6870.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6871.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6872.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6873.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6874.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6875.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6876.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6877.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6878.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6879.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/688.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6880.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6881.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6882.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6883.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6884.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6885.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6886.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6887.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6888.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6889.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/689.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6890.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6891.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6892.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6893.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6894.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6895.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6896.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6897.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6898.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6899.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/69.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/690.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6900.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6901.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6902.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6903.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6904.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6905.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6906.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6907.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6908.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6909.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/691.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6910.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6911.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6912.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6913.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6914.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6915.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6916.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6917.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6918.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6919.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/692.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6920.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6921.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6922.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6923.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6924.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6925.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6926.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6927.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6928.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6929.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/693.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6930.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6931.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6932.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6933.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6934.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6935.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6936.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6937.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6938.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6939.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/694.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6940.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6941.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6942.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6943.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6944.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6945.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6946.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6947.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6948.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6949.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/695.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6950.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6951.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6952.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6953.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6954.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6955.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6956.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6957.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6958.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6959.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/696.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6960.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6961.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6962.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6963.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6964.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6965.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6966.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6967.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6968.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6969.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/697.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6970.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6971.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6972.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6973.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6974.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6975.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6976.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6977.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6978.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6979.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/698.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6980.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6981.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6982.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6983.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6984.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6985.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6986.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6987.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6988.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6989.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/699.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6990.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6991.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6992.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6993.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6994.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6995.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6996.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/6997.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6998.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/6999.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/7.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/70.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/700.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7000.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7001.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7002.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7003.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7004.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7005.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7006.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7007.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7008.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7009.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/701.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7010.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7011.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7012.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7013.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7014.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7015.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7016.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7017.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7018.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7019.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/702.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7020.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7021.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7022.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7023.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7024.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7025.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7026.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7027.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7028.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7029.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/703.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7030.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7031.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7032.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7033.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7034.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7035.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7036.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7037.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7038.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7039.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/704.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7040.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7041.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7042.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7043.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7044.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7045.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7046.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7047.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7048.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7049.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/705.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7050.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7051.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7052.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7053.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7054.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7055.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7056.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7057.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7058.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7059.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/706.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7060.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7061.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7062.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7063.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7064.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7065.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7066.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7067.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7068.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7069.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/707.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7070.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7071.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7072.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7073.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7074.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7075.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7076.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7077.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7078.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7079.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/708.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7080.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7081.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7082.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7083.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7084.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7085.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7086.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7087.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7088.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7089.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/709.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7090.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7091.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7092.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7093.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7094.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7095.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7096.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7097.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7098.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7099.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/71.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/710.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7100.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7101.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7102.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7103.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7104.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7105.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7106.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7107.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7108.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7109.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/711.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7110.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7111.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7112.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7113.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7114.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7115.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7116.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7117.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7118.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7119.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/712.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7120.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7121.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7122.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7123.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7124.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7125.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7126.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7127.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7128.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7129.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/713.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7130.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7131.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7132.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7133.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7134.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7135.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7136.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7137.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7138.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7139.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/714.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7140.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7141.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7142.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7143.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7144.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7145.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7146.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7147.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7148.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7149.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/715.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7150.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7151.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7152.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7153.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7154.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7155.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7156.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7157.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7158.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7159.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/716.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7160.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7161.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7162.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7163.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7164.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7165.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7166.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7167.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7168.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7169.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/717.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7170.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7171.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7172.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7173.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7174.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7175.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7176.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7177.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7178.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7179.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/718.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7180.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7181.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7182.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7183.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7184.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7185.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7186.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7187.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7188.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7189.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/719.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7190.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7191.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7192.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7193.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7194.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7195.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7196.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7197.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7198.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7199.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/72.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/720.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7200.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7201.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7202.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7203.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7204.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7205.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7206.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7207.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7208.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7209.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/721.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7210.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7211.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7212.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7213.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7214.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7215.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7216.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7217.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7218.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7219.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/722.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7220.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7221.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7222.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7223.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7224.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7225.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7226.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7227.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7228.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7229.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/723.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7230.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7231.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7232.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7233.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7234.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7235.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7236.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7237.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7238.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7239.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/724.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7240.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7241.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7242.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7243.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7244.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7245.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7246.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7247.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7248.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7249.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/725.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7250.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7251.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7252.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7253.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7254.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7255.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7256.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7257.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7258.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7259.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/726.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7260.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7261.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7262.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7263.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7264.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7265.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7266.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7267.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7268.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7269.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/727.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7270.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7271.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7272.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7273.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7274.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7275.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7276.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7277.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7278.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7279.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/728.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7280.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7281.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7282.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7283.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7284.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7285.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7286.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7287.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7288.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7289.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/729.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7290.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7291.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7292.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7293.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7294.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7295.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7296.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7297.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7298.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7299.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/73.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/730.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7300.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7301.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7302.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7303.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7304.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7305.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7306.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7307.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7308.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7309.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/731.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7310.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7311.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7312.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7313.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7314.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7315.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7316.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7317.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7318.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7319.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/732.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7320.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7321.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7322.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7323.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7324.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7325.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7326.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7327.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7328.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7329.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/733.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7330.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7331.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7332.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7333.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7334.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7335.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7336.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7337.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7338.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7339.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/734.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7340.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7341.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7342.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7343.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7344.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7345.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7346.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7347.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7348.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7349.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/735.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7350.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7351.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7352.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7353.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7354.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7355.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7356.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7357.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7358.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7359.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/736.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7360.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7361.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7362.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7363.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7364.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7365.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7366.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7367.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7368.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7369.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/737.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7370.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7371.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7372.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7373.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7374.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7375.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7376.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7377.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7378.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7379.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/738.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7380.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7381.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7382.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7383.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7384.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7385.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7386.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7387.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7388.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7389.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/739.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7390.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7391.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7392.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7393.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7394.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7395.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7396.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7397.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7398.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7399.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/74.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/740.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7400.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7401.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7402.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7403.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7404.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7405.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7406.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7407.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7408.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7409.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/741.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7410.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7411.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7412.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7413.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7414.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7415.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7416.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7417.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7418.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7419.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/742.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7420.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7421.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7422.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7423.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7424.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7425.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7426.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7427.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7428.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7429.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/743.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7430.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7431.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7432.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7433.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7434.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7435.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7436.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7437.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7438.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7439.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/744.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7440.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7441.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7442.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7443.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7444.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7445.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7446.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7447.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7448.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7449.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/745.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7450.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7451.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7452.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7453.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7454.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7455.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7456.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7457.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7458.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7459.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/746.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7460.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7461.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7462.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7463.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7464.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7465.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7466.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7467.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7468.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7469.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/747.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7470.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7471.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7472.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7473.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7474.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7475.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7476.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7477.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7478.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7479.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/748.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7480.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7481.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7482.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7483.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7484.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7485.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7486.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7487.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7488.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7489.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/749.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7490.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7491.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7492.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7493.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7494.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7495.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7496.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7497.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7498.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/7499.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/75.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/750.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/751.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/752.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/753.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/754.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/755.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/756.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/757.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/758.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/759.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/76.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/760.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/761.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/762.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/763.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/764.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/765.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/766.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/767.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/768.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/769.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/77.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/770.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/771.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/772.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/773.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/774.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/775.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/776.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/777.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/778.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/779.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/78.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/780.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/781.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/782.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/783.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/784.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/785.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/786.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/787.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/788.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/789.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/79.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/790.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/791.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/792.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/793.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/794.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/795.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/796.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/797.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/798.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/799.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/8.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/80.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/800.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/801.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/802.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/803.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/804.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/805.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/806.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/807.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/808.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/809.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/81.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/810.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/811.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/812.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/813.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/814.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/815.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/816.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/817.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/818.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/819.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/82.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/820.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/821.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/822.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/823.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/824.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/825.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/826.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/827.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/828.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/829.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/83.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/830.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/831.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/832.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/833.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/834.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/835.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/836.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/837.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/838.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/839.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/84.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/840.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/841.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/842.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/843.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/844.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/845.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/846.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/847.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/848.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/849.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/85.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/850.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/851.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/852.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/853.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/854.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/855.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/856.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/857.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/858.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/859.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/86.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/860.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/861.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/862.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/863.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/864.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/865.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/866.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/867.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/868.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/869.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/87.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/870.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/871.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/872.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/873.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/874.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/875.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/876.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/877.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/878.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/879.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/88.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/880.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/881.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/882.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/883.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/884.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/885.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/886.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/887.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/888.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/889.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/89.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/890.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/891.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/892.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/893.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/894.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/895.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/896.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/897.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/898.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/899.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/9.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/90.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/900.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/901.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/902.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/903.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/904.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/905.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/906.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/907.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/908.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/909.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/91.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/910.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/911.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/912.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/913.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/914.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/915.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/916.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/917.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/918.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/91842.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91843.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91844.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91845.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91846.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91847.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91848.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91849.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91850.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91851.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91852.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91853.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91854.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91855.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91856.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91857.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91858.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91859.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91860.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91861.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91862.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91863.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91864.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91865.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91866.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91867.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91868.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91869.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91870.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91871.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91872.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91873.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91874.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91875.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91876.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91877.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91878.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91879.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91880.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91881.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91882.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91883.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91884.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91885.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91886.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91887.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91888.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91889.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91890.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91891.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91892.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91893.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91894.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91895.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91896.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91897.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91898.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91899.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/919.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/91900.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91901.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91902.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91903.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91904.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91905.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91906.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91907.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91908.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91909.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91910.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91911.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91912.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91913.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91914.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91915.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91916.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91917.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91918.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91919.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91920.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91921.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/91922.jpg  \n",
            "  inflating: IndoFashionDataset/images/val/92.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/920.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/921.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/922.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/923.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/924.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/925.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/926.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/927.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/928.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/929.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/93.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/930.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/931.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/932.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/933.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/934.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/935.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/936.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/937.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/938.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/939.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/94.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/940.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/941.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/942.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/943.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/944.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/945.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/946.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/947.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/948.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/949.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/95.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/950.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/951.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/952.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/953.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/954.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/955.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/956.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/957.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/958.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/959.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/96.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/960.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/961.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/962.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/963.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/964.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/965.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/966.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/967.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/968.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/969.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/97.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/970.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/971.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/972.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/973.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/974.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/975.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/976.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/977.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/978.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/979.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/98.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/980.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/981.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/982.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/983.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/984.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/985.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/986.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/987.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/988.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/989.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/99.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/990.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/991.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/992.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/993.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/994.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/995.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/996.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/997.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/998.jpeg  \n",
            "  inflating: IndoFashionDataset/images/val/999.jpeg  \n"
          ]
        }
      ],
      "source": [
        "!unzip drive/My\\ Drive/IndoFashionDataset.zip -d IndoFashionDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwQVz5kx_yl3",
        "outputId": "2af54195-9beb-4df2-c2fa-10aadf0aa8e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34mIndoFashion\u001b[0m/  \u001b[01;34mIndoFashionDataset\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT2htq51_psU"
      },
      "outputs": [],
      "source": [
        "rm -r /content/IndoFashionDataset/annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWC6_lL-_05h",
        "outputId": "68ba8201-9a2b-4459-ed5f-77fcfc8d92ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/annotations.zip\n",
            "  inflating: /content/IndoFashionDataset/annotations/test_data.json  \n",
            "  inflating: /content/IndoFashionDataset/annotations/train_data.json  \n",
            "  inflating: /content/IndoFashionDataset/annotations/val_data.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip drive/My\\ Drive/annotations.zip -d /content/IndoFashionDataset/annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsdHFD1EfKle"
      },
      "outputs": [],
      "source": [
        "rm -r /content/IndoFashion/utils/common_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esN33r6kfY9G",
        "outputId": "7dc710f2-2a4c-4556-b106-73356fdced5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/common_utils.zip\n",
            "  inflating: /content/IndoFashion/utils/common_utils.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip drive/My\\ Drive/common_utils.zip -d /content/IndoFashion/utils/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_CF9XAta9Cm",
        "outputId": "72d20cd7-14bb-4dd5-e09e-fbaa623945e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/models.zip\n",
            "  inflating: /content/IndoFashion/iew_r101.pt  \n",
            "  inflating: /content/IndoFashion/iew_r101_flip.pt  "
          ]
        }
      ],
      "source": [
        "!unzip drive/My\\ Drive/models.zip -d /content/IndoFashion/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJTaFL7WaJYc"
      },
      "outputs": [],
      "source": [
        "rm -r /content/IndoFashion/iew_r101.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwySVi3xaP_K"
      },
      "outputs": [],
      "source": [
        "rm -r /content/IndoFashion/iew_r101_flip.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2Dk1QDOarpw",
        "outputId": "23ca6b26-8da8-4ee3-fe87-97f3b760ffcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/models.zip\n",
            "  inflating: /content/IndoFashion/models/iew_r101.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r101_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r101_jitter.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r101_jitter_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r18.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r18_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r18_jitter.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r18_jitter_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r50.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r50_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r50_jitter.pt  \n",
            "  inflating: /content/IndoFashion/models/iew_r50_jitter_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r101_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r101_jitter.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r101_jitter_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r101_noaug.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r18_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r18_jitter.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r18_jitter_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r18_noaug.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r50_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r50_jitter.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r50_jitter_flip.pt  \n",
            "  inflating: /content/IndoFashion/models/indof_r50_noaug.pt  \n"
          ]
        }
      ],
      "source": [
        "!unzip drive/My\\ Drive/models.zip -d /content/IndoFashion/models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWN3hYZjgEGj",
        "outputId": "2ca7ca73-aa43-4b76-e2e0-e65dc167936a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/img_trainer_files.zip\n",
            "  inflating: /content/IndoFashion/img_trainer_flip.py  \n",
            "  inflating: /content/IndoFashion/img_trainer_jitter.py  \n",
            "  inflating: /content/IndoFashion/img_trainer_no_augmentation.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip drive/My\\ Drive/img_trainer_files.zip -d /content/IndoFashion/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKYLwvzOYfWg",
        "outputId": "300635c0-cce5-4401-d291-36e95afadbdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/img_trainer_flip_jitter.zip\n",
            "  inflating: /content/IndoFashion/img_trainer_flip_jitter.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip drive/My\\ Drive/img_trainer_flip_jitter.zip -d /content/IndoFashion/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J7rOS77_4-N",
        "outputId": "74c825b4-1761-47d3-9c4e-94005343ea92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-07 04:02:34.690096: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-07 04:02:34.736749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-07 04:02:35.585021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [04:09<00:00, 11.88it/s]\n",
            " Train Epoch: 1 Loss: 0.8739 Acc: 0.72 \n",
            "100% 245/245 [00:09<00:00, 26.07it/s]\n",
            " Val Epoch: 1 Avg loss: 0.6738 Acc: 0.77\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:06<00:00, 11.98it/s]\n",
            " Train Epoch: 2 Loss: 0.6022 Acc: 0.81 \n",
            "100% 245/245 [00:09<00:00, 26.51it/s]\n",
            " Val Epoch: 2 Avg loss: 0.6017 Acc: 0.81\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:04<00:00, 12.09it/s]\n",
            " Train Epoch: 3 Loss: 0.5214 Acc: 0.84 \n",
            "100% 245/245 [00:09<00:00, 26.43it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5617 Acc: 0.82\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:06<00:00, 12.03it/s]\n",
            " Train Epoch: 4 Loss: 0.4712 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.49it/s]\n",
            " Val Epoch: 4 Avg loss: 0.4770 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:07<00:00, 11.95it/s]\n",
            " Train Epoch: 5 Loss: 0.4349 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.56it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4698 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:07<00:00, 11.97it/s]\n",
            " Train Epoch: 6 Loss: 0.4048 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4692 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:06<00:00, 12.02it/s]\n",
            " Train Epoch: 7 Loss: 0.3826 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.33it/s]\n",
            " Val Epoch: 7 Avg loss: 0.4816 Acc: 0.84\n",
            "100% 2959/2959 [04:06<00:00, 12.02it/s]\n",
            " Train Epoch: 8 Loss: 0.3582 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.42it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4383 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:04<00:00, 12.08it/s]\n",
            " Train Epoch: 9 Loss: 0.3386 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 9 Avg loss: 0.4304 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:07<00:00, 11.98it/s]\n",
            " Train Epoch: 10 Loss: 0.3192 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.45it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4530 Acc: 0.86\n",
            "100% 2959/2959 [04:05<00:00, 12.06it/s]\n",
            " Train Epoch: 11 Loss: 0.3047 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.17it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4421 Acc: 0.86\n",
            "100% 2959/2959 [04:06<00:00, 12.01it/s]\n",
            " Train Epoch: 12 Loss: 0.2897 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.38it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4239 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:07<00:00, 11.95it/s]\n",
            " Train Epoch: 13 Loss: 0.2749 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.45it/s]\n",
            " Val Epoch: 13 Avg loss: 0.4061 Acc: 0.87\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:05<00:00, 12.04it/s]\n",
            " Train Epoch: 14 Loss: 0.2598 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.46it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4604 Acc: 0.86\n",
            "100% 2959/2959 [04:06<00:00, 12.00it/s]\n",
            " Train Epoch: 15 Loss: 0.2462 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.24it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4210 Acc: 0.87\n",
            "100% 2959/2959 [04:05<00:00, 12.05it/s]\n",
            " Train Epoch: 16 Loss: 0.2348 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.31it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4356 Acc: 0.87\n",
            "100% 2959/2959 [04:05<00:00, 12.06it/s]\n",
            " Train Epoch: 17 Loss: 0.2243 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.50it/s]\n",
            " Val Epoch: 17 Avg loss: 0.4466 Acc: 0.86\n",
            "100% 2959/2959 [04:05<00:00, 12.06it/s]\n",
            " Train Epoch: 18 Loss: 0.2134 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 18 Avg loss: 0.4308 Acc: 0.87\n",
            "100% 2959/2959 [04:08<00:00, 11.92it/s]\n",
            " Train Epoch: 19 Loss: 0.2048 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.63it/s]\n",
            " Val Epoch: 19 Avg loss: 0.4853 Acc: 0.86\n",
            "100% 2959/2959 [04:07<00:00, 11.96it/s]\n",
            " Train Epoch: 20 Loss: 0.1954 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.52it/s]\n",
            " Val Epoch: 20 Avg loss: 0.4612 Acc: 0.86\n",
            "100% 2959/2959 [04:07<00:00, 11.96it/s]\n",
            " Train Epoch: 21 Loss: 0.1886 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.26it/s]\n",
            " Val Epoch: 21 Avg loss: 0.4844 Acc: 0.86\n",
            "100% 2959/2959 [04:05<00:00, 12.06it/s]\n",
            " Train Epoch: 22 Loss: 0.1795 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.31it/s]\n",
            " Val Epoch: 22 Avg loss: 0.4856 Acc: 0.87\n",
            "100% 2959/2959 [04:06<00:00, 11.98it/s]\n",
            " Train Epoch: 23 Loss: 0.1719 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.50it/s]\n",
            " Val Epoch: 23 Avg loss: 0.4794 Acc: 0.87\n",
            "100% 2959/2959 [04:08<00:00, 11.91it/s]\n",
            " Train Epoch: 24 Loss: 0.1674 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.39it/s]\n",
            " Val Epoch: 24 Avg loss: 0.4760 Acc: 0.87\n",
            "100% 2959/2959 [04:07<00:00, 11.93it/s]\n",
            " Train Epoch: 25 Loss: 0.1609 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.41it/s]\n",
            " Val Epoch: 25 Avg loss: 0.5038 Acc: 0.86\n",
            "100% 2959/2959 [04:07<00:00, 11.93it/s]\n",
            " Train Epoch: 26 Loss: 0.1553 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.41it/s]\n",
            " Val Epoch: 26 Avg loss: 0.5278 Acc: 0.86\n",
            "100% 2959/2959 [04:05<00:00, 12.05it/s]\n",
            " Train Epoch: 27 Loss: 0.1495 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 27 Avg loss: 0.5416 Acc: 0.87\n",
            "100% 2959/2959 [04:05<00:00, 12.06it/s]\n",
            " Train Epoch: 28 Loss: 0.1459 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 28 Avg loss: 0.5414 Acc: 0.86\n",
            "100% 2959/2959 [04:08<00:00, 11.92it/s]\n",
            " Train Epoch: 29 Loss: 0.1396 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.41it/s]\n",
            " Val Epoch: 29 Avg loss: 0.5276 Acc: 0.86\n",
            "100% 2959/2959 [04:07<00:00, 11.96it/s]\n",
            " Train Epoch: 30 Loss: 0.1382 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.38it/s]\n",
            " Val Epoch: 30 Avg loss: 0.5804 Acc: 0.86\n",
            "100% 2959/2959 [04:05<00:00, 12.04it/s]\n",
            " Train Epoch: 31 Loss: 0.1323 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.11it/s]\n",
            " Val Epoch: 31 Avg loss: 0.5502 Acc: 0.86\n",
            "100% 2959/2959 [04:07<00:00, 11.94it/s]\n",
            " Train Epoch: 32 Loss: 0.1325 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.48it/s]\n",
            " Val Epoch: 32 Avg loss: 0.5227 Acc: 0.87\n",
            "100% 2959/2959 [04:07<00:00, 11.98it/s]\n",
            " Train Epoch: 33 Loss: 0.1292 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 33 Avg loss: 0.5541 Acc: 0.86\n",
            "100% 2959/2959 [04:05<00:00, 12.07it/s]\n",
            " Train Epoch: 34 Loss: 0.1255 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 34 Avg loss: 0.5511 Acc: 0.87\n",
            "100% 2959/2959 [04:05<00:00, 12.03it/s]\n",
            " Train Epoch: 35 Loss: 0.1211 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.46it/s]\n",
            " Val Epoch: 35 Avg loss: 0.5450 Acc: 0.87\n",
            "100% 2959/2959 [04:06<00:00, 11.99it/s]\n",
            " Train Epoch: 36 Loss: 0.1190 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 36 Avg loss: 0.5610 Acc: 0.87\n",
            "100% 2959/2959 [04:06<00:00, 12.02it/s]\n",
            " Train Epoch: 37 Loss: 0.1177 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.18it/s]\n",
            " Val Epoch: 37 Avg loss: 0.5918 Acc: 0.86\n",
            "100% 2959/2959 [04:07<00:00, 11.96it/s]\n",
            " Train Epoch: 38 Loss: 0.1166 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.43it/s]\n",
            " Val Epoch: 38 Avg loss: 0.5816 Acc: 0.86\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer.py -m train -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOgzPytnO3N7",
        "outputId": "342a780a-ba5d-4b6f-b3d1-dbec92469572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-07 07:27:00.127480: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-07 07:27:00.173442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-07 07:27:01.030814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.17it/s]\n",
            "Test Acc: 0.8832\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer.py -m test --model_name indof_r50 -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ5iPtYEQJbY",
        "outputId": "6cfa9d2e-a1a4-49ee-f714-a2c86f2f625d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-07 07:29:44.215465: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-07 07:29:44.262857: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-07 07:29:45.134470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 11698746\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [04:07<00:00, 11.96it/s]\n",
            " Train Epoch: 1 Loss: 0.7536 Acc: 0.76 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 1 Avg loss: 0.6676 Acc: 0.79\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:08<00:00, 11.88it/s]\n",
            " Train Epoch: 2 Loss: 0.5476 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 25.54it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5893 Acc: 0.81\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:07<00:00, 11.93it/s]\n",
            " Train Epoch: 3 Loss: 0.4783 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.36it/s]\n",
            " Val Epoch: 3 Avg loss: 0.4884 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:08<00:00, 11.93it/s]\n",
            " Train Epoch: 4 Loss: 0.4311 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 25.54it/s]\n",
            " Val Epoch: 4 Avg loss: 0.4635 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:07<00:00, 11.96it/s]\n",
            " Train Epoch: 5 Loss: 0.3951 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 25.71it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4741 Acc: 0.85\n",
            "100% 2959/2959 [04:09<00:00, 11.87it/s]\n",
            " Train Epoch: 6 Loss: 0.3659 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 25.89it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4428 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:09<00:00, 11.86it/s]\n",
            " Train Epoch: 7 Loss: 0.3388 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 25.91it/s]\n",
            " Val Epoch: 7 Avg loss: 0.4469 Acc: 0.86\n",
            "100% 2959/2959 [04:07<00:00, 11.95it/s]\n",
            " Train Epoch: 8 Loss: 0.3154 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 25.23it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4411 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:10<00:00, 11.82it/s]\n",
            " Train Epoch: 9 Loss: 0.2958 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 25.66it/s]\n",
            " Val Epoch: 9 Avg loss: 0.4393 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:11<00:00, 11.77it/s]\n",
            " Train Epoch: 10 Loss: 0.2771 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.05it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4479 Acc: 0.87\n",
            "100% 2959/2959 [04:10<00:00, 11.83it/s]\n",
            " Train Epoch: 11 Loss: 0.2608 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.09it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4508 Acc: 0.86\n",
            "100% 2959/2959 [04:09<00:00, 11.88it/s]\n",
            " Train Epoch: 12 Loss: 0.2450 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4554 Acc: 0.86\n",
            "100% 2959/2959 [04:10<00:00, 11.81it/s]\n",
            " Train Epoch: 13 Loss: 0.2297 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 13 Avg loss: 0.4544 Acc: 0.87\n",
            "100% 2959/2959 [04:09<00:00, 11.84it/s]\n",
            " Train Epoch: 14 Loss: 0.2172 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4793 Acc: 0.86\n",
            "100% 2959/2959 [04:08<00:00, 11.89it/s]\n",
            " Train Epoch: 15 Loss: 0.2047 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4854 Acc: 0.86\n",
            "100% 2959/2959 [04:08<00:00, 11.89it/s]\n",
            " Train Epoch: 16 Loss: 0.1953 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 25.96it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4885 Acc: 0.87\n",
            "100% 2959/2959 [04:10<00:00, 11.82it/s]\n",
            " Train Epoch: 17 Loss: 0.1865 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.17it/s]\n",
            " Val Epoch: 17 Avg loss: 0.5119 Acc: 0.87\n",
            "100% 2959/2959 [04:11<00:00, 11.78it/s]\n",
            " Train Epoch: 18 Loss: 0.1793 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 18 Avg loss: 0.5355 Acc: 0.86\n",
            "100% 2959/2959 [04:08<00:00, 11.92it/s]\n",
            " Train Epoch: 19 Loss: 0.1704 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.10it/s]\n",
            " Val Epoch: 19 Avg loss: 0.4887 Acc: 0.87\n",
            "100% 2959/2959 [04:11<00:00, 11.77it/s]\n",
            " Train Epoch: 20 Loss: 0.1647 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.14it/s]\n",
            " Val Epoch: 20 Avg loss: 0.5504 Acc: 0.86\n",
            "100% 2959/2959 [04:10<00:00, 11.80it/s]\n",
            " Train Epoch: 21 Loss: 0.1583 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.51it/s]\n",
            " Val Epoch: 21 Avg loss: 0.5689 Acc: 0.86\n",
            "100% 2959/2959 [04:09<00:00, 11.85it/s]\n",
            " Train Epoch: 22 Loss: 0.1520 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.98it/s]\n",
            " Val Epoch: 22 Avg loss: 0.5338 Acc: 0.87\n",
            "100% 2959/2959 [04:10<00:00, 11.80it/s]\n",
            " Train Epoch: 23 Loss: 0.1457 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.58it/s]\n",
            " Val Epoch: 23 Avg loss: 0.5441 Acc: 0.87\n",
            "100% 2959/2959 [04:08<00:00, 11.90it/s]\n",
            " Train Epoch: 24 Loss: 0.1421 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.81it/s]\n",
            " Val Epoch: 24 Avg loss: 0.5712 Acc: 0.86\n",
            "100% 2959/2959 [04:10<00:00, 11.81it/s]\n",
            " Train Epoch: 25 Loss: 0.1358 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.65it/s]\n",
            " Val Epoch: 25 Avg loss: 0.5991 Acc: 0.86\n",
            "100% 2959/2959 [04:09<00:00, 11.85it/s]\n",
            " Train Epoch: 26 Loss: 0.1333 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 26 Avg loss: 0.6455 Acc: 0.86\n",
            "100% 2959/2959 [04:10<00:00, 11.82it/s]\n",
            " Train Epoch: 27 Loss: 0.1276 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.55it/s]\n",
            " Val Epoch: 27 Avg loss: 0.6186 Acc: 0.86\n",
            "100% 2959/2959 [04:07<00:00, 11.93it/s]\n",
            " Train Epoch: 28 Loss: 0.1245 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.10it/s]\n",
            " Val Epoch: 28 Avg loss: 0.6335 Acc: 0.86\n",
            "100% 2959/2959 [04:11<00:00, 11.78it/s]\n",
            " Train Epoch: 29 Loss: 0.1222 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.07it/s]\n",
            " Val Epoch: 29 Avg loss: 0.6483 Acc: 0.86\n",
            "100% 2959/2959 [04:10<00:00, 11.81it/s]\n",
            " Train Epoch: 30 Loss: 0.1188 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 30 Avg loss: 0.6553 Acc: 0.86\n",
            "100% 2959/2959 [04:08<00:00, 11.90it/s]\n",
            " Train Epoch: 31 Loss: 0.1158 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.25it/s]\n",
            " Val Epoch: 31 Avg loss: 0.6272 Acc: 0.86\n",
            "100% 2959/2959 [04:08<00:00, 11.90it/s]\n",
            " Train Epoch: 32 Loss: 0.1150 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.23it/s]\n",
            " Val Epoch: 32 Avg loss: 0.6667 Acc: 0.86\n",
            "100% 2959/2959 [04:10<00:00, 11.83it/s]\n",
            " Train Epoch: 33 Loss: 0.1092 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.30it/s]\n",
            " Val Epoch: 33 Avg loss: 0.6799 Acc: 0.86\n",
            "100% 2959/2959 [04:08<00:00, 11.92it/s]\n",
            " Train Epoch: 34 Loss: 0.1090 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.91it/s]\n",
            " Val Epoch: 34 Avg loss: 0.6421 Acc: 0.86\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer.py -m train -a resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCGjVkSBych2",
        "outputId": "992371ea-93ce-40b3-8901-a8cf0b1d4450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-07 09:59:18.127227: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-07 09:59:18.174448: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-07 09:59:19.042376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 11698746\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.28it/s]\n",
            "Test Acc: 0.8745\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer.py -m test --model_name indof_r18 -a resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15OJVV3cy7j6",
        "outputId": "1a7eeb44-f2d7-45a7-f4a3-038d76b9e60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-07 10:01:38.784617: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-07 10:01:38.832065: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-07 10:01:39.731228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 44586042\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [04:42<00:00, 10.48it/s]\n",
            " Train Epoch: 1 Loss: 0.9060 Acc: 0.71 \n",
            "100% 245/245 [00:09<00:00, 26.32it/s]\n",
            " Val Epoch: 1 Avg loss: 0.8171 Acc: 0.73\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:43<00:00, 10.46it/s]\n",
            " Train Epoch: 2 Loss: 0.6146 Acc: 0.81 \n",
            "100% 245/245 [00:09<00:00, 26.20it/s]\n",
            " Val Epoch: 2 Avg loss: 0.6225 Acc: 0.80\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:42<00:00, 10.46it/s]\n",
            " Train Epoch: 3 Loss: 0.5297 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5461 Acc: 0.82\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:43<00:00, 10.43it/s]\n",
            " Train Epoch: 4 Loss: 0.4753 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 4 Avg loss: 0.5122 Acc: 0.83\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:41<00:00, 10.49it/s]\n",
            " Train Epoch: 5 Loss: 0.4420 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 25.69it/s]\n",
            " Val Epoch: 5 Avg loss: 0.5163 Acc: 0.83\n",
            "100% 2959/2959 [04:41<00:00, 10.52it/s]\n",
            " Train Epoch: 6 Loss: 0.4109 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4714 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:42<00:00, 10.47it/s]\n",
            " Train Epoch: 7 Loss: 0.3866 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.18it/s]\n",
            " Val Epoch: 7 Avg loss: 0.4564 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:43<00:00, 10.44it/s]\n",
            " Train Epoch: 8 Loss: 0.3644 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.05it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4340 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:41<00:00, 10.50it/s]\n",
            " Train Epoch: 9 Loss: 0.3434 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 9 Avg loss: 0.4523 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.50it/s]\n",
            " Train Epoch: 10 Loss: 0.3267 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 25.95it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4963 Acc: 0.84\n",
            "100% 2959/2959 [04:42<00:00, 10.47it/s]\n",
            " Train Epoch: 11 Loss: 0.3096 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.10it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4161 Acc: 0.87\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:41<00:00, 10.53it/s]\n",
            " Train Epoch: 12 Loss: 0.2924 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 25.94it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4319 Acc: 0.86\n",
            "100% 2959/2959 [04:40<00:00, 10.54it/s]\n",
            " Train Epoch: 13 Loss: 0.2817 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.00it/s]\n",
            " Val Epoch: 13 Avg loss: 0.4128 Acc: 0.87\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:41<00:00, 10.53it/s]\n",
            " Train Epoch: 14 Loss: 0.2648 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 25.99it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4509 Acc: 0.86\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 15 Loss: 0.2517 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.38it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4477 Acc: 0.86\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 16 Loss: 0.2404 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 25.89it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4334 Acc: 0.86\n",
            "100% 2959/2959 [04:42<00:00, 10.47it/s]\n",
            " Train Epoch: 17 Loss: 0.2276 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.00it/s]\n",
            " Val Epoch: 17 Avg loss: 0.4501 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.52it/s]\n",
            " Train Epoch: 18 Loss: 0.2186 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 25.31it/s]\n",
            " Val Epoch: 18 Avg loss: 0.4546 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.50it/s]\n",
            " Train Epoch: 19 Loss: 0.2079 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 25.84it/s]\n",
            " Val Epoch: 19 Avg loss: 0.4781 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.51it/s]\n",
            " Train Epoch: 20 Loss: 0.1980 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 20 Avg loss: 0.4407 Acc: 0.87\n",
            "100% 2959/2959 [04:40<00:00, 10.54it/s]\n",
            " Train Epoch: 21 Loss: 0.1896 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.23it/s]\n",
            " Val Epoch: 21 Avg loss: 0.4886 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.52it/s]\n",
            " Train Epoch: 22 Loss: 0.1817 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.95it/s]\n",
            " Val Epoch: 22 Avg loss: 0.4881 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.50it/s]\n",
            " Train Epoch: 23 Loss: 0.1757 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 23 Avg loss: 0.4887 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.52it/s]\n",
            " Train Epoch: 24 Loss: 0.1679 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 24 Avg loss: 0.4936 Acc: 0.86\n",
            "100% 2959/2959 [04:42<00:00, 10.48it/s]\n",
            " Train Epoch: 25 Loss: 0.1630 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.41it/s]\n",
            " Val Epoch: 25 Avg loss: 0.4836 Acc: 0.87\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 26 Loss: 0.1565 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.31it/s]\n",
            " Val Epoch: 26 Avg loss: 0.5166 Acc: 0.87\n",
            "100% 2959/2959 [04:41<00:00, 10.49it/s]\n",
            " Train Epoch: 27 Loss: 0.1532 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 27 Avg loss: 0.5199 Acc: 0.87\n",
            "100% 2959/2959 [04:41<00:00, 10.50it/s]\n",
            " Train Epoch: 28 Loss: 0.1474 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.09it/s]\n",
            " Val Epoch: 28 Avg loss: 0.5526 Acc: 0.87\n",
            "100% 2959/2959 [04:41<00:00, 10.51it/s]\n",
            " Train Epoch: 29 Loss: 0.1440 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 29 Avg loss: 0.5249 Acc: 0.87\n",
            "100% 2959/2959 [04:41<00:00, 10.52it/s]\n",
            " Train Epoch: 30 Loss: 0.1408 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.95it/s]\n",
            " Val Epoch: 30 Avg loss: 0.5322 Acc: 0.86\n",
            "100% 2959/2959 [04:42<00:00, 10.48it/s]\n",
            " Train Epoch: 31 Loss: 0.1360 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.01it/s]\n",
            " Val Epoch: 31 Avg loss: 0.5342 Acc: 0.86\n",
            "100% 2959/2959 [04:42<00:00, 10.49it/s]\n",
            " Train Epoch: 32 Loss: 0.1318 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.91it/s]\n",
            " Val Epoch: 32 Avg loss: 0.5597 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.50it/s]\n",
            " Train Epoch: 33 Loss: 0.1302 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.09it/s]\n",
            " Val Epoch: 33 Avg loss: 0.5433 Acc: 0.87\n",
            "100% 2959/2959 [04:41<00:00, 10.53it/s]\n",
            " Train Epoch: 34 Loss: 0.1263 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.81it/s]\n",
            " Val Epoch: 34 Avg loss: 0.5929 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.53it/s]\n",
            " Train Epoch: 35 Loss: 0.1237 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 35 Avg loss: 0.5547 Acc: 0.86\n",
            "100% 2959/2959 [04:42<00:00, 10.49it/s]\n",
            " Train Epoch: 36 Loss: 0.1224 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.78it/s]\n",
            " Val Epoch: 36 Avg loss: 0.6184 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.50it/s]\n",
            " Train Epoch: 37 Loss: 0.1198 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.99it/s]\n",
            " Val Epoch: 37 Avg loss: 0.5644 Acc: 0.86\n",
            "100% 2959/2959 [04:42<00:00, 10.49it/s]\n",
            " Train Epoch: 38 Loss: 0.1160 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.97it/s]\n",
            " Val Epoch: 38 Avg loss: 0.6064 Acc: 0.86\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer.py -m train -a resnet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usiUEjqreiuJ",
        "outputId": "79f2603f-5f45-43b6-a871-5380d1eae1ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-07 13:12:05.362136: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-07 13:12:05.408913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-07 13:12:06.292503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 44586042\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.00it/s]\n",
            "Test Acc: 0.8822\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer.py -m test --model_name indof_r101 -a resnet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq1FbmpVe4-M",
        "outputId": "7b44dbb5-f6b9-4f9f-938c-3e760efb03d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-07 14:01:31.437200: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-07 14:01:31.485241: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-07 14:01:32.357501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: img_trainer.py\n",
            "       [-h]\n",
            "       [-m MODE]\n",
            "       [-n MODEL_NAME]\n",
            "       [-a ARCHITECTURE]\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  -m MODE, --mode MODE\n",
            "    mode,\n",
            "    {'train',\n",
            "    'test'}\n",
            "    (default:\n",
            "    test)\n",
            "  -n MODEL_NAME, --model_name MODEL_NAME\n",
            "    Model Name\n",
            "    (default:\n",
            "    my_model)\n",
            "  -a ARCHITECTURE, --architecture ARCHITECTURE\n",
            "    model archi\n",
            "    tecture, {'\n",
            "    resnet18',\n",
            "    'resnet50',\n",
            "    'resnet101'\n",
            "    } (default:\n",
            "    resnet18)\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer.py -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CR-W3sXqMLr"
      },
      "outputs": [],
      "source": [
        "## flip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNrjBLq8gcrQ",
        "outputId": "be44c8b0-a532-4558-ca1e-427569e32311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 211MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100% 97.8M/97.8M [00:00<00:00, 222MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100% 171M/171M [00:02<00:00, 68.0MB/s]\n",
            "2023-07-10 11:14:18.398843: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-10 11:14:18.445061: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-10 11:14:19.288489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [03:10<00:00, 15.50it/s]\n",
            " Train Epoch: 1 Loss: 0.8521 Acc: 0.73 \n",
            "100% 245/245 [00:09<00:00, 25.99it/s]\n",
            " Val Epoch: 1 Avg loss: 0.7952 Acc: 0.73\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:03<00:00, 16.15it/s]\n",
            " Train Epoch: 2 Loss: 0.5916 Acc: 0.81 \n",
            "100% 245/245 [00:09<00:00, 25.97it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5662 Acc: 0.81\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:04<00:00, 16.06it/s]\n",
            " Train Epoch: 3 Loss: 0.5121 Acc: 0.84 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5472 Acc: 0.82\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:03<00:00, 16.12it/s]\n",
            " Train Epoch: 4 Loss: 0.4635 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 4 Avg loss: 0.5193 Acc: 0.83\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:03<00:00, 16.13it/s]\n",
            " Train Epoch: 5 Loss: 0.4264 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4659 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:03<00:00, 16.10it/s]\n",
            " Train Epoch: 6 Loss: 0.3962 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.12it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4695 Acc: 0.85\n",
            "100% 2959/2959 [03:04<00:00, 16.04it/s]\n",
            " Train Epoch: 7 Loss: 0.3719 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.26it/s]\n",
            " Val Epoch: 7 Avg loss: 0.4553 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:03<00:00, 16.11it/s]\n",
            " Train Epoch: 8 Loss: 0.3512 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.09it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4295 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:04<00:00, 16.06it/s]\n",
            " Train Epoch: 9 Loss: 0.3316 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 9 Avg loss: 0.4538 Acc: 0.85\n",
            "100% 2959/2959 [03:04<00:00, 16.08it/s]\n",
            " Train Epoch: 10 Loss: 0.3140 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.20it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4211 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:03<00:00, 16.14it/s]\n",
            " Train Epoch: 11 Loss: 0.2964 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.00it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4295 Acc: 0.86\n",
            "100% 2959/2959 [03:03<00:00, 16.13it/s]\n",
            " Train Epoch: 12 Loss: 0.2832 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.06it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4328 Acc: 0.86\n",
            "100% 2959/2959 [03:04<00:00, 16.06it/s]\n",
            " Train Epoch: 13 Loss: 0.2672 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 13 Avg loss: 0.4412 Acc: 0.86\n",
            "100% 2959/2959 [03:03<00:00, 16.14it/s]\n",
            " Train Epoch: 14 Loss: 0.2529 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.26it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4577 Acc: 0.86\n",
            "100% 2959/2959 [03:03<00:00, 16.15it/s]\n",
            " Train Epoch: 15 Loss: 0.2420 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 25.94it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4249 Acc: 0.87\n",
            "100% 2959/2959 [03:04<00:00, 16.02it/s]\n",
            " Train Epoch: 16 Loss: 0.2291 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.32it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4282 Acc: 0.87\n",
            "100% 2959/2959 [03:04<00:00, 16.07it/s]\n",
            " Train Epoch: 17 Loss: 0.2185 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 17 Avg loss: 0.4589 Acc: 0.86\n",
            "100% 2959/2959 [03:03<00:00, 16.12it/s]\n",
            " Train Epoch: 18 Loss: 0.2072 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 18 Avg loss: 0.4873 Acc: 0.86\n",
            "100% 2959/2959 [03:04<00:00, 16.08it/s]\n",
            " Train Epoch: 19 Loss: 0.1997 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 19 Avg loss: 0.4641 Acc: 0.87\n",
            "100% 2959/2959 [03:03<00:00, 16.13it/s]\n",
            " Train Epoch: 20 Loss: 0.1888 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 25.94it/s]\n",
            " Val Epoch: 20 Avg loss: 0.4827 Acc: 0.86\n",
            "100% 2959/2959 [03:03<00:00, 16.12it/s]\n",
            " Train Epoch: 21 Loss: 0.1832 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.29it/s]\n",
            " Val Epoch: 21 Avg loss: 0.4747 Acc: 0.87\n",
            "100% 2959/2959 [03:03<00:00, 16.09it/s]\n",
            " Train Epoch: 22 Loss: 0.1733 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.25it/s]\n",
            " Val Epoch: 22 Avg loss: 0.4988 Acc: 0.86\n",
            "100% 2959/2959 [03:04<00:00, 16.04it/s]\n",
            " Train Epoch: 23 Loss: 0.1696 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.01it/s]\n",
            " Val Epoch: 23 Avg loss: 0.5345 Acc: 0.86\n",
            "100% 2959/2959 [03:04<00:00, 16.07it/s]\n",
            " Train Epoch: 24 Loss: 0.1606 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.65it/s]\n",
            " Val Epoch: 24 Avg loss: 0.5610 Acc: 0.86\n",
            "100% 2959/2959 [03:04<00:00, 16.00it/s]\n",
            " Train Epoch: 25 Loss: 0.1553 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 25 Avg loss: 0.4897 Acc: 0.87\n",
            "100% 2959/2959 [03:05<00:00, 15.95it/s]\n",
            " Train Epoch: 26 Loss: 0.1515 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.89it/s]\n",
            " Val Epoch: 26 Avg loss: 0.4764 Acc: 0.87\n",
            "100% 2959/2959 [03:05<00:00, 15.94it/s]\n",
            " Train Epoch: 27 Loss: 0.1434 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.01it/s]\n",
            " Val Epoch: 27 Avg loss: 0.5441 Acc: 0.87\n",
            "100% 2959/2959 [03:04<00:00, 16.05it/s]\n",
            " Train Epoch: 28 Loss: 0.1409 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.79it/s]\n",
            " Val Epoch: 28 Avg loss: 0.4966 Acc: 0.86\n",
            "100% 2959/2959 [03:04<00:00, 16.01it/s]\n",
            " Train Epoch: 29 Loss: 0.1367 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.10it/s]\n",
            " Val Epoch: 29 Avg loss: 0.5482 Acc: 0.87\n",
            "100% 2959/2959 [03:04<00:00, 16.05it/s]\n",
            " Train Epoch: 30 Loss: 0.1326 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.17it/s]\n",
            " Val Epoch: 30 Avg loss: 0.5309 Acc: 0.87\n",
            "100% 2959/2959 [03:03<00:00, 16.12it/s]\n",
            " Train Epoch: 31 Loss: 0.1283 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.20it/s]\n",
            " Val Epoch: 31 Avg loss: 0.5734 Acc: 0.87\n",
            "100% 2959/2959 [03:03<00:00, 16.09it/s]\n",
            " Train Epoch: 32 Loss: 0.1263 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.11it/s]\n",
            " Val Epoch: 32 Avg loss: 0.5765 Acc: 0.87\n",
            "100% 2959/2959 [03:04<00:00, 16.07it/s]\n",
            " Train Epoch: 33 Loss: 0.1206 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.03it/s]\n",
            " Val Epoch: 33 Avg loss: 0.5373 Acc: 0.87\n",
            "100% 2959/2959 [03:03<00:00, 16.11it/s]\n",
            " Train Epoch: 34 Loss: 0.1211 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.84it/s]\n",
            " Val Epoch: 34 Avg loss: 0.5556 Acc: 0.86\n",
            "100% 2959/2959 [03:03<00:00, 16.12it/s]\n",
            " Train Epoch: 35 Loss: 0.1178 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 35 Avg loss: 0.5416 Acc: 0.87\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_flip.py -m train -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDmNjYH7EscZ",
        "outputId": "d6e86038-9d66-465f-8892-58991c68d931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-10 13:57:46.205863: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-10 13:57:46.252637: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-10 13:57:47.145036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.23it/s]\n",
            "Test Acc: 0.8808\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_flip.py -m test --model_name indof_r50_flip -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UdLxxUnFEJo",
        "outputId": "ca8dd17d-d6e6-4f3a-dfc9-066bda476db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-10 13:58:14.906784: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-10 13:58:14.952905: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-10 13:58:15.801523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 11698746\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [02:16<00:00, 21.69it/s]\n",
            " Train Epoch: 1 Loss: 0.7455 Acc: 0.77 \n",
            "100% 245/245 [00:09<00:00, 25.58it/s]\n",
            " Val Epoch: 1 Avg loss: 0.6434 Acc: 0.79\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:13<00:00, 22.20it/s]\n",
            " Train Epoch: 2 Loss: 0.5434 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5842 Acc: 0.81\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:14<00:00, 22.07it/s]\n",
            " Train Epoch: 3 Loss: 0.4774 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 3 Avg loss: 0.4808 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:13<00:00, 22.10it/s]\n",
            " Train Epoch: 4 Loss: 0.4321 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.25it/s]\n",
            " Val Epoch: 4 Avg loss: 0.4767 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:14<00:00, 22.01it/s]\n",
            " Train Epoch: 5 Loss: 0.3918 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 25.27it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4843 Acc: 0.84\n",
            "100% 2959/2959 [02:13<00:00, 22.09it/s]\n",
            " Train Epoch: 6 Loss: 0.3590 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4411 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:13<00:00, 22.13it/s]\n",
            " Train Epoch: 7 Loss: 0.3366 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 7 Avg loss: 0.4456 Acc: 0.86\n",
            "100% 2959/2959 [02:13<00:00, 22.14it/s]\n",
            " Train Epoch: 8 Loss: 0.3127 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.26it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4363 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:13<00:00, 22.11it/s]\n",
            " Train Epoch: 9 Loss: 0.2928 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 9 Avg loss: 0.5179 Acc: 0.84\n",
            "100% 2959/2959 [02:12<00:00, 22.31it/s]\n",
            " Train Epoch: 10 Loss: 0.2736 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.40it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4344 Acc: 0.87\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:12<00:00, 22.33it/s]\n",
            " Train Epoch: 11 Loss: 0.2572 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.61it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4703 Acc: 0.86\n",
            "100% 2959/2959 [02:12<00:00, 22.28it/s]\n",
            " Train Epoch: 12 Loss: 0.2428 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.33it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4579 Acc: 0.87\n",
            "100% 2959/2959 [02:12<00:00, 22.38it/s]\n",
            " Train Epoch: 13 Loss: 0.2253 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 13 Avg loss: 0.4958 Acc: 0.85\n",
            "100% 2959/2959 [02:13<00:00, 22.20it/s]\n",
            " Train Epoch: 14 Loss: 0.2169 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.23it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4788 Acc: 0.87\n",
            "100% 2959/2959 [02:11<00:00, 22.45it/s]\n",
            " Train Epoch: 15 Loss: 0.2026 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.49it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4749 Acc: 0.87\n",
            "100% 2959/2959 [02:11<00:00, 22.49it/s]\n",
            " Train Epoch: 16 Loss: 0.1949 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.51it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4877 Acc: 0.87\n",
            "100% 2959/2959 [02:11<00:00, 22.57it/s]\n",
            " Train Epoch: 17 Loss: 0.1834 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.53it/s]\n",
            " Val Epoch: 17 Avg loss: 0.5047 Acc: 0.86\n",
            "100% 2959/2959 [02:11<00:00, 22.46it/s]\n",
            " Train Epoch: 18 Loss: 0.1762 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.48it/s]\n",
            " Val Epoch: 18 Avg loss: 0.5110 Acc: 0.86\n",
            "100% 2959/2959 [02:12<00:00, 22.39it/s]\n",
            " Train Epoch: 19 Loss: 0.1689 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.16it/s]\n",
            " Val Epoch: 19 Avg loss: 0.5359 Acc: 0.86\n",
            "100% 2959/2959 [02:13<00:00, 22.21it/s]\n",
            " Train Epoch: 20 Loss: 0.1601 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.85it/s]\n",
            " Val Epoch: 20 Avg loss: 0.5226 Acc: 0.87\n",
            "100% 2959/2959 [02:12<00:00, 22.38it/s]\n",
            " Train Epoch: 21 Loss: 0.1564 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.47it/s]\n",
            " Val Epoch: 21 Avg loss: 0.5330 Acc: 0.86\n",
            "100% 2959/2959 [02:13<00:00, 22.09it/s]\n",
            " Train Epoch: 22 Loss: 0.1483 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.38it/s]\n",
            " Val Epoch: 22 Avg loss: 0.5868 Acc: 0.86\n",
            "100% 2959/2959 [02:12<00:00, 22.37it/s]\n",
            " Train Epoch: 23 Loss: 0.1440 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.13it/s]\n",
            " Val Epoch: 23 Avg loss: 0.5909 Acc: 0.86\n",
            "100% 2959/2959 [02:11<00:00, 22.54it/s]\n",
            " Train Epoch: 24 Loss: 0.1378 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.38it/s]\n",
            " Val Epoch: 24 Avg loss: 0.5604 Acc: 0.87\n",
            "100% 2959/2959 [02:12<00:00, 22.35it/s]\n",
            " Train Epoch: 25 Loss: 0.1363 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.57it/s]\n",
            " Val Epoch: 25 Avg loss: 0.5905 Acc: 0.86\n",
            "100% 2959/2959 [02:12<00:00, 22.37it/s]\n",
            " Train Epoch: 26 Loss: 0.1292 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.99it/s]\n",
            " Val Epoch: 26 Avg loss: 0.5791 Acc: 0.87\n",
            "100% 2959/2959 [02:11<00:00, 22.57it/s]\n",
            " Train Epoch: 27 Loss: 0.1285 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 27 Avg loss: 0.6160 Acc: 0.86\n",
            "100% 2959/2959 [02:13<00:00, 22.21it/s]\n",
            " Train Epoch: 28 Loss: 0.1226 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.38it/s]\n",
            " Val Epoch: 28 Avg loss: 0.6148 Acc: 0.86\n",
            "100% 2959/2959 [02:12<00:00, 22.37it/s]\n",
            " Train Epoch: 29 Loss: 0.1190 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 29 Avg loss: 0.6317 Acc: 0.86\n",
            "100% 2959/2959 [02:11<00:00, 22.48it/s]\n",
            " Train Epoch: 30 Loss: 0.1158 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 30 Avg loss: 0.6688 Acc: 0.86\n",
            "100% 2959/2959 [02:11<00:00, 22.58it/s]\n",
            " Train Epoch: 31 Loss: 0.1141 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.97it/s]\n",
            " Val Epoch: 31 Avg loss: 0.6734 Acc: 0.86\n",
            "100% 2959/2959 [02:12<00:00, 22.39it/s]\n",
            " Train Epoch: 32 Loss: 0.1129 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 32 Avg loss: 0.6051 Acc: 0.86\n",
            "100% 2959/2959 [02:13<00:00, 22.14it/s]\n",
            " Train Epoch: 33 Loss: 0.1083 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.10it/s]\n",
            " Val Epoch: 33 Avg loss: 0.6355 Acc: 0.87\n",
            "100% 2959/2959 [02:13<00:00, 22.22it/s]\n",
            " Train Epoch: 34 Loss: 0.1072 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.03it/s]\n",
            " Val Epoch: 34 Avg loss: 0.6509 Acc: 0.87\n",
            "100% 2959/2959 [02:13<00:00, 22.14it/s]\n",
            " Train Epoch: 35 Loss: 0.1043 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.36it/s]\n",
            " Val Epoch: 35 Avg loss: 0.7093 Acc: 0.86\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_flip.py -m train -a resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkD5GXlIayTi",
        "outputId": "011dff26-1b3c-48de-972f-7e18829abc9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-10 15:30:47.364851: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-10 15:30:47.412182: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-10 15:30:48.288022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 11698746\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.29it/s]\n",
            "Test Acc: 0.8781\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_flip.py -m test --model_name indof_r18_flip -a resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utiGm2HwbuQy",
        "outputId": "d024adf1-d655-4557-cef6-c8d19bfc8514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-10 15:33:01.909929: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-10 15:33:01.955913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-10 15:33:02.838189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 44586042\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [04:39<00:00, 10.58it/s]\n",
            " Train Epoch: 1 Loss: 0.8861 Acc: 0.72 \n",
            "100% 245/245 [00:09<00:00, 25.41it/s]\n",
            " Val Epoch: 1 Avg loss: 0.7865 Acc: 0.75\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:37<00:00, 10.66it/s]\n",
            " Train Epoch: 2 Loss: 0.6039 Acc: 0.81 \n",
            "100% 245/245 [00:09<00:00, 25.99it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5915 Acc: 0.81\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:38<00:00, 10.61it/s]\n",
            " Train Epoch: 3 Loss: 0.5215 Acc: 0.84 \n",
            "100% 245/245 [00:09<00:00, 26.24it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5179 Acc: 0.83\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:37<00:00, 10.67it/s]\n",
            " Train Epoch: 4 Loss: 0.4707 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 25.92it/s]\n",
            " Val Epoch: 4 Avg loss: 0.5078 Acc: 0.83\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:37<00:00, 10.67it/s]\n",
            " Train Epoch: 5 Loss: 0.4331 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.11it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4851 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:39<00:00, 10.59it/s]\n",
            " Train Epoch: 6 Loss: 0.4042 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.04it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4501 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:37<00:00, 10.65it/s]\n",
            " Train Epoch: 7 Loss: 0.3785 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 7 Avg loss: 0.4965 Acc: 0.85\n",
            "100% 2959/2959 [04:37<00:00, 10.66it/s]\n",
            " Train Epoch: 8 Loss: 0.3564 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 25.91it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4513 Acc: 0.86\n",
            "100% 2959/2959 [04:39<00:00, 10.58it/s]\n",
            " Train Epoch: 9 Loss: 0.3402 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 25.96it/s]\n",
            " Val Epoch: 9 Avg loss: 0.4208 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:39<00:00, 10.57it/s]\n",
            " Train Epoch: 10 Loss: 0.3216 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.03it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4468 Acc: 0.86\n",
            "100% 2959/2959 [04:38<00:00, 10.62it/s]\n",
            " Train Epoch: 11 Loss: 0.3030 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.09it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4698 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.60it/s]\n",
            " Train Epoch: 12 Loss: 0.2872 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4482 Acc: 0.86\n",
            "100% 2959/2959 [04:39<00:00, 10.57it/s]\n",
            " Train Epoch: 13 Loss: 0.2718 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 13 Avg loss: 0.5002 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.57it/s]\n",
            " Train Epoch: 14 Loss: 0.2592 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.00it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4269 Acc: 0.87\n",
            "100% 2959/2959 [04:37<00:00, 10.66it/s]\n",
            " Train Epoch: 15 Loss: 0.2480 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4521 Acc: 0.86\n",
            "100% 2959/2959 [04:38<00:00, 10.62it/s]\n",
            " Train Epoch: 16 Loss: 0.2344 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 25.88it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4386 Acc: 0.87\n",
            "100% 2959/2959 [04:37<00:00, 10.67it/s]\n",
            " Train Epoch: 17 Loss: 0.2225 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.17it/s]\n",
            " Val Epoch: 17 Avg loss: 0.4381 Acc: 0.87\n",
            "100% 2959/2959 [04:38<00:00, 10.64it/s]\n",
            " Train Epoch: 18 Loss: 0.2129 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 18 Avg loss: 0.4337 Acc: 0.87\n",
            "100% 2959/2959 [04:38<00:00, 10.62it/s]\n",
            " Train Epoch: 19 Loss: 0.2035 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 25.95it/s]\n",
            " Val Epoch: 19 Avg loss: 0.4525 Acc: 0.87\n",
            "100% 2959/2959 [04:38<00:00, 10.64it/s]\n",
            " Train Epoch: 20 Loss: 0.1940 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.18it/s]\n",
            " Val Epoch: 20 Avg loss: 0.4803 Acc: 0.86\n",
            "100% 2959/2959 [04:37<00:00, 10.65it/s]\n",
            " Train Epoch: 21 Loss: 0.1877 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 21 Avg loss: 0.4470 Acc: 0.87\n",
            "100% 2959/2959 [04:37<00:00, 10.65it/s]\n",
            " Train Epoch: 22 Loss: 0.1773 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.29it/s]\n",
            " Val Epoch: 22 Avg loss: 0.4753 Acc: 0.86\n",
            "100% 2959/2959 [04:38<00:00, 10.62it/s]\n",
            " Train Epoch: 23 Loss: 0.1725 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.07it/s]\n",
            " Val Epoch: 23 Avg loss: 0.5074 Acc: 0.86\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 24 Loss: 0.1645 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.05it/s]\n",
            " Val Epoch: 24 Avg loss: 0.4848 Acc: 0.87\n",
            "100% 2959/2959 [04:38<00:00, 10.64it/s]\n",
            " Train Epoch: 25 Loss: 0.1585 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 25 Avg loss: 0.4840 Acc: 0.87\n",
            "100% 2959/2959 [04:38<00:00, 10.64it/s]\n",
            " Train Epoch: 26 Loss: 0.1551 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 26 Avg loss: 0.5089 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.53it/s]\n",
            " Train Epoch: 27 Loss: 0.1496 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 27 Avg loss: 0.5277 Acc: 0.86\n",
            "100% 2959/2959 [04:41<00:00, 10.52it/s]\n",
            " Train Epoch: 28 Loss: 0.1435 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 28 Avg loss: 0.4970 Acc: 0.87\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 29 Loss: 0.1391 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.95it/s]\n",
            " Val Epoch: 29 Avg loss: 0.5495 Acc: 0.86\n",
            "100% 2959/2959 [04:40<00:00, 10.57it/s]\n",
            " Train Epoch: 30 Loss: 0.1364 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.02it/s]\n",
            " Val Epoch: 30 Avg loss: 0.5203 Acc: 0.86\n",
            "100% 2959/2959 [04:39<00:00, 10.60it/s]\n",
            " Train Epoch: 31 Loss: 0.1324 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.24it/s]\n",
            " Val Epoch: 31 Avg loss: 0.5577 Acc: 0.86\n",
            "100% 2959/2959 [04:38<00:00, 10.62it/s]\n",
            " Train Epoch: 32 Loss: 0.1296 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 32 Avg loss: 0.5178 Acc: 0.86\n",
            "100% 2959/2959 [04:37<00:00, 10.68it/s]\n",
            " Train Epoch: 33 Loss: 0.1257 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.89it/s]\n",
            " Val Epoch: 33 Avg loss: 0.5401 Acc: 0.86\n",
            "100% 2959/2959 [04:37<00:00, 10.66it/s]\n",
            " Train Epoch: 34 Loss: 0.1231 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 34 Avg loss: 0.5706 Acc: 0.86\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_flip.py -m train -a resnet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqCXPmW2CGjL",
        "outputId": "9086adc0-9819-4877-f708-e35fde8e7ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-10 18:21:13.644535: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-10 18:21:13.690208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-10 18:21:14.571322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 44586042\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 20.88it/s]\n",
            "Test Acc: 0.8722\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_flip.py -m test --model_name indof_r101_flip -a resnet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVz9QaSgCbxR",
        "outputId": "af062eb5-dea8-44d8-f470-d391bbc22608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 311MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100% 97.8M/97.8M [00:00<00:00, 368MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100% 171M/171M [00:00<00:00, 378MB/s]\n",
            "2023-07-10 23:34:31.021143: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-10 23:34:31.067507: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-10 23:34:31.919386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [04:01<00:00, 12.27it/s]\n",
            " Train Epoch: 1 Loss: 0.8073 Acc: 0.74 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 1 Avg loss: 0.6648 Acc: 0.78\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:55<00:00, 12.58it/s]\n",
            " Train Epoch: 2 Loss: 0.5462 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 26.61it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5831 Acc: 0.81\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:53<00:00, 12.66it/s]\n",
            " Train Epoch: 3 Loss: 0.4630 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.69it/s]\n",
            " Val Epoch: 3 Avg loss: 0.4852 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 4 Loss: 0.4058 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.66it/s]\n",
            " Val Epoch: 4 Avg loss: 0.5121 Acc: 0.83\n",
            "100% 2959/2959 [03:55<00:00, 12.59it/s]\n",
            " Train Epoch: 5 Loss: 0.3548 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.47it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4668 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:54<00:00, 12.61it/s]\n",
            " Train Epoch: 6 Loss: 0.3111 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.57it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4514 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:54<00:00, 12.61it/s]\n",
            " Train Epoch: 7 Loss: 0.2703 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.25it/s]\n",
            " Val Epoch: 7 Avg loss: 0.5064 Acc: 0.84\n",
            "100% 2959/2959 [03:53<00:00, 12.67it/s]\n",
            " Train Epoch: 8 Loss: 0.2363 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.46it/s]\n",
            " Val Epoch: 8 Avg loss: 0.5177 Acc: 0.85\n",
            "100% 2959/2959 [03:53<00:00, 12.67it/s]\n",
            " Train Epoch: 9 Loss: 0.2083 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.62it/s]\n",
            " Val Epoch: 9 Avg loss: 0.4840 Acc: 0.86\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 10 Loss: 0.1846 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.52it/s]\n",
            " Val Epoch: 10 Avg loss: 0.5483 Acc: 0.85\n",
            "100% 2959/2959 [03:52<00:00, 12.70it/s]\n",
            " Train Epoch: 11 Loss: 0.1668 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.61it/s]\n",
            " Val Epoch: 11 Avg loss: 0.5367 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.58it/s]\n",
            " Train Epoch: 12 Loss: 0.1532 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.38it/s]\n",
            " Val Epoch: 12 Avg loss: 0.5554 Acc: 0.85\n",
            "100% 2959/2959 [03:54<00:00, 12.63it/s]\n",
            " Train Epoch: 13 Loss: 0.1445 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.32it/s]\n",
            " Val Epoch: 13 Avg loss: 0.5970 Acc: 0.84\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 14 Loss: 0.1339 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.83it/s]\n",
            " Val Epoch: 14 Avg loss: 0.5951 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.59it/s]\n",
            " Train Epoch: 15 Loss: 0.1274 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.55it/s]\n",
            " Val Epoch: 15 Avg loss: 0.6283 Acc: 0.85\n",
            "100% 2959/2959 [03:53<00:00, 12.65it/s]\n",
            " Train Epoch: 16 Loss: 0.1207 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.57it/s]\n",
            " Val Epoch: 16 Avg loss: 0.6314 Acc: 0.85\n",
            "100% 2959/2959 [03:54<00:00, 12.61it/s]\n",
            " Train Epoch: 17 Loss: 0.1146 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.54it/s]\n",
            " Val Epoch: 17 Avg loss: 0.6167 Acc: 0.85\n",
            "100% 2959/2959 [03:53<00:00, 12.69it/s]\n",
            " Train Epoch: 18 Loss: 0.1112 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.39it/s]\n",
            " Val Epoch: 18 Avg loss: 0.6793 Acc: 0.84\n",
            "100% 2959/2959 [03:56<00:00, 12.53it/s]\n",
            " Train Epoch: 19 Loss: 0.1051 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.51it/s]\n",
            " Val Epoch: 19 Avg loss: 0.6598 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.55it/s]\n",
            " Train Epoch: 20 Loss: 0.1006 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.07it/s]\n",
            " Val Epoch: 20 Avg loss: 0.6557 Acc: 0.85\n",
            "100% 2959/2959 [03:56<00:00, 12.50it/s]\n",
            " Train Epoch: 21 Loss: 0.0979 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.31it/s]\n",
            " Val Epoch: 21 Avg loss: 0.6942 Acc: 0.84\n",
            "100% 2959/2959 [03:56<00:00, 12.50it/s]\n",
            " Train Epoch: 22 Loss: 0.0927 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.42it/s]\n",
            " Val Epoch: 22 Avg loss: 0.7281 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.54it/s]\n",
            " Train Epoch: 23 Loss: 0.0909 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.40it/s]\n",
            " Val Epoch: 23 Avg loss: 0.6887 Acc: 0.85\n",
            "100% 2959/2959 [03:57<00:00, 12.45it/s]\n",
            " Train Epoch: 24 Loss: 0.0860 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.42it/s]\n",
            " Val Epoch: 24 Avg loss: 0.7295 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.57it/s]\n",
            " Train Epoch: 25 Loss: 0.0850 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.23it/s]\n",
            " Val Epoch: 25 Avg loss: 0.7715 Acc: 0.84\n",
            "100% 2959/2959 [03:55<00:00, 12.56it/s]\n",
            " Train Epoch: 26 Loss: 0.0821 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 26 Avg loss: 0.7644 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.54it/s]\n",
            " Train Epoch: 27 Loss: 0.0787 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.32it/s]\n",
            " Val Epoch: 27 Avg loss: 0.8176 Acc: 0.85\n",
            "100% 2959/2959 [03:56<00:00, 12.53it/s]\n",
            " Train Epoch: 28 Loss: 0.0788 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 28 Avg loss: 0.7774 Acc: 0.84\n",
            "100% 2959/2959 [03:56<00:00, 12.49it/s]\n",
            " Train Epoch: 29 Loss: 0.0752 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.12it/s]\n",
            " Val Epoch: 29 Avg loss: 0.8310 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.55it/s]\n",
            " Train Epoch: 30 Loss: 0.0726 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.29it/s]\n",
            " Val Epoch: 30 Avg loss: 0.8102 Acc: 0.84\n",
            "100% 2959/2959 [03:56<00:00, 12.53it/s]\n",
            " Train Epoch: 31 Loss: 0.0714 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.31it/s]\n",
            " Val Epoch: 31 Avg loss: 0.8458 Acc: 0.84\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_jitter.py -m train -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPNgI531p2O8",
        "outputId": "cc930205-04e1-4d90-af2b-41e090d976de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-11 01:55:14.184422: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 01:55:14.231669: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 01:55:15.090752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.25it/s]\n",
            "Test Acc: 0.8635\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_jitter.py -m test --model_name indof_r50_jitter -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLJi5l0CqesL",
        "outputId": "2fffac40-bc3e-409f-a338-ec910526dc33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-11 01:57:27.604511: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 01:57:27.650717: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 01:57:28.519582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 11698746\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [03:57<00:00, 12.47it/s]\n",
            " Train Epoch: 1 Loss: 0.6871 Acc: 0.78 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 1 Avg loss: 0.5975 Acc: 0.80\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 2 Loss: 0.4896 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.24it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5327 Acc: 0.83\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:54<00:00, 12.61it/s]\n",
            " Train Epoch: 3 Loss: 0.4098 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.29it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5172 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:55<00:00, 12.56it/s]\n",
            " Train Epoch: 4 Loss: 0.3418 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 25.87it/s]\n",
            " Val Epoch: 4 Avg loss: 0.4608 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 5 Loss: 0.2875 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4591 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:54<00:00, 12.62it/s]\n",
            " Train Epoch: 6 Loss: 0.2403 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.29it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4788 Acc: 0.86\n",
            "100% 2959/2959 [03:55<00:00, 12.55it/s]\n",
            " Train Epoch: 7 Loss: 0.2034 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.12it/s]\n",
            " Val Epoch: 7 Avg loss: 0.5187 Acc: 0.85\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 8 Loss: 0.1715 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.68it/s]\n",
            " Val Epoch: 8 Avg loss: 0.5706 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.57it/s]\n",
            " Train Epoch: 9 Loss: 0.1542 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.11it/s]\n",
            " Val Epoch: 9 Avg loss: 0.5986 Acc: 0.85\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 10 Loss: 0.1371 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.52it/s]\n",
            " Val Epoch: 10 Avg loss: 0.6340 Acc: 0.85\n",
            "100% 2959/2959 [03:57<00:00, 12.48it/s]\n",
            " Train Epoch: 11 Loss: 0.1280 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.39it/s]\n",
            " Val Epoch: 11 Avg loss: 0.6816 Acc: 0.84\n",
            "100% 2959/2959 [03:56<00:00, 12.53it/s]\n",
            " Train Epoch: 12 Loss: 0.1147 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.31it/s]\n",
            " Val Epoch: 12 Avg loss: 0.7621 Acc: 0.85\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 13 Loss: 0.1071 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.46it/s]\n",
            " Val Epoch: 13 Avg loss: 0.8269 Acc: 0.84\n",
            "100% 2959/2959 [03:54<00:00, 12.62it/s]\n",
            " Train Epoch: 14 Loss: 0.1036 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 14 Avg loss: 0.7391 Acc: 0.85\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 15 Loss: 0.0938 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.57it/s]\n",
            " Val Epoch: 15 Avg loss: 0.8553 Acc: 0.84\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 16 Loss: 0.0929 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.17it/s]\n",
            " Val Epoch: 16 Avg loss: 0.8427 Acc: 0.84\n",
            "100% 2959/2959 [03:54<00:00, 12.63it/s]\n",
            " Train Epoch: 17 Loss: 0.0880 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.10it/s]\n",
            " Val Epoch: 17 Avg loss: 0.8567 Acc: 0.84\n",
            "100% 2959/2959 [03:53<00:00, 12.66it/s]\n",
            " Train Epoch: 18 Loss: 0.0848 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.33it/s]\n",
            " Val Epoch: 18 Avg loss: 0.9238 Acc: 0.85\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 19 Loss: 0.0820 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.43it/s]\n",
            " Val Epoch: 19 Avg loss: 0.8998 Acc: 0.85\n",
            "100% 2959/2959 [03:53<00:00, 12.65it/s]\n",
            " Train Epoch: 20 Loss: 0.0792 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 20 Avg loss: 0.9876 Acc: 0.84\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 21 Loss: 0.0767 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 21 Avg loss: 0.8798 Acc: 0.84\n",
            "100% 2959/2959 [03:54<00:00, 12.61it/s]\n",
            " Train Epoch: 22 Loss: 0.0735 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 22 Avg loss: 0.9954 Acc: 0.85\n",
            "100% 2959/2959 [03:56<00:00, 12.52it/s]\n",
            " Train Epoch: 23 Loss: 0.0734 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 23 Avg loss: 1.0035 Acc: 0.84\n",
            "100% 2959/2959 [03:56<00:00, 12.51it/s]\n",
            " Train Epoch: 24 Loss: 0.0711 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.82it/s]\n",
            " Val Epoch: 24 Avg loss: 0.9860 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.55it/s]\n",
            " Train Epoch: 25 Loss: 0.0700 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.83it/s]\n",
            " Val Epoch: 25 Avg loss: 1.0945 Acc: 0.85\n",
            "100% 2959/2959 [03:56<00:00, 12.50it/s]\n",
            " Train Epoch: 26 Loss: 0.0676 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 26 Avg loss: 1.0718 Acc: 0.84\n",
            "100% 2959/2959 [03:54<00:00, 12.60it/s]\n",
            " Train Epoch: 27 Loss: 0.0690 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.33it/s]\n",
            " Val Epoch: 27 Avg loss: 1.0004 Acc: 0.85\n",
            "100% 2959/2959 [03:56<00:00, 12.49it/s]\n",
            " Train Epoch: 28 Loss: 0.0653 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.01it/s]\n",
            " Val Epoch: 28 Avg loss: 1.1078 Acc: 0.84\n",
            "100% 2959/2959 [03:55<00:00, 12.54it/s]\n",
            " Train Epoch: 29 Loss: 0.0653 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.48it/s]\n",
            " Val Epoch: 29 Avg loss: 1.0111 Acc: 0.85\n",
            "100% 2959/2959 [03:55<00:00, 12.58it/s]\n",
            " Train Epoch: 30 Loss: 0.0644 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.98it/s]\n",
            " Val Epoch: 30 Avg loss: 1.0686 Acc: 0.85\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_jitter.py -m train -a resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpW7UDvEHu5X",
        "outputId": "b0ee596f-2d0e-4625-aa7c-bb9c542d0c91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-11 04:04:28.098845: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 04:04:28.146780: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 04:04:29.013067: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 11698746\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.57it/s]\n",
            "Test Acc: 0.8614\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_jitter.py -m test --model_name indof_r18_jitter -a resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7_Bt4GjH-Hq",
        "outputId": "a1b43411-e671-4cdd-b3e5-242c37d22db9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-11 04:05:34.305105: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 04:05:34.351789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 04:05:35.200015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 44586042\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [04:38<00:00, 10.62it/s]\n",
            " Train Epoch: 1 Loss: 0.8203 Acc: 0.74 \n",
            "100% 245/245 [00:09<00:00, 25.79it/s]\n",
            " Val Epoch: 1 Avg loss: 0.7158 Acc: 0.77\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:37<00:00, 10.67it/s]\n",
            " Train Epoch: 2 Loss: 0.5513 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 26.32it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5650 Acc: 0.82\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:36<00:00, 10.71it/s]\n",
            " Train Epoch: 3 Loss: 0.4705 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.43it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5255 Acc: 0.83\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:36<00:00, 10.70it/s]\n",
            " Train Epoch: 4 Loss: 0.4122 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 4 Avg loss: 0.5029 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:37<00:00, 10.67it/s]\n",
            " Train Epoch: 5 Loss: 0.3598 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 25.68it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4824 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 6 Loss: 0.3172 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.50it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4923 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.61it/s]\n",
            " Train Epoch: 7 Loss: 0.2778 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.12it/s]\n",
            " Val Epoch: 7 Avg loss: 0.4933 Acc: 0.85\n",
            "100% 2959/2959 [04:38<00:00, 10.64it/s]\n",
            " Train Epoch: 8 Loss: 0.2427 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.06it/s]\n",
            " Val Epoch: 8 Avg loss: 0.5013 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.59it/s]\n",
            " Train Epoch: 9 Loss: 0.2136 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 9 Avg loss: 0.5336 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.59it/s]\n",
            " Train Epoch: 10 Loss: 0.1910 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.05it/s]\n",
            " Val Epoch: 10 Avg loss: 0.5368 Acc: 0.85\n",
            "100% 2959/2959 [04:44<00:00, 10.39it/s]\n",
            " Train Epoch: 11 Loss: 0.1725 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.97it/s]\n",
            " Val Epoch: 11 Avg loss: 0.5382 Acc: 0.85\n",
            "100% 2959/2959 [04:41<00:00, 10.51it/s]\n",
            " Train Epoch: 12 Loss: 0.1593 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.06it/s]\n",
            " Val Epoch: 12 Avg loss: 0.5639 Acc: 0.84\n",
            "100% 2959/2959 [04:41<00:00, 10.50it/s]\n",
            " Train Epoch: 13 Loss: 0.1463 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.96it/s]\n",
            " Val Epoch: 13 Avg loss: 0.6042 Acc: 0.85\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 14 Loss: 0.1400 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 14 Avg loss: 0.6280 Acc: 0.84\n",
            "100% 2959/2959 [04:38<00:00, 10.62it/s]\n",
            " Train Epoch: 15 Loss: 0.1304 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.17it/s]\n",
            " Val Epoch: 15 Avg loss: 0.6515 Acc: 0.85\n",
            "100% 2959/2959 [04:40<00:00, 10.54it/s]\n",
            " Train Epoch: 16 Loss: 0.1232 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.24it/s]\n",
            " Val Epoch: 16 Avg loss: 0.6375 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.57it/s]\n",
            " Train Epoch: 17 Loss: 0.1185 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.02it/s]\n",
            " Val Epoch: 17 Avg loss: 0.6910 Acc: 0.84\n",
            "100% 2959/2959 [04:40<00:00, 10.56it/s]\n",
            " Train Epoch: 18 Loss: 0.1127 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 18 Avg loss: 0.6136 Acc: 0.84\n",
            "100% 2959/2959 [04:39<00:00, 10.57it/s]\n",
            " Train Epoch: 19 Loss: 0.1090 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 19 Avg loss: 0.6409 Acc: 0.85\n",
            "100% 2959/2959 [04:38<00:00, 10.61it/s]\n",
            " Train Epoch: 20 Loss: 0.1037 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 20 Avg loss: 0.6547 Acc: 0.84\n",
            "100% 2959/2959 [04:39<00:00, 10.60it/s]\n",
            " Train Epoch: 21 Loss: 0.1010 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.01it/s]\n",
            " Val Epoch: 21 Avg loss: 0.6659 Acc: 0.85\n",
            "100% 2959/2959 [04:38<00:00, 10.62it/s]\n",
            " Train Epoch: 22 Loss: 0.0953 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.93it/s]\n",
            " Val Epoch: 22 Avg loss: 0.7382 Acc: 0.84\n",
            "100% 2959/2959 [04:39<00:00, 10.59it/s]\n",
            " Train Epoch: 23 Loss: 0.0932 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.97it/s]\n",
            " Val Epoch: 23 Avg loss: 0.7073 Acc: 0.85\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 24 Loss: 0.0878 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 24 Avg loss: 0.6973 Acc: 0.85\n",
            "100% 2959/2959 [04:38<00:00, 10.63it/s]\n",
            " Train Epoch: 25 Loss: 0.0866 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 25 Avg loss: 0.7936 Acc: 0.84\n",
            "100% 2959/2959 [04:39<00:00, 10.57it/s]\n",
            " Train Epoch: 26 Loss: 0.0832 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.86it/s]\n",
            " Val Epoch: 26 Avg loss: 0.7087 Acc: 0.85\n",
            "100% 2959/2959 [04:38<00:00, 10.61it/s]\n",
            " Train Epoch: 27 Loss: 0.0810 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 27 Avg loss: 0.7939 Acc: 0.85\n",
            "100% 2959/2959 [04:40<00:00, 10.57it/s]\n",
            " Train Epoch: 28 Loss: 0.0790 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 28 Avg loss: 0.8008 Acc: 0.85\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 29 Loss: 0.0772 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.13it/s]\n",
            " Val Epoch: 29 Avg loss: 0.8269 Acc: 0.84\n",
            "100% 2959/2959 [04:40<00:00, 10.56it/s]\n",
            " Train Epoch: 30 Loss: 0.0755 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.04it/s]\n",
            " Val Epoch: 30 Avg loss: 0.8764 Acc: 0.84\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_jitter.py -m train -a resnet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ek43HglpxPi",
        "outputId": "bcd6e0e9-3042-4ad2-d41b-72e1a2dcc0b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-11 06:33:07.333082: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 06:33:07.380004: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 06:33:08.226677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 44586042\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 20.88it/s]\n",
            "Test Acc: 0.8633\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_jitter.py -m test --model_name indof_r101_jitter -a resnet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWjf_Iwgp9m_",
        "outputId": "4e373f09-7e9a-463c-8b67-f99bb569e6d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-11 06:34:49.595010: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 06:34:49.642167: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 06:34:50.493719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [03:04<00:00, 16.05it/s]\n",
            " Train Epoch: 1 Loss: 0.7923 Acc: 0.75 \n",
            "100% 245/245 [00:09<00:00, 25.96it/s]\n",
            " Val Epoch: 1 Avg loss: 0.7081 Acc: 0.78\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:02<00:00, 16.17it/s]\n",
            " Train Epoch: 2 Loss: 0.5407 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5545 Acc: 0.82\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:02<00:00, 16.22it/s]\n",
            " Train Epoch: 3 Loss: 0.4576 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 25.74it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5321 Acc: 0.82\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:03<00:00, 16.17it/s]\n",
            " Train Epoch: 4 Loss: 0.3979 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 25.93it/s]\n",
            " Val Epoch: 4 Avg loss: 0.4669 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:03<00:00, 16.15it/s]\n",
            " Train Epoch: 5 Loss: 0.3498 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.03it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4793 Acc: 0.85\n",
            "100% 2959/2959 [03:02<00:00, 16.20it/s]\n",
            " Train Epoch: 6 Loss: 0.3088 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.10it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4606 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [03:03<00:00, 16.16it/s]\n",
            " Train Epoch: 7 Loss: 0.2679 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.04it/s]\n",
            " Val Epoch: 7 Avg loss: 0.4848 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.14it/s]\n",
            " Train Epoch: 8 Loss: 0.2328 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 25.79it/s]\n",
            " Val Epoch: 8 Avg loss: 0.5409 Acc: 0.84\n",
            "100% 2959/2959 [03:02<00:00, 16.19it/s]\n",
            " Train Epoch: 9 Loss: 0.2059 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.06it/s]\n",
            " Val Epoch: 9 Avg loss: 0.5230 Acc: 0.84\n",
            "100% 2959/2959 [03:02<00:00, 16.20it/s]\n",
            " Train Epoch: 10 Loss: 0.1831 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.13it/s]\n",
            " Val Epoch: 10 Avg loss: 0.5564 Acc: 0.84\n",
            "100% 2959/2959 [03:03<00:00, 16.16it/s]\n",
            " Train Epoch: 11 Loss: 0.1670 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.04it/s]\n",
            " Val Epoch: 11 Avg loss: 0.5351 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.14it/s]\n",
            " Train Epoch: 12 Loss: 0.1541 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.20it/s]\n",
            " Val Epoch: 12 Avg loss: 0.5141 Acc: 0.85\n",
            "100% 2959/2959 [03:02<00:00, 16.23it/s]\n",
            " Train Epoch: 13 Loss: 0.1431 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 13 Avg loss: 0.5849 Acc: 0.85\n",
            "100% 2959/2959 [03:04<00:00, 16.08it/s]\n",
            " Train Epoch: 14 Loss: 0.1349 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 14 Avg loss: 0.6206 Acc: 0.84\n",
            "100% 2959/2959 [03:03<00:00, 16.16it/s]\n",
            " Train Epoch: 15 Loss: 0.1276 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.81it/s]\n",
            " Val Epoch: 15 Avg loss: 0.6303 Acc: 0.84\n",
            "100% 2959/2959 [03:02<00:00, 16.18it/s]\n",
            " Train Epoch: 16 Loss: 0.1212 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.98it/s]\n",
            " Val Epoch: 16 Avg loss: 0.5901 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.12it/s]\n",
            " Train Epoch: 17 Loss: 0.1165 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.94it/s]\n",
            " Val Epoch: 17 Avg loss: 0.5978 Acc: 0.85\n",
            "100% 2959/2959 [03:02<00:00, 16.17it/s]\n",
            " Train Epoch: 18 Loss: 0.1085 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.13it/s]\n",
            " Val Epoch: 18 Avg loss: 0.5900 Acc: 0.85\n",
            "100% 2959/2959 [03:04<00:00, 16.08it/s]\n",
            " Train Epoch: 19 Loss: 0.1070 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.18it/s]\n",
            " Val Epoch: 19 Avg loss: 0.6590 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.14it/s]\n",
            " Train Epoch: 20 Loss: 0.1022 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.88it/s]\n",
            " Val Epoch: 20 Avg loss: 0.6655 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.16it/s]\n",
            " Train Epoch: 21 Loss: 0.0979 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.03it/s]\n",
            " Val Epoch: 21 Avg loss: 0.6836 Acc: 0.85\n",
            "100% 2959/2959 [03:02<00:00, 16.17it/s]\n",
            " Train Epoch: 22 Loss: 0.0928 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.96it/s]\n",
            " Val Epoch: 22 Avg loss: 0.6642 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.11it/s]\n",
            " Train Epoch: 23 Loss: 0.0902 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 23 Avg loss: 0.7301 Acc: 0.84\n",
            "100% 2959/2959 [03:03<00:00, 16.16it/s]\n",
            " Train Epoch: 24 Loss: 0.0865 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 24 Avg loss: 0.7095 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.10it/s]\n",
            " Train Epoch: 25 Loss: 0.0847 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 25 Avg loss: 0.7257 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.11it/s]\n",
            " Train Epoch: 26 Loss: 0.0798 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.74it/s]\n",
            " Val Epoch: 26 Avg loss: 0.7494 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.14it/s]\n",
            " Train Epoch: 27 Loss: 0.0800 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.86it/s]\n",
            " Val Epoch: 27 Avg loss: 0.7391 Acc: 0.85\n",
            "100% 2959/2959 [03:04<00:00, 16.08it/s]\n",
            " Train Epoch: 28 Loss: 0.0777 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.95it/s]\n",
            " Val Epoch: 28 Avg loss: 0.6919 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.11it/s]\n",
            " Train Epoch: 29 Loss: 0.0750 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.63it/s]\n",
            " Val Epoch: 29 Avg loss: 0.7950 Acc: 0.85\n",
            "100% 2959/2959 [03:03<00:00, 16.11it/s]\n",
            " Train Epoch: 30 Loss: 0.0716 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.98it/s]\n",
            " Val Epoch: 30 Avg loss: 0.8925 Acc: 0.84\n",
            "100% 2959/2959 [03:03<00:00, 16.16it/s]\n",
            " Train Epoch: 31 Loss: 0.0709 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.87it/s]\n",
            " Val Epoch: 31 Avg loss: 0.8222 Acc: 0.84\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_no_augmentation.py -m train -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIGlPHaLFbjg",
        "outputId": "6f49fa1d-ef4f-40ab-91a4-b133318ba3df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-11 08:36:07.717898: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 08:36:07.764583: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 08:36:08.638215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 20.96it/s]\n",
            "Test Acc: 0.8648\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_no_augmentation.py -m test --model_name indof_r50_noaug -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApVfZokc3cbB",
        "outputId": "da6f8fa1-7438-40f6-b1f6-aa52cc33697c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 181MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100% 97.8M/97.8M [00:00<00:00, 153MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100% 171M/171M [00:02<00:00, 76.3MB/s]\n",
            "2023-07-11 12:12:39.423926: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 12:12:39.471829: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 12:12:40.391647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 11698746\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [01:59<00:00, 24.69it/s]\n",
            " Train Epoch: 1 Loss: 0.6799 Acc: 0.79 \n",
            "100% 245/245 [00:09<00:00, 25.21it/s]\n",
            " Val Epoch: 1 Avg loss: 0.6035 Acc: 0.80\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [01:51<00:00, 26.61it/s]\n",
            " Train Epoch: 2 Loss: 0.4830 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 25.86it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5605 Acc: 0.82\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [01:51<00:00, 26.48it/s]\n",
            " Train Epoch: 3 Loss: 0.4027 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 25.41it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5299 Acc: 0.83\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [01:50<00:00, 26.83it/s]\n",
            " Train Epoch: 4 Loss: 0.3410 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.01it/s]\n",
            " Val Epoch: 4 Avg loss: 0.4635 Acc: 0.85\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [01:51<00:00, 26.48it/s]\n",
            " Train Epoch: 5 Loss: 0.2838 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.09it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4782 Acc: 0.85\n",
            "100% 2959/2959 [01:51<00:00, 26.52it/s]\n",
            " Train Epoch: 6 Loss: 0.2387 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 6 Avg loss: 0.4841 Acc: 0.86\n",
            "100% 2959/2959 [01:52<00:00, 26.36it/s]\n",
            " Train Epoch: 7 Loss: 0.2003 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 25.66it/s]\n",
            " Val Epoch: 7 Avg loss: 0.5378 Acc: 0.85\n",
            "100% 2959/2959 [01:51<00:00, 26.50it/s]\n",
            " Train Epoch: 8 Loss: 0.1713 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.82it/s]\n",
            " Val Epoch: 8 Avg loss: 0.6409 Acc: 0.84\n",
            "100% 2959/2959 [01:51<00:00, 26.53it/s]\n",
            " Train Epoch: 9 Loss: 0.1540 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.88it/s]\n",
            " Val Epoch: 9 Avg loss: 0.5849 Acc: 0.85\n",
            "100% 2959/2959 [01:52<00:00, 26.22it/s]\n",
            " Train Epoch: 10 Loss: 0.1362 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.89it/s]\n",
            " Val Epoch: 10 Avg loss: 0.6478 Acc: 0.85\n",
            "100% 2959/2959 [01:50<00:00, 26.70it/s]\n",
            " Train Epoch: 11 Loss: 0.1247 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.89it/s]\n",
            " Val Epoch: 11 Avg loss: 0.6716 Acc: 0.85\n",
            "100% 2959/2959 [01:52<00:00, 26.35it/s]\n",
            " Train Epoch: 12 Loss: 0.1142 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 25.92it/s]\n",
            " Val Epoch: 12 Avg loss: 0.7183 Acc: 0.85\n",
            "100% 2959/2959 [01:53<00:00, 26.18it/s]\n",
            " Train Epoch: 13 Loss: 0.1049 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.07it/s]\n",
            " Val Epoch: 13 Avg loss: 0.7598 Acc: 0.85\n",
            "100% 2959/2959 [01:51<00:00, 26.55it/s]\n",
            " Train Epoch: 14 Loss: 0.1018 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.80it/s]\n",
            " Val Epoch: 14 Avg loss: 0.7923 Acc: 0.85\n",
            "100% 2959/2959 [01:51<00:00, 26.44it/s]\n",
            " Train Epoch: 15 Loss: 0.0937 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.54it/s]\n",
            " Val Epoch: 15 Avg loss: 0.8085 Acc: 0.85\n",
            "100% 2959/2959 [01:52<00:00, 26.33it/s]\n",
            " Train Epoch: 16 Loss: 0.0908 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.39it/s]\n",
            " Val Epoch: 16 Avg loss: 0.8507 Acc: 0.84\n",
            "100% 2959/2959 [01:52<00:00, 26.37it/s]\n",
            " Train Epoch: 17 Loss: 0.0855 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.47it/s]\n",
            " Val Epoch: 17 Avg loss: 0.8685 Acc: 0.84\n",
            "100% 2959/2959 [01:52<00:00, 26.37it/s]\n",
            " Train Epoch: 18 Loss: 0.0852 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.49it/s]\n",
            " Val Epoch: 18 Avg loss: 0.8591 Acc: 0.85\n",
            "100% 2959/2959 [01:51<00:00, 26.50it/s]\n",
            " Train Epoch: 19 Loss: 0.0808 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.85it/s]\n",
            " Val Epoch: 19 Avg loss: 0.9335 Acc: 0.84\n",
            "100% 2959/2959 [01:51<00:00, 26.42it/s]\n",
            " Train Epoch: 20 Loss: 0.0785 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.90it/s]\n",
            " Val Epoch: 20 Avg loss: 1.0008 Acc: 0.85\n",
            "100% 2959/2959 [01:51<00:00, 26.52it/s]\n",
            " Train Epoch: 21 Loss: 0.0750 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.00it/s]\n",
            " Val Epoch: 21 Avg loss: 0.9545 Acc: 0.84\n",
            "100% 2959/2959 [01:52<00:00, 26.40it/s]\n",
            " Train Epoch: 22 Loss: 0.0741 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.78it/s]\n",
            " Val Epoch: 22 Avg loss: 0.8857 Acc: 0.85\n",
            "100% 2959/2959 [01:52<00:00, 26.42it/s]\n",
            " Train Epoch: 23 Loss: 0.0726 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.26it/s]\n",
            " Val Epoch: 23 Avg loss: 0.9580 Acc: 0.85\n",
            "100% 2959/2959 [01:52<00:00, 26.32it/s]\n",
            " Train Epoch: 24 Loss: 0.0691 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.91it/s]\n",
            " Val Epoch: 24 Avg loss: 1.0075 Acc: 0.84\n",
            "100% 2959/2959 [01:52<00:00, 26.39it/s]\n",
            " Train Epoch: 25 Loss: 0.0700 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.78it/s]\n",
            " Val Epoch: 25 Avg loss: 0.9883 Acc: 0.85\n",
            "100% 2959/2959 [01:51<00:00, 26.46it/s]\n",
            " Train Epoch: 26 Loss: 0.0664 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.87it/s]\n",
            " Val Epoch: 26 Avg loss: 1.0624 Acc: 0.84\n",
            "100% 2959/2959 [01:51<00:00, 26.50it/s]\n",
            " Train Epoch: 27 Loss: 0.0669 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.64it/s]\n",
            " Val Epoch: 27 Avg loss: 1.0042 Acc: 0.85\n",
            "100% 2959/2959 [01:52<00:00, 26.40it/s]\n",
            " Train Epoch: 28 Loss: 0.0644 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.93it/s]\n",
            " Val Epoch: 28 Avg loss: 1.0379 Acc: 0.85\n",
            "100% 2959/2959 [01:51<00:00, 26.44it/s]\n",
            " Train Epoch: 29 Loss: 0.0638 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.45it/s]\n",
            " Val Epoch: 29 Avg loss: 1.1246 Acc: 0.84\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_no_augmentation.py -m train -a resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTeAatZ6HRAg",
        "outputId": "c73bcb25-9710-4cc9-f66f-f17f0ccf3017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-11 13:23:02.073044: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 13:23:02.118820: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 13:23:02.989779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 11698746\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.55it/s]\n",
            "Test Acc: 0.8682\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_no_augmentation.py -m test --model_name indof_r18_noaug -a resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a68yPQt-Hv6f",
        "outputId": "0e24e576-1051-4b84-a739-681584360de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-11 13:24:02.115851: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-11 13:24:02.161351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 13:24:03.032733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 44586042\n",
            "Loading Saved Model\n",
            "100% 2959/2959 [04:39<00:00, 10.57it/s]\n",
            " Train Epoch: 1 Loss: 0.8109 Acc: 0.74 \n",
            "100% 245/245 [00:09<00:00, 25.50it/s]\n",
            " Val Epoch: 1 Avg loss: 0.7458 Acc: 0.75\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:39<00:00, 10.59it/s]\n",
            " Train Epoch: 2 Loss: 0.5430 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 26.00it/s]\n",
            " Val Epoch: 2 Avg loss: 0.5827 Acc: 0.81\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:39<00:00, 10.60it/s]\n",
            " Train Epoch: 3 Loss: 0.4594 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.39it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5522 Acc: 0.82\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:39<00:00, 10.59it/s]\n",
            " Train Epoch: 4 Loss: 0.4016 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.17it/s]\n",
            " Val Epoch: 4 Avg loss: 0.5357 Acc: 0.83\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:38<00:00, 10.64it/s]\n",
            " Train Epoch: 5 Loss: 0.3548 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.26it/s]\n",
            " Val Epoch: 5 Avg loss: 0.4842 Acc: 0.84\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:40<00:00, 10.54it/s]\n",
            " Train Epoch: 6 Loss: 0.3098 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 25.75it/s]\n",
            " Val Epoch: 6 Avg loss: 0.5002 Acc: 0.84\n",
            "100% 2959/2959 [04:40<00:00, 10.56it/s]\n",
            " Train Epoch: 7 Loss: 0.2710 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 7 Avg loss: 0.5029 Acc: 0.84\n",
            "100% 2959/2959 [04:39<00:00, 10.58it/s]\n",
            " Train Epoch: 8 Loss: 0.2381 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4724 Acc: 0.86\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [04:40<00:00, 10.57it/s]\n",
            " Train Epoch: 9 Loss: 0.2081 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 9 Avg loss: 0.5033 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.60it/s]\n",
            " Train Epoch: 10 Loss: 0.1858 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 25.97it/s]\n",
            " Val Epoch: 10 Avg loss: 0.5202 Acc: 0.84\n",
            "100% 2959/2959 [04:38<00:00, 10.61it/s]\n",
            " Train Epoch: 11 Loss: 0.1693 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.46it/s]\n",
            " Val Epoch: 11 Avg loss: 0.5501 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.60it/s]\n",
            " Train Epoch: 12 Loss: 0.1533 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.10it/s]\n",
            " Val Epoch: 12 Avg loss: 0.6344 Acc: 0.83\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 13 Loss: 0.1445 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 13 Avg loss: 0.5593 Acc: 0.85\n",
            "100% 2959/2959 [04:40<00:00, 10.54it/s]\n",
            " Train Epoch: 14 Loss: 0.1352 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.17it/s]\n",
            " Val Epoch: 14 Avg loss: 0.5495 Acc: 0.85\n",
            "100% 2959/2959 [04:38<00:00, 10.61it/s]\n",
            " Train Epoch: 15 Loss: 0.1293 Acc: 0.95 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 15 Avg loss: 0.5889 Acc: 0.84\n",
            "100% 2959/2959 [04:39<00:00, 10.59it/s]\n",
            " Train Epoch: 16 Loss: 0.1214 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.04it/s]\n",
            " Val Epoch: 16 Avg loss: 0.6022 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.58it/s]\n",
            " Train Epoch: 17 Loss: 0.1160 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.01it/s]\n",
            " Val Epoch: 17 Avg loss: 0.6344 Acc: 0.85\n",
            "100% 2959/2959 [04:40<00:00, 10.56it/s]\n",
            " Train Epoch: 18 Loss: 0.1119 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.12it/s]\n",
            " Val Epoch: 18 Avg loss: 0.6229 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.57it/s]\n",
            " Train Epoch: 19 Loss: 0.1048 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.91it/s]\n",
            " Val Epoch: 19 Avg loss: 0.6622 Acc: 0.85\n",
            "100% 2959/2959 [04:41<00:00, 10.52it/s]\n",
            " Train Epoch: 20 Loss: 0.1019 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 20 Avg loss: 0.6748 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.60it/s]\n",
            " Train Epoch: 21 Loss: 0.0992 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 21 Avg loss: 0.6862 Acc: 0.85\n",
            "100% 2959/2959 [04:40<00:00, 10.53it/s]\n",
            " Train Epoch: 22 Loss: 0.0936 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.32it/s]\n",
            " Val Epoch: 22 Avg loss: 0.6914 Acc: 0.85\n",
            "100% 2959/2959 [04:39<00:00, 10.58it/s]\n",
            " Train Epoch: 23 Loss: 0.0934 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 23 Avg loss: 0.7314 Acc: 0.84\n",
            "100% 2959/2959 [04:40<00:00, 10.54it/s]\n",
            " Train Epoch: 24 Loss: 0.0899 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 24 Avg loss: 0.7323 Acc: 0.84\n",
            "100% 2959/2959 [04:41<00:00, 10.53it/s]\n",
            " Train Epoch: 25 Loss: 0.0843 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.18it/s]\n",
            " Val Epoch: 25 Avg loss: 0.7844 Acc: 0.84\n",
            "100% 2959/2959 [04:40<00:00, 10.54it/s]\n",
            " Train Epoch: 26 Loss: 0.0833 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 25.91it/s]\n",
            " Val Epoch: 26 Avg loss: 0.7344 Acc: 0.85\n",
            "100% 2959/2959 [04:40<00:00, 10.54it/s]\n",
            " Train Epoch: 27 Loss: 0.0807 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.02it/s]\n",
            " Val Epoch: 27 Avg loss: 0.7753 Acc: 0.84\n",
            "100% 2959/2959 [04:38<00:00, 10.63it/s]\n",
            " Train Epoch: 28 Loss: 0.0790 Acc: 0.96 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 28 Avg loss: 0.7909 Acc: 0.84\n",
            "100% 2959/2959 [04:40<00:00, 10.53it/s]\n",
            " Train Epoch: 29 Loss: 0.0767 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 29 Avg loss: 0.8210 Acc: 0.84\n",
            "100% 2959/2959 [04:39<00:00, 10.58it/s]\n",
            " Train Epoch: 30 Loss: 0.0742 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.33it/s]\n",
            " Val Epoch: 30 Avg loss: 0.7952 Acc: 0.84\n",
            "100% 2959/2959 [04:39<00:00, 10.58it/s]\n",
            " Train Epoch: 31 Loss: 0.0734 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 31 Avg loss: 0.8951 Acc: 0.84\n",
            "100% 2959/2959 [04:40<00:00, 10.55it/s]\n",
            " Train Epoch: 32 Loss: 0.0710 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 25.81it/s]\n",
            " Val Epoch: 32 Avg loss: 0.8868 Acc: 0.84\n",
            "100% 2959/2959 [04:42<00:00, 10.48it/s]\n",
            " Train Epoch: 33 Loss: 0.0702 Acc: 0.97 \n",
            "100% 245/245 [00:09<00:00, 26.12it/s]\n",
            " Val Epoch: 33 Avg loss: 0.9124 Acc: 0.84\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_no_augmentation.py -m train -a resnet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhFGZAnks2XG",
        "outputId": "2a4c3a8f-e0db-431d-bc8c-416a7136e9a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-12 15:41:30.475197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-12 15:41:31.598608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 44586042\n",
            "Loading Saved Model\n",
            "Model Not Found\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer.py -m test --model_name iew_r101 -a resnet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTZ3lgSDbMMe",
        "outputId": "d7d81c80-6d02-4bf1-ec0f-7952c1567b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 162MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100% 97.8M/97.8M [00:00<00:00, 193MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100% 171M/171M [00:01<00:00, 115MB/s]\n",
            "2023-07-12 15:22:05.254829: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-12 15:22:06.715379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Model Not Found\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_flip_jitter.py -m test --model_name indof_r50_jitter_flip -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_coZz1xUwVWt",
        "outputId": "b83a2803-6da5-4ebc-8c4c-3d9314927680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34mIndoFashion\u001b[0m/  \u001b[01;34mIndoFashionDataset\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o53c6iVvwXa_",
        "outputId": "1c1459c0-0103-433e-b714-0842a6e6c39e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/IndoFashion\n"
          ]
        }
      ],
      "source": [
        "cd IndoFashion/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrHPMFxGwdto",
        "outputId": "1997fdcf-8099-4597-df82-39b6d24a0882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdocs\u001b[0m/                       img_trainer_no_augmentation.py  README.md\n",
            "\u001b[01;34mimages\u001b[0m/                     img_trainer.py                  requirements.txt\n",
            "img_trainer_flip_jitter.py  LICENSE                         \u001b[01;34mtf_logs\u001b[0m/\n",
            "img_trainer_flip.py         \u001b[01;34mmodel_archs\u001b[0m/                    \u001b[01;34mutils\u001b[0m/\n",
            "img_trainer_jitter.py       \u001b[01;34mmodels\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAkpG6wlwgvZ",
        "outputId": "a62010bb-f037-4a3e-c748-a6df180e1baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/IndoFashion/models\n"
          ]
        }
      ],
      "source": [
        "cd models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ih46nAxwjJ5",
        "outputId": "f822bc8a-5a88-4147-a330-19a1aadb2778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"\"\"\n",
            "    Script File to train for Cloth Classification task\n",
            "\"\"\"\n",
            "import os\n",
            "import torch\n",
            "import argparse\n",
            "import torch.utils.data\n",
            "from tqdm import tqdm\n",
            "import torch.optim as optim\n",
            "import utils.config as config\n",
            "from model_archs.img_models import ResNetFeaturesFlatten\n",
            "from utils import tflogger\n",
            "from utils.common_utils import image_transform, image_transform_jitter_flip, get_accuracy\n",
            "from utils.dataset import EthnicFinderDataset\n",
            "import numpy as np\n",
            "\n",
            "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
            "parser.add_argument('-m', '--mode', type=str, default='test',\n",
            "                    help=\"mode, {'\" + \"train\" + \"', '\" + \"test\" + \"'}\")\n",
            "parser.add_argument('-n', '--model_name', type=str,\n",
            "                    default='my_model',\n",
            "                    help='Model Name')\n",
            "parser.add_argument('-a', '--architecture', type=str, default='resnet18',\n",
            "                    help=\"model architecture, {'\" + \"', '\".join(config.MODELS.keys()) + \"'}\")\n",
            "\n",
            "args = parser.parse_args()\n",
            "\n",
            "model_name = args.model_name\n",
            "\n",
            "# Dataloaders\n",
            "train_set = EthnicFinderDataset(metadata_file=config.train_file, mode='train', transform=image_transform_jitter_flip)\n",
            "train_loader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
            "val_set = EthnicFinderDataset(metadata_file=config.val_file, mode='val', transform=image_transform)\n",
            "val_loader = torch.utils.data.DataLoader(val_set, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
            "test_set = EthnicFinderDataset(metadata_file=config.test_file, mode='test', transform=image_transform)\n",
            "test_loader = torch.utils.data.DataLoader(test_set, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
            "\n",
            "# Model\n",
            "cloth_model = ResNetFeaturesFlatten(model_key=args.architecture).to(config.device)\n",
            "\n",
            "# Optimizer\n",
            "optimizer = optim.Adam(cloth_model.parameters(), lr=config.lr)\n",
            "print(\"Total Params\", sum(p.numel() for p in cloth_model.parameters() if p.requires_grad))\n",
            "\n",
            "# Logger\n",
            "logger = tflogger.Logger(model_name=model_name, data_name='ours',\n",
            "                         log_path=os.path.join(config.BASE_DIR, 'tf_logs', model_name))\n",
            "\n",
            "\n",
            "def train_epoch(epoch):\n",
            "    \"\"\"\n",
            "        Runs one training epoch\n",
            "\n",
            "        Args:\n",
            "            epoch (int): Epoch number\n",
            "    \"\"\"\n",
            "    train_loss = 0.\n",
            "    total = 0.\n",
            "    correct = 0.\n",
            "    cloth_model.train()\n",
            "    # Training loop\n",
            "    for batch_idx, (image, label) in enumerate(tqdm(train_loader)):\n",
            "        image = image.to(config.device)\n",
            "        label = label.to(config.device)\n",
            "\n",
            "        batch = image.shape[0]\n",
            "        with torch.set_grad_enabled(True):\n",
            "            y_pred = cloth_model(image)\n",
            "            loss = config.ce_criterion(y_pred, label)\n",
            "            loss.backward()\n",
            "            train_loss += float(loss.item())\n",
            "            optimizer.step()\n",
            "            optimizer.zero_grad()  # clear gradients for this training step\n",
            "            correct += get_accuracy(y_pred, label)\n",
            "            total += batch\n",
            "            torch.cuda.empty_cache()\n",
            "            del image, label, y_pred\n",
            "\n",
            "    # Calculate loss and accuracy for current epoch\n",
            "    logger.log(mode=\"train\", scalar_value=train_loss / len(train_loader), epoch=epoch, scalar_name='loss')\n",
            "    logger.log(mode=\"train\", scalar_value=correct / total, epoch=epoch, scalar_name='accuracy')\n",
            "\n",
            "    print(' Train Epoch: {} Loss: {:.4f} Acc: {:.2f} '.format(epoch, train_loss / len(train_loader), correct / total))\n",
            "\n",
            "\n",
            "def eval_epoch(epoch):\n",
            "    \"\"\"\n",
            "        Runs one evaluation epoch\n",
            "\n",
            "        Args:\n",
            "            epoch (int): Epoch number\n",
            "    \"\"\"\n",
            "    cloth_model.eval()\n",
            "    val_loss = 0.\n",
            "    total = 0.\n",
            "    correct = 0.\n",
            "    with torch.no_grad():\n",
            "        for batch_idx, (image, label) in enumerate(tqdm(val_loader, desc='')):\n",
            "            image = image.to(config.device)\n",
            "            label = label.to(config.device)\n",
            "            batch = image.shape[0]\n",
            "            y_pred = cloth_model(image)\n",
            "            loss = config.ce_criterion(y_pred, label)\n",
            "            val_loss += float(loss.item())\n",
            "            correct += get_accuracy(y_pred, label)\n",
            "            total += batch\n",
            "            torch.cuda.empty_cache()\n",
            "            del image, label, y_pred\n",
            "\n",
            "        logger.log(mode=\"val\", scalar_value=val_loss / len(val_loader), epoch=epoch, scalar_name='loss')\n",
            "        logger.log(mode=\"val\", scalar_value=correct / total, epoch=epoch, scalar_name='accuracy')\n",
            "\n",
            "        print(' Val Epoch: {} Avg loss: {:.4f} Acc: {:.2f}'.format(epoch, val_loss / len(val_loader), correct / total))\n",
            "    return val_loss\n",
            "\n",
            "\n",
            "def train_model():\n",
            "    \"\"\"\n",
            "        Trains and evaluates the classification model\n",
            "    \"\"\"\n",
            "    try:\n",
            "        print(\"Loading Saved Model\")\n",
            "        checkpoint = torch.load(config.BASE_DIR + 'models/' + model_name + '.pt')\n",
            "        cloth_model.load_state_dict(checkpoint)\n",
            "        print(\"Saved Model successfully loaded\")\n",
            "        cloth_model.eval()\n",
            "        best_loss = eval_classifier_full()\n",
            "    except:\n",
            "        best_loss = np.Inf\n",
            "    early_stop = False\n",
            "    counter = 0\n",
            "    for epoch in range(1, config.epochs + 1):\n",
            "        # Training epoch\n",
            "        train_epoch(epoch)\n",
            "        # Validation epoch\n",
            "        avg_test_loss = eval_epoch(epoch)\n",
            "        if avg_test_loss <= best_loss:\n",
            "            counter = 0\n",
            "            best_loss = avg_test_loss\n",
            "            torch.save(cloth_model.state_dict(), config.BASE_DIR + 'models/' + model_name + '.pt')\n",
            "            print(\"Best model saved/updated..\")\n",
            "            torch.cuda.empty_cache()\n",
            "        else:\n",
            "            counter += 1\n",
            "            if counter >= config.patience:\n",
            "                early_stop = True\n",
            "        # If early stopping flag is true, then stop the training\n",
            "        if early_stop:\n",
            "            print(\"Early stopping\")\n",
            "            break\n",
            "\n",
            "\n",
            "def eval_classifier_full():\n",
            "    \"\"\"\n",
            "        Evaluates validation loss for computing previous best loss when model weights are loaded from memory\n",
            "    \"\"\"\n",
            "    val_loss = 0.\n",
            "    with torch.no_grad():\n",
            "        for batch_idx, (image, label) in enumerate(tqdm(val_loader, desc='')):\n",
            "            image = image.to(config.device)\n",
            "            label = label.to(config.device)\n",
            "            y_pred = cloth_model(image)\n",
            "            loss = config.ce_criterion(y_pred, label)\n",
            "            val_loss += loss.item()\n",
            "            torch.cuda.empty_cache()\n",
            "            del image, label, y_pred\n",
            "        print(' Val Avg loss: {:.4f}'.format(val_loss / len(val_loader)))\n",
            "    return val_loss\n",
            "\n",
            "\n",
            "def test_classifier():\n",
            "    \"\"\"\n",
            "        Test Classification Accuracy on test set after training\n",
            "    \"\"\"\n",
            "    try:\n",
            "        print(\"Loading Saved Model\")\n",
            "        checkpoint = torch.load(config.BASE_DIR + 'models/' + model_name + '.pt')\n",
            "        cloth_model.load_state_dict(checkpoint)\n",
            "        print(\"Saved Model successfully loaded\")\n",
            "        cloth_model.eval()\n",
            "    except:\n",
            "        print(\"Model Not Found\")\n",
            "        exit()\n",
            "    total = 0.\n",
            "    correct = 0.\n",
            "    labels = torch.LongTensor([])\n",
            "    y_preds = torch.LongTensor([])\n",
            "\n",
            "    with torch.no_grad():\n",
            "        for batch_idx, (image, label) in enumerate(tqdm(test_loader, desc='')):\n",
            "            image = image.to(config.device)\n",
            "            label = label.to(config.device)\n",
            "            batch = image.shape[0]\n",
            "            y_pred = cloth_model(image)\n",
            "            _, y_pred_cls = torch.max(y_pred, 1)\n",
            "            labels = torch.cat([labels, label.cpu()])\n",
            "            y_preds = torch.cat([y_preds, y_pred_cls.cpu()])\n",
            "            correct += get_accuracy(y_pred, label)\n",
            "            total += batch\n",
            "            torch.cuda.empty_cache()\n",
            "        print('Test Acc: {:.4f}'.format(correct / total))\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "\n",
            "    if args.mode == \"train\":\n",
            "        train_model()\n",
            "    elif args.mode == \"test\":\n",
            "        test_classifier()\n"
          ]
        }
      ],
      "source": [
        "cat /content/IndoFashion/img_trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvXDx4VM2tFj",
        "outputId": "9912eb08-0b33-4efa-c4ca-0f558511a9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-21 12:29:07.785111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-21 12:29:08.899251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Total Params 44586042\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [23:48<00:00,  5.85s/it]\n",
            "Test Acc: 0.8703\n",
            "Precision: 0.8806 Recall: 0.8703 F1-Score: 0.8701 Error Rate: 0.1297\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_no_augmentation.py -m test --model_name indof_r101_noaug -a resnet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-fSIm77o5ne",
        "outputId": "65456628-0477-4ff8-c5fd-3778c62d9e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-31 01:56:36.213434: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-31 01:56:36.262826: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-31 01:56:37.298290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Metadata file: /content/IndoFashionDataset/annotations/train_data.json\n",
            "Number of samples in the dataset: 94677\n",
            "Metadata file: /content/IndoFashionDataset/annotations/val_data.json\n",
            "Number of samples in the dataset: 7809\n",
            "Metadata file: /content/IndoFashionDataset/annotations/test_data.json\n",
            "Number of samples in the dataset: 7808\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Number of classes: 18\n",
            "Class counts: {'saree': 10791, 'women_kurta': 11694, 'lehenga': 5745, 'blouse': 9174, 'gowns': 5211, 'dupattas': 6585, 'leggings_and_salwars': 7786, 'palazzos': 3374, 'petticoats': 4419, 'mojaris_women': 3228, 'dhoti_pants': 4145, 'kurta_men': 6951, 'nehru_jackets': 6489, 'sherwanis': 2990, 'mojaris_men': 2546, 'men_pagdi': 923, 'women_anarkali_kurta': 1439, 'women_a_line_kurta': 1187}\n",
            "Class weights: [9.266981743187195e-05, 8.55139387647072e-05, 0.00017406440379911847, 0.00010900370611412647, 0.0001919017462690651, 0.00015186028851148667, 0.0001284356537209818, 0.00029638411372365616, 0.00022629554192661334, 0.00030978934315062286, 0.00024125452346411227, 0.00014386419218186388, 0.00015410695019970613, 0.0003344481604232615, 0.0003927729770648967, 0.0010834236174610795, 0.0006949270321786469, 0.0008424599824410614]\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:39<00:00, 18.53it/s]\n",
            " Train Epoch: 1 Loss: 1.1793 Acc: 0.62 \n",
            "100% 245/245 [00:09<00:00, 26.03it/s]\n",
            " Val Epoch: 1 Avg loss: 0.7862 Acc: 0.74 Precision: 0.7538 Recall: 0.7403 F1-Score: 0.7351 Error Rate: 0.2597\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:31<00:00, 19.58it/s]\n",
            " Train Epoch: 2 Loss: 0.8016 Acc: 0.74 \n",
            "100% 245/245 [00:09<00:00, 26.50it/s]\n",
            " Val Epoch: 2 Avg loss: 0.7706 Acc: 0.75 Precision: 0.7928 Recall: 0.7462 F1-Score: 0.7569 Error Rate: 0.2538\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.86it/s]\n",
            " Train Epoch: 3 Loss: 0.6880 Acc: 0.78 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 3 Avg loss: 0.6396 Acc: 0.79 Precision: 0.8010 Recall: 0.7911 F1-Score: 0.7924 Error Rate: 0.2089\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.68it/s]\n",
            " Train Epoch: 4 Loss: 0.6185 Acc: 0.80 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 4 Avg loss: 0.5506 Acc: 0.82 Precision: 0.8373 Recall: 0.8234 F1-Score: 0.8257 Error Rate: 0.1766\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.61it/s]\n",
            " Train Epoch: 5 Loss: 0.5613 Acc: 0.82 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 5 Avg loss: 0.5343 Acc: 0.83 Precision: 0.8316 Recall: 0.8262 F1-Score: 0.8267 Error Rate: 0.1738\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:28<00:00, 19.88it/s]\n",
            " Train Epoch: 6 Loss: 0.5319 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 6 Avg loss: 0.5704 Acc: 0.80 Precision: 0.8147 Recall: 0.8043 F1-Score: 0.8016 Error Rate: 0.1957\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:31<00:00, 19.54it/s]\n",
            " Train Epoch: 7 Loss: 0.4974 Acc: 0.84 \n",
            "100% 245/245 [00:09<00:00, 26.24it/s]\n",
            " Val Epoch: 7 Avg loss: 0.5070 Acc: 0.84 Precision: 0.8453 Recall: 0.8386 F1-Score: 0.8380 Error Rate: 0.1614\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.68it/s]\n",
            " Train Epoch: 8 Loss: 0.4751 Acc: 0.84 \n",
            "100% 245/245 [00:09<00:00, 26.23it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4983 Acc: 0.84 Precision: 0.8438 Recall: 0.8354 F1-Score: 0.8363 Error Rate: 0.1646\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:31<00:00, 19.56it/s]\n",
            " Train Epoch: 9 Loss: 0.4498 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.13it/s]\n",
            " Val Epoch: 9 Avg loss: 0.5009 Acc: 0.84 Precision: 0.8462 Recall: 0.8407 F1-Score: 0.8400 Error Rate: 0.1593\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.76it/s]\n",
            " Train Epoch: 10 Loss: 0.4302 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4580 Acc: 0.85 Precision: 0.8513 Recall: 0.8457 F1-Score: 0.8458 Error Rate: 0.1543\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.66it/s]\n",
            " Train Epoch: 11 Loss: 0.4104 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.26it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4655 Acc: 0.85 Precision: 0.8534 Recall: 0.8480 F1-Score: 0.8490 Error Rate: 0.1520\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.60it/s]\n",
            " Train Epoch: 12 Loss: 0.3952 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.18it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4323 Acc: 0.86 Precision: 0.8619 Recall: 0.8559 F1-Score: 0.8557 Error Rate: 0.1441\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.83it/s]\n",
            " Train Epoch: 13 Loss: 0.3816 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.17it/s]\n",
            " Val Epoch: 13 Avg loss: 0.4638 Acc: 0.85 Precision: 0.8579 Recall: 0.8516 F1-Score: 0.8507 Error Rate: 0.1484\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.77it/s]\n",
            " Train Epoch: 14 Loss: 0.3680 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4728 Acc: 0.85 Precision: 0.8575 Recall: 0.8497 F1-Score: 0.8494 Error Rate: 0.1503\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:31<00:00, 19.53it/s]\n",
            " Train Epoch: 15 Loss: 0.3528 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 25.67it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4596 Acc: 0.85 Precision: 0.8604 Recall: 0.8539 F1-Score: 0.8544 Error Rate: 0.1461\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:31<00:00, 19.55it/s]\n",
            " Train Epoch: 16 Loss: 0.3370 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.24it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4830 Acc: 0.84 Precision: 0.8517 Recall: 0.8449 F1-Score: 0.8449 Error Rate: 0.1551\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:32<00:00, 19.46it/s]\n",
            " Train Epoch: 17 Loss: 0.3295 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.12it/s]\n",
            " Val Epoch: 17 Avg loss: 0.4394 Acc: 0.86 Precision: 0.8628 Recall: 0.8607 F1-Score: 0.8603 Error Rate: 0.1393\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.60it/s]\n",
            " Train Epoch: 18 Loss: 0.3138 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.26it/s]\n",
            " Val Epoch: 18 Avg loss: 0.4487 Acc: 0.85 Precision: 0.8591 Recall: 0.8541 F1-Score: 0.8545 Error Rate: 0.1459\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:31<00:00, 19.56it/s]\n",
            " Train Epoch: 19 Loss: 0.3056 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.02it/s]\n",
            " Val Epoch: 19 Avg loss: 0.4200 Acc: 0.86 Precision: 0.8683 Recall: 0.8649 F1-Score: 0.8652 Error Rate: 0.1351\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.70it/s]\n",
            " Train Epoch: 20 Loss: 0.2911 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 20 Avg loss: 0.4982 Acc: 0.85 Precision: 0.8601 Recall: 0.8491 F1-Score: 0.8518 Error Rate: 0.1509\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:28<00:00, 19.96it/s]\n",
            " Train Epoch: 21 Loss: 0.2837 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.48it/s]\n",
            " Val Epoch: 21 Avg loss: 0.4570 Acc: 0.85 Precision: 0.8558 Recall: 0.8515 F1-Score: 0.8514 Error Rate: 0.1485\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.83it/s]\n",
            " Train Epoch: 22 Loss: 0.2743 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 22 Avg loss: 0.4390 Acc: 0.87 Precision: 0.8673 Recall: 0.8650 F1-Score: 0.8645 Error Rate: 0.1350\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.85it/s]\n",
            " Train Epoch: 23 Loss: 0.2697 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 23 Avg loss: 0.4548 Acc: 0.86 Precision: 0.8684 Recall: 0.8636 F1-Score: 0.8634 Error Rate: 0.1364\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.80it/s]\n",
            " Train Epoch: 24 Loss: 0.2551 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.43it/s]\n",
            " Val Epoch: 24 Avg loss: 0.4844 Acc: 0.85 Precision: 0.8575 Recall: 0.8539 F1-Score: 0.8545 Error Rate: 0.1461\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:31<00:00, 19.56it/s]\n",
            " Train Epoch: 25 Loss: 0.2518 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 25 Avg loss: 0.4345 Acc: 0.87 Precision: 0.8691 Recall: 0.8667 F1-Score: 0.8664 Error Rate: 0.1333\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.61it/s]\n",
            " Train Epoch: 26 Loss: 0.2422 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.47it/s]\n",
            " Val Epoch: 26 Avg loss: 0.4472 Acc: 0.86 Precision: 0.8637 Recall: 0.8620 F1-Score: 0.8618 Error Rate: 0.1380\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.82it/s]\n",
            " Train Epoch: 27 Loss: 0.2355 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 27 Avg loss: 0.4551 Acc: 0.87 Precision: 0.8673 Recall: 0.8662 F1-Score: 0.8657 Error Rate: 0.1338\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.64it/s]\n",
            " Train Epoch: 28 Loss: 0.2281 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 28 Avg loss: 0.4701 Acc: 0.86 Precision: 0.8710 Recall: 0.8643 F1-Score: 0.8652 Error Rate: 0.1357\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.67it/s]\n",
            " Train Epoch: 29 Loss: 0.2219 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 25.99it/s]\n",
            " Val Epoch: 29 Avg loss: 0.6023 Acc: 0.84 Precision: 0.8544 Recall: 0.8351 F1-Score: 0.8400 Error Rate: 0.1649\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.82it/s]\n",
            " Train Epoch: 30 Loss: 0.2144 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.36it/s]\n",
            " Val Epoch: 30 Avg loss: 0.4967 Acc: 0.86 Precision: 0.8629 Recall: 0.8617 F1-Score: 0.8608 Error Rate: 0.1383\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.79it/s]\n",
            " Train Epoch: 31 Loss: 0.2097 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.04it/s]\n",
            " Val Epoch: 31 Avg loss: 0.5066 Acc: 0.86 Precision: 0.8619 Recall: 0.8573 F1-Score: 0.8575 Error Rate: 0.1427\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:31<00:00, 19.58it/s]\n",
            " Train Epoch: 32 Loss: 0.2034 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 32 Avg loss: 0.4758 Acc: 0.86 Precision: 0.8678 Recall: 0.8646 F1-Score: 0.8651 Error Rate: 0.1354\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.85it/s]\n",
            " Train Epoch: 33 Loss: 0.1972 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.18it/s]\n",
            " Val Epoch: 33 Avg loss: 0.5065 Acc: 0.86 Precision: 0.8598 Recall: 0.8598 F1-Score: 0.8587 Error Rate: 0.1402\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.64it/s]\n",
            " Train Epoch: 34 Loss: 0.1902 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.39it/s]\n",
            " Val Epoch: 34 Avg loss: 0.5129 Acc: 0.86 Precision: 0.8592 Recall: 0.8566 F1-Score: 0.8565 Error Rate: 0.1434\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.86it/s]\n",
            " Train Epoch: 35 Loss: 0.1910 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 35 Avg loss: 0.4792 Acc: 0.86 Precision: 0.8651 Recall: 0.8645 F1-Score: 0.8641 Error Rate: 0.1355\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:30<00:00, 19.69it/s]\n",
            " Train Epoch: 36 Loss: 0.1780 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.23it/s]\n",
            " Val Epoch: 36 Avg loss: 0.5260 Acc: 0.86 Precision: 0.8664 Recall: 0.8646 F1-Score: 0.8647 Error Rate: 0.1354\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:28<00:00, 19.94it/s]\n",
            " Train Epoch: 37 Loss: 0.1833 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.44it/s]\n",
            " Val Epoch: 37 Avg loss: 0.5383 Acc: 0.86 Precision: 0.8696 Recall: 0.8649 F1-Score: 0.8647 Error Rate: 0.1351\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:28<00:00, 19.87it/s]\n",
            " Train Epoch: 38 Loss: 0.1745 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 38 Avg loss: 0.5151 Acc: 0.86 Precision: 0.8646 Recall: 0.8631 F1-Score: 0.8633 Error Rate: 0.1369\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.80it/s]\n",
            " Train Epoch: 39 Loss: 0.1680 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 39 Avg loss: 0.5360 Acc: 0.86 Precision: 0.8634 Recall: 0.8599 F1-Score: 0.8588 Error Rate: 0.1401\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.83it/s]\n",
            " Train Epoch: 40 Loss: 0.1684 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.40it/s]\n",
            " Val Epoch: 40 Avg loss: 0.5429 Acc: 0.86 Precision: 0.8609 Recall: 0.8588 F1-Score: 0.8581 Error Rate: 0.1412\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.81it/s]\n",
            " Train Epoch: 41 Loss: 0.1620 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.13it/s]\n",
            " Val Epoch: 41 Avg loss: 0.5378 Acc: 0.86 Precision: 0.8620 Recall: 0.8618 F1-Score: 0.8613 Error Rate: 0.1382\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.84it/s]\n",
            " Train Epoch: 42 Loss: 0.1591 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.36it/s]\n",
            " Val Epoch: 42 Avg loss: 0.5630 Acc: 0.86 Precision: 0.8583 Recall: 0.8568 F1-Score: 0.8563 Error Rate: 0.1432\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.76it/s]\n",
            " Train Epoch: 43 Loss: 0.1584 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.24it/s]\n",
            " Val Epoch: 43 Avg loss: 0.5648 Acc: 0.86 Precision: 0.8668 Recall: 0.8609 F1-Score: 0.8605 Error Rate: 0.1391\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_bright_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:29<00:00, 19.80it/s]\n",
            " Train Epoch: 44 Loss: 0.1516 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 44 Avg loss: 0.5483 Acc: 0.86 Precision: 0.8593 Recall: 0.8568 F1-Score: 0.8565 Error Rate: 0.1432\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_bright_shear.py -m train -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJSZOmS1suOV",
        "outputId": "ba41536a-6e40-4250-9a5f-1b68d38fa967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-31 03:53:58.179724: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-31 03:53:58.230866: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-31 03:53:59.215802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Metadata file: /content/IndoFashionDataset/annotations/train_data.json\n",
            "Number of samples in the dataset: 94677\n",
            "Metadata file: /content/IndoFashionDataset/annotations/val_data.json\n",
            "Number of samples in the dataset: 7809\n",
            "Metadata file: /content/IndoFashionDataset/annotations/test_data.json\n",
            "Number of samples in the dataset: 7808\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.35it/s]\n",
            "Test Acc: 0.8773\n",
            "Precision: 0.8800 Recall: 0.8773 F1-Score: 0.8772 Error Rate: 0.1227\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_bright_shear.py -m test --model_name my_model -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sNSwvHsAtTMq",
        "outputId": "705dbd11-c79e-4656-e809-ff4f8c38c7a0"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_9ee3e43c-b6cb-4861-afca-242a294701eb\", \"my_model.pt\", 102741249)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Define the path to the model file in Colab environment\n",
        "colab_model_path = \"/content/IndoFashion/models/my_model.pt\"\n",
        "\n",
        "# Download the model file\n",
        "files.download(colab_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "80poDtZaIv_A",
        "outputId": "d44a2d69-9490-4bbe-e85f-876c80035857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-31 04:50:11.275857: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-31 04:50:11.325228: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-31 04:50:12.312957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Metadata file: /content/IndoFashionDataset/annotations/train_data.json\n",
            "Number of samples in the dataset: 94677\n",
            "Metadata file: /content/IndoFashionDataset/annotations/val_data.json\n",
            "Number of samples in the dataset: 7809\n",
            "Metadata file: /content/IndoFashionDataset/annotations/test_data.json\n",
            "Number of samples in the dataset: 7808\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Number of classes: 18\n",
            "Class counts: {'saree': 10791, 'women_kurta': 11694, 'lehenga': 5745, 'blouse': 9174, 'gowns': 5211, 'dupattas': 6585, 'leggings_and_salwars': 7786, 'palazzos': 3374, 'petticoats': 4419, 'mojaris_women': 3228, 'dhoti_pants': 4145, 'kurta_men': 6951, 'nehru_jackets': 6489, 'sherwanis': 2990, 'mojaris_men': 2546, 'men_pagdi': 923, 'women_anarkali_kurta': 1439, 'women_a_line_kurta': 1187}\n",
            "Class weights: [9.266981743187195e-05, 8.55139387647072e-05, 0.00017406440379911847, 0.00010900370611412647, 0.0001919017462690651, 0.00015186028851148667, 0.0001284356537209818, 0.00029638411372365616, 0.00022629554192661334, 0.00030978934315062286, 0.00024125452346411227, 0.00014386419218186388, 0.00015410695019970613, 0.0003344481604232615, 0.0003927729770648967, 0.0010834236174610795, 0.0006949270321786469, 0.0008424599824410614]\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:11<00:00, 22.56it/s]\n",
            " Train Epoch: 1 Loss: 1.1966 Acc: 0.62 \n",
            "100% 245/245 [00:09<00:00, 26.18it/s]\n",
            " Val Epoch: 1 Avg loss: 0.9003 Acc: 0.70 Precision: 0.7279 Recall: 0.6983 F1-Score: 0.6925 Error Rate: 0.3017\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.76it/s]\n",
            " Train Epoch: 2 Loss: 0.7949 Acc: 0.75 \n",
            "100% 245/245 [00:09<00:00, 26.15it/s]\n",
            " Val Epoch: 2 Avg loss: 0.7033 Acc: 0.75 Precision: 0.7709 Recall: 0.7514 F1-Score: 0.7481 Error Rate: 0.2486\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.88it/s]\n",
            " Train Epoch: 3 Loss: 0.6869 Acc: 0.78 \n",
            "100% 245/245 [00:09<00:00, 26.32it/s]\n",
            " Val Epoch: 3 Avg loss: 0.6434 Acc: 0.79 Precision: 0.8032 Recall: 0.7915 F1-Score: 0.7902 Error Rate: 0.2085\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.78it/s]\n",
            " Train Epoch: 4 Loss: 0.6202 Acc: 0.80 \n",
            "100% 245/245 [00:09<00:00, 26.32it/s]\n",
            " Val Epoch: 4 Avg loss: 0.5923 Acc: 0.80 Precision: 0.8115 Recall: 0.7988 F1-Score: 0.8007 Error Rate: 0.2012\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.87it/s]\n",
            " Train Epoch: 5 Loss: 0.5682 Acc: 0.82 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 5 Avg loss: 0.5708 Acc: 0.82 Precision: 0.8271 Recall: 0.8173 F1-Score: 0.8179 Error Rate: 0.1827\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.83it/s]\n",
            " Train Epoch: 6 Loss: 0.5320 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 6 Avg loss: 0.6751 Acc: 0.77 Precision: 0.7930 Recall: 0.7742 F1-Score: 0.7703 Error Rate: 0.2258\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 22.96it/s]\n",
            " Train Epoch: 7 Loss: 0.4959 Acc: 0.84 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 7 Avg loss: 0.5402 Acc: 0.81 Precision: 0.8301 Recall: 0.8109 F1-Score: 0.8115 Error Rate: 0.1891\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.86it/s]\n",
            " Train Epoch: 8 Loss: 0.4730 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 25.60it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4667 Acc: 0.84 Precision: 0.8496 Recall: 0.8434 F1-Score: 0.8440 Error Rate: 0.1566\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 22.94it/s]\n",
            " Train Epoch: 9 Loss: 0.4482 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 25.95it/s]\n",
            " Val Epoch: 9 Avg loss: 0.5161 Acc: 0.83 Precision: 0.8331 Recall: 0.8260 F1-Score: 0.8259 Error Rate: 0.1740\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 22.95it/s]\n",
            " Train Epoch: 10 Loss: 0.4251 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.25it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4700 Acc: 0.84 Precision: 0.8494 Recall: 0.8421 F1-Score: 0.8424 Error Rate: 0.1579\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.90it/s]\n",
            " Train Epoch: 11 Loss: 0.4091 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.43it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4893 Acc: 0.84 Precision: 0.8470 Recall: 0.8379 F1-Score: 0.8395 Error Rate: 0.1621\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.83it/s]\n",
            " Train Epoch: 12 Loss: 0.3919 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.17it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4769 Acc: 0.84 Precision: 0.8503 Recall: 0.8425 F1-Score: 0.8426 Error Rate: 0.1575\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 23.01it/s]\n",
            " Train Epoch: 13 Loss: 0.3772 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.20it/s]\n",
            " Val Epoch: 13 Avg loss: 0.5040 Acc: 0.85 Precision: 0.8544 Recall: 0.8452 F1-Score: 0.8447 Error Rate: 0.1548\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.85it/s]\n",
            " Train Epoch: 14 Loss: 0.3618 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.23it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4462 Acc: 0.86 Precision: 0.8650 Recall: 0.8590 F1-Score: 0.8594 Error Rate: 0.1410\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.92it/s]\n",
            " Train Epoch: 15 Loss: 0.3502 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.26it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4482 Acc: 0.85 Precision: 0.8599 Recall: 0.8527 F1-Score: 0.8531 Error Rate: 0.1473\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.85it/s]\n",
            " Train Epoch: 16 Loss: 0.3344 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.23it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4788 Acc: 0.84 Precision: 0.8546 Recall: 0.8445 F1-Score: 0.8465 Error Rate: 0.1555\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 23.04it/s]\n",
            " Train Epoch: 17 Loss: 0.3257 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.30it/s]\n",
            " Val Epoch: 17 Avg loss: 0.4328 Acc: 0.86 Precision: 0.8615 Recall: 0.8564 F1-Score: 0.8566 Error Rate: 0.1436\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.84it/s]\n",
            " Train Epoch: 18 Loss: 0.3158 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 18 Avg loss: 0.4577 Acc: 0.85 Precision: 0.8539 Recall: 0.8500 F1-Score: 0.8500 Error Rate: 0.1500\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 22.96it/s]\n",
            " Train Epoch: 19 Loss: 0.3010 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 19 Avg loss: 0.4671 Acc: 0.85 Precision: 0.8539 Recall: 0.8518 F1-Score: 0.8515 Error Rate: 0.1482\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:11<00:00, 22.59it/s]\n",
            " Train Epoch: 20 Loss: 0.2981 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 20 Avg loss: 0.5022 Acc: 0.84 Precision: 0.8586 Recall: 0.8430 F1-Score: 0.8464 Error Rate: 0.1570\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 22.96it/s]\n",
            " Train Epoch: 21 Loss: 0.2791 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.44it/s]\n",
            " Val Epoch: 21 Avg loss: 0.4423 Acc: 0.86 Precision: 0.8651 Recall: 0.8620 F1-Score: 0.8616 Error Rate: 0.1380\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 23.02it/s]\n",
            " Train Epoch: 22 Loss: 0.2741 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 22 Avg loss: 0.5963 Acc: 0.83 Precision: 0.8530 Recall: 0.8290 F1-Score: 0.8340 Error Rate: 0.1710\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 23.03it/s]\n",
            " Train Epoch: 23 Loss: 0.2657 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 23 Avg loss: 0.4301 Acc: 0.87 Precision: 0.8687 Recall: 0.8659 F1-Score: 0.8656 Error Rate: 0.1341\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 23.04it/s]\n",
            " Train Epoch: 24 Loss: 0.2572 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.47it/s]\n",
            " Val Epoch: 24 Avg loss: 0.4259 Acc: 0.87 Precision: 0.8688 Recall: 0.8669 F1-Score: 0.8671 Error Rate: 0.1331\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.87it/s]\n",
            " Train Epoch: 25 Loss: 0.2491 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.40it/s]\n",
            " Val Epoch: 25 Avg loss: 0.4457 Acc: 0.86 Precision: 0.8610 Recall: 0.8566 F1-Score: 0.8569 Error Rate: 0.1434\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.88it/s]\n",
            " Train Epoch: 26 Loss: 0.2402 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 26 Avg loss: 0.4291 Acc: 0.87 Precision: 0.8697 Recall: 0.8673 F1-Score: 0.8677 Error Rate: 0.1327\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.94it/s]\n",
            " Train Epoch: 27 Loss: 0.2388 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 27 Avg loss: 0.8245 Acc: 0.80 Precision: 0.8453 Recall: 0.8013 F1-Score: 0.8103 Error Rate: 0.1987\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.89it/s]\n",
            " Train Epoch: 28 Loss: 0.2283 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.25it/s]\n",
            " Val Epoch: 28 Avg loss: 0.4423 Acc: 0.87 Precision: 0.8682 Recall: 0.8661 F1-Score: 0.8656 Error Rate: 0.1339\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:10<00:00, 22.70it/s]\n",
            " Train Epoch: 29 Loss: 0.2229 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.38it/s]\n",
            " Val Epoch: 29 Avg loss: 0.5006 Acc: 0.86 Precision: 0.8647 Recall: 0.8593 F1-Score: 0.8577 Error Rate: 0.1407\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.80it/s]\n",
            " Train Epoch: 30 Loss: 0.2156 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 30 Avg loss: 0.6810 Acc: 0.82 Precision: 0.8493 Recall: 0.8178 F1-Score: 0.8245 Error Rate: 0.1822\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:10<00:00, 22.75it/s]\n",
            " Train Epoch: 31 Loss: 0.2114 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.42it/s]\n",
            " Val Epoch: 31 Avg loss: 0.5074 Acc: 0.86 Precision: 0.8633 Recall: 0.8609 F1-Score: 0.8605 Error Rate: 0.1391\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.82it/s]\n",
            " Train Epoch: 32 Loss: 0.2042 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 32 Avg loss: 0.4794 Acc: 0.86 Precision: 0.8622 Recall: 0.8598 F1-Score: 0.8597 Error Rate: 0.1402\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.77it/s]\n",
            " Train Epoch: 33 Loss: 0.2017 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.45it/s]\n",
            " Val Epoch: 33 Avg loss: 0.4798 Acc: 0.86 Precision: 0.8615 Recall: 0.8611 F1-Score: 0.8600 Error Rate: 0.1389\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.83it/s]\n",
            " Train Epoch: 34 Loss: 0.1969 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 25.95it/s]\n",
            " Val Epoch: 34 Avg loss: 0.4781 Acc: 0.87 Precision: 0.8685 Recall: 0.8653 F1-Score: 0.8651 Error Rate: 0.1347\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.84it/s]\n",
            " Train Epoch: 35 Loss: 0.1880 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.07it/s]\n",
            " Val Epoch: 35 Avg loss: 0.4958 Acc: 0.86 Precision: 0.8619 Recall: 0.8590 F1-Score: 0.8591 Error Rate: 0.1410\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.85it/s]\n",
            " Train Epoch: 36 Loss: 0.1860 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 36 Avg loss: 0.4923 Acc: 0.86 Precision: 0.8661 Recall: 0.8607 F1-Score: 0.8611 Error Rate: 0.1393\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 22.98it/s]\n",
            " Train Epoch: 37 Loss: 0.1836 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 37 Avg loss: 0.5484 Acc: 0.85 Precision: 0.8565 Recall: 0.8522 F1-Score: 0.8526 Error Rate: 0.1478\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:10<00:00, 22.72it/s]\n",
            " Train Epoch: 38 Loss: 0.1747 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.35it/s]\n",
            " Val Epoch: 38 Avg loss: 0.4789 Acc: 0.86 Precision: 0.8648 Recall: 0.8630 F1-Score: 0.8624 Error Rate: 0.1370\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.88it/s]\n",
            " Train Epoch: 39 Loss: 0.1715 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 25.87it/s]\n",
            " Val Epoch: 39 Avg loss: 0.5811 Acc: 0.85 Precision: 0.8581 Recall: 0.8502 F1-Score: 0.8513 Error Rate: 0.1498\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.82it/s]\n",
            " Train Epoch: 40 Loss: 0.1662 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.10it/s]\n",
            " Val Epoch: 40 Avg loss: 0.5229 Acc: 0.87 Precision: 0.8680 Recall: 0.8659 F1-Score: 0.8653 Error Rate: 0.1341\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 23.02it/s]\n",
            " Train Epoch: 41 Loss: 0.1679 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.33it/s]\n",
            " Val Epoch: 41 Avg loss: 0.5018 Acc: 0.87 Precision: 0.8731 Recall: 0.8712 F1-Score: 0.8712 Error Rate: 0.1288\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:08<00:00, 22.97it/s]\n",
            " Train Epoch: 42 Loss: 0.1600 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.14it/s]\n",
            " Val Epoch: 42 Avg loss: 0.4972 Acc: 0.87 Precision: 0.8733 Recall: 0.8694 F1-Score: 0.8689 Error Rate: 0.1306\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.80it/s]\n",
            " Train Epoch: 43 Loss: 0.1580 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.21it/s]\n",
            " Val Epoch: 43 Avg loss: 0.5228 Acc: 0.87 Precision: 0.8668 Recall: 0.8658 F1-Score: 0.8654 Error Rate: 0.1342\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:10<00:00, 22.69it/s]\n",
            " Train Epoch: 44 Loss: 0.1577 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.02it/s]\n",
            " Val Epoch: 44 Avg loss: 0.5038 Acc: 0.87 Precision: 0.8688 Recall: 0.8657 F1-Score: 0.8651 Error Rate: 0.1343\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:10<00:00, 22.75it/s]\n",
            " Train Epoch: 45 Loss: 0.1495 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 45 Avg loss: 0.5623 Acc: 0.86 Precision: 0.8600 Recall: 0.8591 F1-Score: 0.8586 Error Rate: 0.1409\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.82it/s]\n",
            " Train Epoch: 46 Loss: 0.1510 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 25.98it/s]\n",
            " Val Epoch: 46 Avg loss: 0.6558 Acc: 0.85 Precision: 0.8558 Recall: 0.8470 F1-Score: 0.8471 Error Rate: 0.1530\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.90it/s]\n",
            " Train Epoch: 47 Loss: 0.1465 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.27it/s]\n",
            " Val Epoch: 47 Avg loss: 0.5614 Acc: 0.87 Precision: 0.8656 Recall: 0.8653 F1-Score: 0.8645 Error Rate: 0.1347\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:10<00:00, 22.67it/s]\n",
            " Train Epoch: 48 Loss: 0.1421 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.13it/s]\n",
            " Val Epoch: 48 Avg loss: 0.5816 Acc: 0.86 Precision: 0.8648 Recall: 0.8614 F1-Score: 0.8603 Error Rate: 0.1386\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_translate_rotate.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [02:09<00:00, 22.80it/s]\n",
            " Train Epoch: 49 Loss: 0.1415 Acc: 0.94 \n",
            "100% 245/245 [00:09<00:00, 26.07it/s]\n",
            " Val Epoch: 49 Avg loss: 0.5661 Acc: 0.86 Precision: 0.8651 Recall: 0.8620 F1-Score: 0.8609 Error Rate: 0.1380\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_translate_rotate.py -m train -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSZlEGOxSOV2",
        "outputId": "ff145d4d-51cf-4eb8-fb5b-d3e788611ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-31 06:30:30.066844: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-31 06:30:31.030649: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/IndoFashion/utils/common_utils.py\", line 99, in read_json_data\n",
            "    article_list = [json.loads(line) for line in f]\n",
            "  File \"/content/IndoFashion/utils/common_utils.py\", line 99, in <listcomp>\n",
            "    article_list = [json.loads(line) for line in f]\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
            "    obj, end = self.scan_once(s, idx)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/IndoFashion/img_trainer_translate_rotate.py\", line 35, in <module>\n",
            "    train_set = EthnicFinderDataset(metadata_file=config.train_file, mode='train', transform=image_transform_translation_rotation)\n",
            "  File \"/content/IndoFashion/utils/dataset.py\", line 13, in __init__\n",
            "    self.metadata_list = read_json_data(metadata_file)\n",
            "  File \"/content/IndoFashion/utils/common_utils.py\", line 99, in read_json_data\n",
            "    article_list = [json.loads(line) for line in f]\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python /content/IndoFashion/img_trainer_translate_rotate.py -m test --model_name my_model -a resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHz9QSqcSX13"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Define the path to the model file in Colab environment\n",
        "colab_model_path = \"/content/IndoFashion/models/my_model.pt\"\n",
        "\n",
        "# Download the model file\n",
        "files.download(colab_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/IndoFashion/img_trainer_shear.py -m train -a resnet50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mhxHbjCCm6v",
        "outputId": "c827df71-1643-4d0a-d8b2-3cab3eb93d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-07-31 08:36:38.250514: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-31 08:36:39.213621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Metadata file: /content/IndoFashionDataset/annotations/train_data.json\n",
            "Number of samples in the dataset: 94677\n",
            "Metadata file: /content/IndoFashionDataset/annotations/val_data.json\n",
            "Number of samples in the dataset: 7809\n",
            "Metadata file: /content/IndoFashionDataset/annotations/test_data.json\n",
            "Number of samples in the dataset: 7808\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Number of classes: 18\n",
            "Class counts: {'saree': 10791, 'women_kurta': 11694, 'lehenga': 5745, 'blouse': 9174, 'gowns': 5211, 'dupattas': 6585, 'leggings_and_salwars': 7786, 'palazzos': 3374, 'petticoats': 4419, 'mojaris_women': 3228, 'dhoti_pants': 4145, 'kurta_men': 6951, 'nehru_jackets': 6489, 'sherwanis': 2990, 'mojaris_men': 2546, 'men_pagdi': 923, 'women_anarkali_kurta': 1439, 'women_a_line_kurta': 1187}\n",
            "Class weights: [9.266981743187195e-05, 8.55139387647072e-05, 0.00017406440379911847, 0.00010900370611412647, 0.0001919017462690651, 0.00015186028851148667, 0.0001284356537209818, 0.00029638411372365616, 0.00022629554192661334, 0.00030978934315062286, 0.00024125452346411227, 0.00014386419218186388, 0.00015410695019970613, 0.0003344481604232615, 0.0003927729770648967, 0.0010834236174610795, 0.0006949270321786469, 0.0008424599824410614]\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:40<00:00,  4.62it/s]\n",
            " Train Epoch: 1 Loss: 1.1263 Acc: 0.64 \n",
            "100% 245/245 [00:15<00:00, 16.02it/s]\n",
            " Val Epoch: 1 Avg loss: 0.7759 Acc: 0.74 Precision: 0.7489 Recall: 0.7415 F1-Score: 0.7345 Error Rate: 0.2585\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:37<00:00,  4.64it/s]\n",
            " Train Epoch: 2 Loss: 0.7581 Acc: 0.76 \n",
            "100% 245/245 [00:15<00:00, 16.23it/s]\n",
            " Val Epoch: 2 Avg loss: 0.7338 Acc: 0.77 Precision: 0.7907 Recall: 0.7651 F1-Score: 0.7618 Error Rate: 0.2349\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:37<00:00,  4.64it/s]\n",
            " Train Epoch: 3 Loss: 0.6557 Acc: 0.79 \n",
            "100% 245/245 [00:15<00:00, 16.19it/s]\n",
            " Val Epoch: 3 Avg loss: 0.6045 Acc: 0.80 Precision: 0.8128 Recall: 0.7960 F1-Score: 0.7986 Error Rate: 0.2040\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:36<00:00,  4.65it/s]\n",
            " Train Epoch: 4 Loss: 0.5901 Acc: 0.81 \n",
            "100% 245/245 [00:15<00:00, 16.23it/s]\n",
            " Val Epoch: 4 Avg loss: 0.5686 Acc: 0.81 Precision: 0.8201 Recall: 0.8075 F1-Score: 0.8084 Error Rate: 0.1925\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:35<00:00,  4.65it/s]\n",
            " Train Epoch: 5 Loss: 0.5439 Acc: 0.82 \n",
            "100% 245/245 [00:15<00:00, 16.26it/s]\n",
            " Val Epoch: 5 Avg loss: 0.5303 Acc: 0.82 Precision: 0.8294 Recall: 0.8216 F1-Score: 0.8211 Error Rate: 0.1784\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:34<00:00,  4.66it/s]\n",
            " Train Epoch: 6 Loss: 0.5065 Acc: 0.84 \n",
            "100% 245/245 [00:15<00:00, 16.12it/s]\n",
            " Val Epoch: 6 Avg loss: 0.5607 Acc: 0.81 Precision: 0.8366 Recall: 0.8146 F1-Score: 0.8182 Error Rate: 0.1854\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:34<00:00,  4.67it/s]\n",
            " Train Epoch: 7 Loss: 0.4777 Acc: 0.84 \n",
            "100% 245/245 [00:15<00:00, 16.21it/s]\n",
            " Val Epoch: 7 Avg loss: 0.4741 Acc: 0.85 Precision: 0.8498 Recall: 0.8465 F1-Score: 0.8468 Error Rate: 0.1535\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:34<00:00,  4.66it/s]\n",
            " Train Epoch: 8 Loss: 0.4490 Acc: 0.85 \n",
            "100% 245/245 [00:15<00:00, 16.13it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4742 Acc: 0.84 Precision: 0.8447 Recall: 0.8415 F1-Score: 0.8418 Error Rate: 0.1585\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:33<00:00,  4.67it/s]\n",
            " Train Epoch: 9 Loss: 0.4298 Acc: 0.86 \n",
            "100% 245/245 [00:15<00:00, 16.23it/s]\n",
            " Val Epoch: 9 Avg loss: 0.4556 Acc: 0.85 Precision: 0.8570 Recall: 0.8512 F1-Score: 0.8517 Error Rate: 0.1488\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:33<00:00,  4.67it/s]\n",
            " Train Epoch: 10 Loss: 0.4091 Acc: 0.86 \n",
            "100% 245/245 [00:15<00:00, 16.23it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4334 Acc: 0.86 Precision: 0.8574 Recall: 0.8558 F1-Score: 0.8554 Error Rate: 0.1442\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:32<00:00,  4.68it/s]\n",
            " Train Epoch: 11 Loss: 0.3873 Acc: 0.87 \n",
            "100% 245/245 [00:15<00:00, 16.22it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4439 Acc: 0.85 Precision: 0.8579 Recall: 0.8511 F1-Score: 0.8510 Error Rate: 0.1489\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:34<00:00,  4.66it/s]\n",
            " Train Epoch: 12 Loss: 0.3735 Acc: 0.87 \n",
            "100% 245/245 [00:15<00:00, 15.98it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4299 Acc: 0.87 Precision: 0.8693 Recall: 0.8658 F1-Score: 0.8658 Error Rate: 0.1342\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:33<00:00,  4.67it/s]\n",
            " Train Epoch: 13 Loss: 0.3566 Acc: 0.88 \n",
            "100% 245/245 [00:15<00:00, 16.22it/s]\n",
            " Val Epoch: 13 Avg loss: 0.4640 Acc: 0.85 Precision: 0.8557 Recall: 0.8508 F1-Score: 0.8508 Error Rate: 0.1492\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:32<00:00,  4.68it/s]\n",
            " Train Epoch: 14 Loss: 0.3510 Acc: 0.88 \n",
            "100% 245/245 [00:15<00:00, 16.18it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4245 Acc: 0.86 Precision: 0.8684 Recall: 0.8632 F1-Score: 0.8632 Error Rate: 0.1368\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:33<00:00,  4.67it/s]\n",
            " Train Epoch: 15 Loss: 0.3266 Acc: 0.88 \n",
            "100% 245/245 [00:15<00:00, 16.15it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4249 Acc: 0.86 Precision: 0.8657 Recall: 0.8629 F1-Score: 0.8626 Error Rate: 0.1371\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:33<00:00,  4.67it/s]\n",
            " Train Epoch: 16 Loss: 0.3152 Acc: 0.89 \n",
            "100% 245/245 [00:15<00:00, 16.09it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4763 Acc: 0.85 Precision: 0.8599 Recall: 0.8499 F1-Score: 0.8496 Error Rate: 0.1501\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:33<00:00,  4.67it/s]\n",
            " Train Epoch: 17 Loss: 0.3094 Acc: 0.89 \n",
            "100% 245/245 [00:15<00:00, 16.17it/s]\n",
            " Val Epoch: 17 Avg loss: 0.4268 Acc: 0.86 Precision: 0.8628 Recall: 0.8591 F1-Score: 0.8592 Error Rate: 0.1409\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:33<00:00,  4.67it/s]\n",
            " Train Epoch: 18 Loss: 0.2963 Acc: 0.89 \n",
            "100% 245/245 [00:15<00:00, 16.10it/s]\n",
            " Val Epoch: 18 Avg loss: 0.6239 Acc: 0.81 Precision: 0.8528 Recall: 0.8119 F1-Score: 0.8216 Error Rate: 0.1881\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:33<00:00,  4.67it/s]\n",
            " Train Epoch: 19 Loss: 0.2882 Acc: 0.89 \n",
            "100% 245/245 [00:15<00:00, 16.13it/s]\n",
            " Val Epoch: 19 Avg loss: 0.4156 Acc: 0.86 Precision: 0.8701 Recall: 0.8631 F1-Score: 0.8636 Error Rate: 0.1369\n",
            "Best model saved/updated..\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:32<00:00,  4.68it/s]\n",
            " Train Epoch: 20 Loss: 0.2771 Acc: 0.90 \n",
            "100% 245/245 [00:15<00:00, 16.16it/s]\n",
            " Val Epoch: 20 Avg loss: 0.4422 Acc: 0.86 Precision: 0.8635 Recall: 0.8600 F1-Score: 0.8602 Error Rate: 0.1400\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:32<00:00,  4.68it/s]\n",
            " Train Epoch: 21 Loss: 0.2645 Acc: 0.90 \n",
            "100% 245/245 [00:15<00:00, 16.09it/s]\n",
            " Val Epoch: 21 Avg loss: 0.4220 Acc: 0.87 Precision: 0.8678 Recall: 0.8652 F1-Score: 0.8656 Error Rate: 0.1348\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:32<00:00,  4.68it/s]\n",
            " Train Epoch: 22 Loss: 0.2500 Acc: 0.90 \n",
            "100% 245/245 [00:15<00:00, 16.15it/s]\n",
            " Val Epoch: 22 Avg loss: 0.4493 Acc: 0.86 Precision: 0.8678 Recall: 0.8643 F1-Score: 0.8634 Error Rate: 0.1357\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:32<00:00,  4.68it/s]\n",
            " Train Epoch: 23 Loss: 0.2509 Acc: 0.90 \n",
            "100% 245/245 [00:15<00:00, 16.15it/s]\n",
            " Val Epoch: 23 Avg loss: 0.4209 Acc: 0.87 Precision: 0.8698 Recall: 0.8684 F1-Score: 0.8682 Error Rate: 0.1316\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.68it/s]\n",
            " Train Epoch: 24 Loss: 0.2403 Acc: 0.91 \n",
            "100% 245/245 [00:15<00:00, 16.14it/s]\n",
            " Val Epoch: 24 Avg loss: 0.4527 Acc: 0.86 Precision: 0.8629 Recall: 0.8579 F1-Score: 0.8581 Error Rate: 0.1421\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.68it/s]\n",
            " Train Epoch: 25 Loss: 0.2330 Acc: 0.91 \n",
            "100% 245/245 [00:15<00:00, 16.15it/s]\n",
            " Val Epoch: 25 Avg loss: 0.4411 Acc: 0.86 Precision: 0.8633 Recall: 0.8609 F1-Score: 0.8603 Error Rate: 0.1391\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.68it/s]\n",
            " Train Epoch: 26 Loss: 0.2244 Acc: 0.91 \n",
            "100% 245/245 [00:15<00:00, 16.16it/s]\n",
            " Val Epoch: 26 Avg loss: 0.4652 Acc: 0.86 Precision: 0.8653 Recall: 0.8618 F1-Score: 0.8613 Error Rate: 0.1382\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.69it/s]\n",
            " Train Epoch: 27 Loss: 0.2175 Acc: 0.91 \n",
            "100% 245/245 [00:15<00:00, 16.15it/s]\n",
            " Val Epoch: 27 Avg loss: 0.4887 Acc: 0.86 Precision: 0.8636 Recall: 0.8599 F1-Score: 0.8594 Error Rate: 0.1401\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.68it/s]\n",
            " Train Epoch: 28 Loss: 0.2120 Acc: 0.91 \n",
            "100% 245/245 [00:15<00:00, 16.07it/s]\n",
            " Val Epoch: 28 Avg loss: 0.4759 Acc: 0.86 Precision: 0.8627 Recall: 0.8620 F1-Score: 0.8616 Error Rate: 0.1380\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.68it/s]\n",
            " Train Epoch: 29 Loss: 0.2091 Acc: 0.92 \n",
            "100% 245/245 [00:15<00:00, 16.12it/s]\n",
            " Val Epoch: 29 Avg loss: 0.6769 Acc: 0.82 Precision: 0.8466 Recall: 0.8162 F1-Score: 0.8261 Error Rate: 0.1838\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:32<00:00,  4.68it/s]\n",
            " Train Epoch: 30 Loss: 0.1981 Acc: 0.92 \n",
            "100% 245/245 [00:15<00:00, 16.10it/s]\n",
            " Val Epoch: 30 Avg loss: 0.4958 Acc: 0.87 Precision: 0.8685 Recall: 0.8663 F1-Score: 0.8658 Error Rate: 0.1337\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.69it/s]\n",
            " Train Epoch: 31 Loss: 0.1938 Acc: 0.92 \n",
            "100% 245/245 [00:15<00:00, 16.11it/s]\n",
            " Val Epoch: 31 Avg loss: 0.4793 Acc: 0.86 Precision: 0.8639 Recall: 0.8616 F1-Score: 0.8614 Error Rate: 0.1384\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.69it/s]\n",
            " Train Epoch: 32 Loss: 0.1899 Acc: 0.92 \n",
            "100% 245/245 [00:15<00:00, 16.09it/s]\n",
            " Val Epoch: 32 Avg loss: 0.7334 Acc: 0.82 Precision: 0.8584 Recall: 0.8211 F1-Score: 0.8297 Error Rate: 0.1789\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.68it/s]\n",
            " Train Epoch: 33 Loss: 0.1842 Acc: 0.92 \n",
            "100% 245/245 [00:15<00:00, 16.17it/s]\n",
            " Val Epoch: 33 Avg loss: 0.6412 Acc: 0.84 Precision: 0.8544 Recall: 0.8401 F1-Score: 0.8418 Error Rate: 0.1599\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.69it/s]\n",
            " Train Epoch: 34 Loss: 0.1818 Acc: 0.92 \n",
            "100% 245/245 [00:15<00:00, 16.10it/s]\n",
            " Val Epoch: 34 Avg loss: 0.4960 Acc: 0.86 Precision: 0.8594 Recall: 0.8572 F1-Score: 0.8563 Error Rate: 0.1428\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.69it/s]\n",
            " Train Epoch: 35 Loss: 0.1749 Acc: 0.93 \n",
            "100% 245/245 [00:15<00:00, 16.10it/s]\n",
            " Val Epoch: 35 Avg loss: 0.5422 Acc: 0.85 Precision: 0.8564 Recall: 0.8527 F1-Score: 0.8531 Error Rate: 0.1473\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.69it/s]\n",
            " Train Epoch: 36 Loss: 0.1695 Acc: 0.93 \n",
            "100% 245/245 [00:15<00:00, 16.16it/s]\n",
            " Val Epoch: 36 Avg loss: 0.5188 Acc: 0.86 Precision: 0.8611 Recall: 0.8602 F1-Score: 0.8587 Error Rate: 0.1398\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.68it/s]\n",
            " Train Epoch: 37 Loss: 0.1649 Acc: 0.93 \n",
            "100% 245/245 [00:15<00:00, 16.06it/s]\n",
            " Val Epoch: 37 Avg loss: 0.5171 Acc: 0.86 Precision: 0.8593 Recall: 0.8577 F1-Score: 0.8570 Error Rate: 0.1423\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            "100% 2959/2959 [10:31<00:00,  4.69it/s]\n",
            " Train Epoch: 38 Loss: 0.1601 Acc: 0.93 \n",
            "100% 245/245 [00:15<00:00, 16.19it/s]\n",
            " Val Epoch: 38 Avg loss: 1.2112 Acc: 0.79 Precision: 0.8487 Recall: 0.7860 F1-Score: 0.7982 Error Rate: 0.2140\n",
            "  0% 0/2959 [00:00<?, ?it/s]/content/IndoFashion/img_trainer_shear.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(config.device))(y_pred, label)\n",
            " 99% 2918/2959 [10:22<00:08,  4.69it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/IndoFashion/img_trainer_shear.py -m test --model_name my_model -a resnet50"
      ],
      "metadata": {
        "id": "AI6kyjROEjCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Define the path to the model file in Colab environment\n",
        "colab_model_path = \"/content/IndoFashion/models/my_model.pt\"\n",
        "\n",
        "# Download the model file\n",
        "files.download(colab_model_path)"
      ],
      "metadata": {
        "id": "osP9BigrEpFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### New training for ploting visual"
      ],
      "metadata": {
        "id": "BZ1xIQ-1ZXDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/IndoFashion/img_trainer_translate_rotate.py -m train -a resnet50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRac824uZbSu",
        "outputId": "553cec26-807d-4bbf-9735-47518027e0e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-08-14 18:59:13.394813: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-08-14 18:59:13.444761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-14 18:59:14.437103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Metadata file: /content/IndoFashionDataset/annotations/train_data.json\n",
            "Number of samples in the dataset: 94677\n",
            "Metadata file: /content/IndoFashionDataset/annotations/val_data.json\n",
            "Number of samples in the dataset: 7809\n",
            "Metadata file: /content/IndoFashionDataset/annotations/test_data.json\n",
            "Number of samples in the dataset: 7808\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Number of classes: 18\n",
            "Class counts: {'saree': 10791, 'women_kurta': 11694, 'lehenga': 5745, 'blouse': 9174, 'gowns': 5211, 'dupattas': 6585, 'leggings_and_salwars': 7786, 'palazzos': 3374, 'petticoats': 4419, 'mojaris_women': 3228, 'dhoti_pants': 4145, 'kurta_men': 6951, 'nehru_jackets': 6489, 'sherwanis': 2990, 'mojaris_men': 2546, 'men_pagdi': 923, 'women_anarkali_kurta': 1439, 'women_a_line_kurta': 1187}\n",
            "Class weights: [9.266981743187195e-05, 8.55139387647072e-05, 0.00017406440379911847, 0.00010900370611412647, 0.0001919017462690651, 0.00015186028851148667, 0.0001284356537209818, 0.00029638411372365616, 0.00022629554192661334, 0.00030978934315062286, 0.00024125452346411227, 0.00014386419218186388, 0.00015410695019970613, 0.0003344481604232615, 0.0003927729770648967, 0.0010834236174610795, 0.0006949270321786469, 0.0008424599824410614]\n",
            "100% 2959/2959 [02:15<00:00, 21.78it/s]\n",
            " Train Epoch: 1 Loss: 1.1667 Acc: 0.63 \n",
            "100% 245/245 [00:09<00:00, 26.29it/s]\n",
            " Val Epoch: 1 Avg loss: 0.8446 Acc: 0.72 Precision: 0.7271 Recall: 0.7152 F1-Score: 0.7140 Error Rate: 0.2848\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:08<00:00, 23.09it/s]\n",
            " Train Epoch: 2 Loss: 0.7766 Acc: 0.75 \n",
            "100% 245/245 [00:09<00:00, 26.41it/s]\n",
            " Val Epoch: 2 Avg loss: 0.7651 Acc: 0.73 Precision: 0.7670 Recall: 0.7321 F1-Score: 0.7264 Error Rate: 0.2679\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:08<00:00, 23.04it/s]\n",
            " Train Epoch: 3 Loss: 0.6770 Acc: 0.79 \n",
            "100% 245/245 [00:09<00:00, 26.56it/s]\n",
            " Val Epoch: 3 Avg loss: 0.5740 Acc: 0.81 Precision: 0.8151 Recall: 0.8092 F1-Score: 0.8094 Error Rate: 0.1908\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:09<00:00, 22.91it/s]\n",
            " Train Epoch: 4 Loss: 0.5974 Acc: 0.81 \n",
            "100% 245/245 [00:09<00:00, 26.08it/s]\n",
            " Val Epoch: 4 Avg loss: 0.6473 Acc: 0.79 Precision: 0.8106 Recall: 0.7879 F1-Score: 0.7908 Error Rate: 0.2121\n",
            "100% 2959/2959 [02:09<00:00, 22.82it/s]\n",
            " Train Epoch: 5 Loss: 0.5516 Acc: 0.82 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 5 Avg loss: 0.5153 Acc: 0.83 Precision: 0.8330 Recall: 0.8252 F1-Score: 0.8233 Error Rate: 0.1748\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:08<00:00, 22.97it/s]\n",
            " Train Epoch: 6 Loss: 0.5153 Acc: 0.83 \n",
            "100% 245/245 [00:09<00:00, 26.31it/s]\n",
            " Val Epoch: 6 Avg loss: 0.5337 Acc: 0.82 Precision: 0.8324 Recall: 0.8234 F1-Score: 0.8238 Error Rate: 0.1766\n",
            "100% 2959/2959 [02:09<00:00, 22.91it/s]\n",
            " Train Epoch: 7 Loss: 0.4909 Acc: 0.84 \n",
            "100% 245/245 [00:09<00:00, 26.62it/s]\n",
            " Val Epoch: 7 Avg loss: 0.5298 Acc: 0.82 Precision: 0.8337 Recall: 0.8189 F1-Score: 0.8214 Error Rate: 0.1811\n",
            "100% 2959/2959 [02:08<00:00, 22.95it/s]\n",
            " Train Epoch: 8 Loss: 0.4648 Acc: 0.85 \n",
            "100% 245/245 [00:09<00:00, 26.53it/s]\n",
            " Val Epoch: 8 Avg loss: 0.4585 Acc: 0.85 Precision: 0.8498 Recall: 0.8467 F1-Score: 0.8466 Error Rate: 0.1533\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:08<00:00, 23.08it/s]\n",
            " Train Epoch: 9 Loss: 0.4388 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.20it/s]\n",
            " Val Epoch: 9 Avg loss: 0.4653 Acc: 0.84 Precision: 0.8525 Recall: 0.8443 F1-Score: 0.8447 Error Rate: 0.1557\n",
            "100% 2959/2959 [02:09<00:00, 22.93it/s]\n",
            " Train Epoch: 10 Loss: 0.4187 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.60it/s]\n",
            " Val Epoch: 10 Avg loss: 0.4573 Acc: 0.85 Precision: 0.8525 Recall: 0.8483 F1-Score: 0.8468 Error Rate: 0.1517\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:08<00:00, 23.04it/s]\n",
            " Train Epoch: 11 Loss: 0.4009 Acc: 0.86 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 11 Avg loss: 0.4804 Acc: 0.84 Precision: 0.8476 Recall: 0.8358 F1-Score: 0.8387 Error Rate: 0.1642\n",
            "100% 2959/2959 [02:08<00:00, 22.95it/s]\n",
            " Train Epoch: 12 Loss: 0.3851 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.55it/s]\n",
            " Val Epoch: 12 Avg loss: 0.4533 Acc: 0.85 Precision: 0.8571 Recall: 0.8532 F1-Score: 0.8531 Error Rate: 0.1468\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:08<00:00, 23.11it/s]\n",
            " Train Epoch: 13 Loss: 0.3643 Acc: 0.87 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 13 Avg loss: 0.4533 Acc: 0.85 Precision: 0.8575 Recall: 0.8526 F1-Score: 0.8531 Error Rate: 0.1474\n",
            "100% 2959/2959 [02:08<00:00, 22.95it/s]\n",
            " Train Epoch: 14 Loss: 0.3558 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.29it/s]\n",
            " Val Epoch: 14 Avg loss: 0.4671 Acc: 0.85 Precision: 0.8506 Recall: 0.8470 F1-Score: 0.8453 Error Rate: 0.1530\n",
            "100% 2959/2959 [02:08<00:00, 23.00it/s]\n",
            " Train Epoch: 15 Loss: 0.3418 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.55it/s]\n",
            " Val Epoch: 15 Avg loss: 0.4472 Acc: 0.85 Precision: 0.8570 Recall: 0.8523 F1-Score: 0.8518 Error Rate: 0.1477\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:08<00:00, 23.05it/s]\n",
            " Train Epoch: 16 Loss: 0.3295 Acc: 0.88 \n",
            "100% 245/245 [00:09<00:00, 26.38it/s]\n",
            " Val Epoch: 16 Avg loss: 0.4584 Acc: 0.85 Precision: 0.8618 Recall: 0.8530 F1-Score: 0.8538 Error Rate: 0.1470\n",
            "100% 2959/2959 [02:08<00:00, 23.00it/s]\n",
            " Train Epoch: 17 Loss: 0.3170 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.57it/s]\n",
            " Val Epoch: 17 Avg loss: 0.4770 Acc: 0.85 Precision: 0.8561 Recall: 0.8522 F1-Score: 0.8518 Error Rate: 0.1478\n",
            "100% 2959/2959 [02:09<00:00, 22.90it/s]\n",
            " Train Epoch: 18 Loss: 0.3089 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.16it/s]\n",
            " Val Epoch: 18 Avg loss: 0.4208 Acc: 0.86 Precision: 0.8667 Recall: 0.8646 F1-Score: 0.8643 Error Rate: 0.1354\n",
            "Best model saved/updated..\n",
            "100% 2959/2959 [02:08<00:00, 22.99it/s]\n",
            " Train Epoch: 19 Loss: 0.2976 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.57it/s]\n",
            " Val Epoch: 19 Avg loss: 0.5725 Acc: 0.83 Precision: 0.8529 Recall: 0.8270 F1-Score: 0.8320 Error Rate: 0.1730\n",
            "100% 2959/2959 [02:08<00:00, 22.96it/s]\n",
            " Train Epoch: 20 Loss: 0.2872 Acc: 0.89 \n",
            "100% 245/245 [00:09<00:00, 26.24it/s]\n",
            " Val Epoch: 20 Avg loss: 0.4373 Acc: 0.86 Precision: 0.8687 Recall: 0.8614 F1-Score: 0.8610 Error Rate: 0.1386\n",
            "100% 2959/2959 [02:09<00:00, 22.89it/s]\n",
            " Train Epoch: 21 Loss: 0.2757 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.61it/s]\n",
            " Val Epoch: 21 Avg loss: 0.4389 Acc: 0.86 Precision: 0.8628 Recall: 0.8613 F1-Score: 0.8605 Error Rate: 0.1387\n",
            "100% 2959/2959 [02:08<00:00, 23.09it/s]\n",
            " Train Epoch: 22 Loss: 0.2709 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.23it/s]\n",
            " Val Epoch: 22 Avg loss: 0.4464 Acc: 0.86 Precision: 0.8614 Recall: 0.8590 F1-Score: 0.8586 Error Rate: 0.1410\n",
            "100% 2959/2959 [02:08<00:00, 23.05it/s]\n",
            " Train Epoch: 23 Loss: 0.2618 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.05it/s]\n",
            " Val Epoch: 23 Avg loss: 0.4615 Acc: 0.86 Precision: 0.8609 Recall: 0.8593 F1-Score: 0.8584 Error Rate: 0.1407\n",
            "100% 2959/2959 [02:08<00:00, 22.94it/s]\n",
            " Train Epoch: 24 Loss: 0.2529 Acc: 0.90 \n",
            "100% 245/245 [00:09<00:00, 26.49it/s]\n",
            " Val Epoch: 24 Avg loss: 0.4546 Acc: 0.86 Precision: 0.8681 Recall: 0.8612 F1-Score: 0.8621 Error Rate: 0.1388\n",
            "100% 2959/2959 [02:08<00:00, 22.95it/s]\n",
            " Train Epoch: 25 Loss: 0.2435 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.05it/s]\n",
            " Val Epoch: 25 Avg loss: 0.4419 Acc: 0.87 Precision: 0.8723 Recall: 0.8691 F1-Score: 0.8683 Error Rate: 0.1309\n",
            "100% 2959/2959 [02:08<00:00, 23.00it/s]\n",
            " Train Epoch: 26 Loss: 0.2390 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.42it/s]\n",
            " Val Epoch: 26 Avg loss: 0.4643 Acc: 0.86 Precision: 0.8614 Recall: 0.8581 F1-Score: 0.8580 Error Rate: 0.1419\n",
            "100% 2959/2959 [02:08<00:00, 23.07it/s]\n",
            " Train Epoch: 27 Loss: 0.2313 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.22it/s]\n",
            " Val Epoch: 27 Avg loss: 0.4452 Acc: 0.87 Precision: 0.8699 Recall: 0.8678 F1-Score: 0.8670 Error Rate: 0.1322\n",
            "100% 2959/2959 [02:08<00:00, 23.01it/s]\n",
            " Train Epoch: 28 Loss: 0.2255 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.52it/s]\n",
            " Val Epoch: 28 Avg loss: 0.4820 Acc: 0.86 Precision: 0.8650 Recall: 0.8603 F1-Score: 0.8608 Error Rate: 0.1397\n",
            "100% 2959/2959 [02:09<00:00, 22.77it/s]\n",
            " Train Epoch: 29 Loss: 0.2210 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.51it/s]\n",
            " Val Epoch: 29 Avg loss: 0.4637 Acc: 0.87 Precision: 0.8691 Recall: 0.8650 F1-Score: 0.8659 Error Rate: 0.1350\n",
            "100% 2959/2959 [02:08<00:00, 23.03it/s]\n",
            " Train Epoch: 30 Loss: 0.2157 Acc: 0.91 \n",
            "100% 245/245 [00:09<00:00, 26.32it/s]\n",
            " Val Epoch: 30 Avg loss: 0.6648 Acc: 0.84 Precision: 0.8645 Recall: 0.8356 F1-Score: 0.8432 Error Rate: 0.1644\n",
            "100% 2959/2959 [02:08<00:00, 22.94it/s]\n",
            " Train Epoch: 31 Loss: 0.2080 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.34it/s]\n",
            " Val Epoch: 31 Avg loss: 0.4607 Acc: 0.86 Precision: 0.8662 Recall: 0.8637 F1-Score: 0.8637 Error Rate: 0.1363\n",
            "100% 2959/2959 [02:09<00:00, 22.93it/s]\n",
            " Train Epoch: 32 Loss: 0.2018 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.28it/s]\n",
            " Val Epoch: 32 Avg loss: 0.4822 Acc: 0.87 Precision: 0.8722 Recall: 0.8691 F1-Score: 0.8685 Error Rate: 0.1309\n",
            "100% 2959/2959 [02:08<00:00, 23.07it/s]\n",
            " Train Epoch: 33 Loss: 0.1979 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.46it/s]\n",
            " Val Epoch: 33 Avg loss: 0.4822 Acc: 0.86 Precision: 0.8642 Recall: 0.8644 F1-Score: 0.8630 Error Rate: 0.1356\n",
            "100% 2959/2959 [02:08<00:00, 23.05it/s]\n",
            " Train Epoch: 34 Loss: 0.1932 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.33it/s]\n",
            " Val Epoch: 34 Avg loss: 0.4665 Acc: 0.87 Precision: 0.8699 Recall: 0.8666 F1-Score: 0.8663 Error Rate: 0.1334\n",
            "100% 2959/2959 [02:08<00:00, 23.00it/s]\n",
            " Train Epoch: 35 Loss: 0.1866 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.46it/s]\n",
            " Val Epoch: 35 Avg loss: 0.4856 Acc: 0.87 Precision: 0.8676 Recall: 0.8657 F1-Score: 0.8654 Error Rate: 0.1343\n",
            "100% 2959/2959 [02:08<00:00, 23.06it/s]\n",
            " Train Epoch: 36 Loss: 0.1830 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.54it/s]\n",
            " Val Epoch: 36 Avg loss: 0.4778 Acc: 0.87 Precision: 0.8693 Recall: 0.8664 F1-Score: 0.8667 Error Rate: 0.1336\n",
            "100% 2959/2959 [02:07<00:00, 23.15it/s]\n",
            " Train Epoch: 37 Loss: 0.1795 Acc: 0.92 \n",
            "100% 245/245 [00:09<00:00, 26.40it/s]\n",
            " Val Epoch: 37 Avg loss: 0.5207 Acc: 0.86 Precision: 0.8683 Recall: 0.8639 F1-Score: 0.8634 Error Rate: 0.1361\n",
            "100% 2959/2959 [02:09<00:00, 22.77it/s]\n",
            " Train Epoch: 38 Loss: 0.1746 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.37it/s]\n",
            " Val Epoch: 38 Avg loss: 0.4928 Acc: 0.86 Precision: 0.8674 Recall: 0.8649 F1-Score: 0.8649 Error Rate: 0.1351\n",
            "100% 2959/2959 [02:08<00:00, 23.00it/s]\n",
            " Train Epoch: 39 Loss: 0.1664 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.19it/s]\n",
            " Val Epoch: 39 Avg loss: 0.5309 Acc: 0.86 Precision: 0.8582 Recall: 0.8573 F1-Score: 0.8564 Error Rate: 0.1427\n",
            "100% 2959/2959 [02:10<00:00, 22.76it/s]\n",
            " Train Epoch: 40 Loss: 0.1691 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.41it/s]\n",
            " Val Epoch: 40 Avg loss: 0.5264 Acc: 0.86 Precision: 0.8657 Recall: 0.8640 F1-Score: 0.8636 Error Rate: 0.1360\n",
            "100% 2959/2959 [02:08<00:00, 23.11it/s]\n",
            " Train Epoch: 41 Loss: 0.1619 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.50it/s]\n",
            " Val Epoch: 41 Avg loss: 0.5299 Acc: 0.86 Precision: 0.8636 Recall: 0.8603 F1-Score: 0.8595 Error Rate: 0.1397\n",
            "100% 2959/2959 [02:09<00:00, 22.93it/s]\n",
            " Train Epoch: 42 Loss: 0.1599 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.48it/s]\n",
            " Val Epoch: 42 Avg loss: 0.5557 Acc: 0.86 Precision: 0.8579 Recall: 0.8566 F1-Score: 0.8558 Error Rate: 0.1434\n",
            "100% 2959/2959 [02:08<00:00, 23.03it/s]\n",
            " Train Epoch: 43 Loss: 0.1538 Acc: 0.93 \n",
            "100% 245/245 [00:09<00:00, 26.64it/s]\n",
            " Val Epoch: 43 Avg loss: 0.6908 Acc: 0.83 Precision: 0.8580 Recall: 0.8342 F1-Score: 0.8399 Error Rate: 0.1658\n",
            "Early stopping\n",
            "Figure(640x480)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/IndoFashion/img_trainer_translate_rotate.py -m test --model_name my_model -a resnet50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en37M1b7cb0P",
        "outputId": "931b59bd-85bb-42d9-fe70-349da566ec20"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2023-08-14 20:38:35.605755: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-08-14 20:38:35.653949: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-14 20:38:36.642379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Metadata file: /content/IndoFashionDataset/annotations/train_data.json\n",
            "Number of samples in the dataset: 94677\n",
            "Metadata file: /content/IndoFashionDataset/annotations/val_data.json\n",
            "Number of samples in the dataset: 7809\n",
            "Metadata file: /content/IndoFashionDataset/annotations/test_data.json\n",
            "Number of samples in the dataset: 7808\n",
            "Total Params 25593914\n",
            "Loading Saved Model\n",
            "Saved Model successfully loaded\n",
            "100% 244/244 [00:11<00:00, 21.27it/s]\n",
            "Test Acc: 0.8786\n",
            "Precision: 0.8804 Recall: 0.8786 F1-Score: 0.8782 Error Rate: 0.1214\n",
            "Figure(1000x800)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Define the path to the model file in Colab environment\n",
        "colab_model_path = \"/content/IndoFashion/models/my_model.pt\"\n",
        "\n",
        "# Download the model file\n",
        "files.download(colab_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "GtbpBZBVcjh_",
        "outputId": "e4809aac-3f51-45a1-90b0-62310d442c24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7cd283c2-4880-4211-8a15-01f12997501c\", \"my_model.pt\", 102741249)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate accuracy values\n",
        "train_accuracy = [1.0 - loss for loss in train_losses]\n",
        "val_accuracy = [1.0 - loss for loss in val_losses]\n",
        "\n",
        "# Plot train and validation accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_accuracy) + 1), train_accuracy, label='Train Accuracy')\n",
        "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fwfLHYrauOTN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}